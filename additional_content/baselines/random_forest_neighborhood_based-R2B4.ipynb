{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "**For Table 3 of the paper**\n",
    "\n",
    "Neighborhood-based NARVAL R2B4 model\n",
    "\n",
    "n_estimator = 1 takes 41min 59s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "NUM = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/home/b/b309170'\n",
    "data_path = os.path.join(root_path,\n",
    "                         'my_work/icon-ml_data/cloud_cover_parameterization/region_based/based_on_var_interpolated_data')\n",
    "model_path = os.path.join(root_path,\n",
    "                          'workspace_icon-ml/cloud_cover_parameterization/region_based/saved_models')\n",
    "info_file = os.path.join(root_path, \n",
    "                        'workspace_icon-ml/cloud_cover_parameterization/region_based/saved_models/model_region_based_final_1.txt')\n",
    "\n",
    "n_layers = 27 # Is also the number of NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data (Data is already normalized w.r.t. training data)\n",
    "input_train = np.load(os.path.join(data_path, 'cloud_cover_input_train_1.npy'))\n",
    "output_train = np.load(os.path.join(data_path, 'cloud_cover_output_train_1.npy'))\n",
    "input_valid = np.load(os.path.join(data_path, 'cloud_cover_input_valid_1.npy'))\n",
    "output_valid = np.load(os.path.join(data_path, 'cloud_cover_output_valid_1.npy'))\n",
    "input_test = np.load(os.path.join(data_path, 'cloud_cover_input_test_1.npy')) \n",
    "output_test = np.load(os.path.join(data_path, 'cloud_cover_output_test_1.npy')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data pertaining to a specific NN\n",
    "n_train_samples = output_train.shape[0]\n",
    "n_valid_samples = output_valid.shape[0]\n",
    "n_test_samples = output_test.shape[0]\n",
    "n_features = input_train.shape[1]\n",
    "\n",
    "# Load the data into dictionaries. Can't use 3D tensors here as some features will be removed depending on the NN.\n",
    "input_train_NN = {}\n",
    "for i in range(n_layers):\n",
    "    input_train_NN[i] = np.zeros((n_train_samples//n_layers, n_features))\n",
    "    \n",
    "output_train_NN = {}\n",
    "for i in range(n_layers):\n",
    "    output_train_NN[i] = np.zeros((n_train_samples//n_layers))\n",
    "    \n",
    "input_valid_NN = {}\n",
    "for i in range(n_layers):\n",
    "    input_valid_NN[i] = np.zeros((n_valid_samples//n_layers, n_features))\n",
    "    \n",
    "output_valid_NN = {}\n",
    "for i in range(n_layers):\n",
    "    output_valid_NN[i] = np.zeros((n_valid_samples//n_layers))\n",
    "    \n",
    "input_test_NN = {}\n",
    "for i in range(n_layers):\n",
    "    input_test_NN[i] = np.zeros((n_test_samples//n_layers, n_features))\n",
    "    \n",
    "output_test_NN = {}\n",
    "for i in range(n_layers):\n",
    "    output_test_NN[i] = np.zeros((n_test_samples//n_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_layers):\n",
    "    start_ind_train = (n_train_samples//27)*i\n",
    "    end_ind_train = (n_train_samples//27)*(i+1)\n",
    "    start_ind_valid = (n_valid_samples//27)*i\n",
    "    end_ind_valid = (n_valid_samples//27)*(i+1)\n",
    "    start_ind_test = (n_test_samples//27)*i\n",
    "    end_ind_test = (n_test_samples//27)*(i+1) \n",
    "\n",
    "    input_train_NN[i] = input_train[start_ind_train:end_ind_train]\n",
    "    output_train_NN[i] = output_train[start_ind_train:end_ind_train]\n",
    "    input_valid_NN[i] = input_valid[start_ind_valid:end_ind_valid]\n",
    "    output_valid_NN[i] = output_valid[start_ind_valid:end_ind_valid]\n",
    "    input_test_NN[i] = input_test[start_ind_test:end_ind_test]  \n",
    "    output_test_NN[i] = output_test[start_ind_test:end_ind_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the input variables with zero variance. We compute the resulting input dimension for the NN.\n",
    "input_dim = n_features\n",
    "for i in range(n_layers):\n",
    "    vars_to_remove = []\n",
    "    for j in range(n_features):\n",
    "        if np.var(input_train_NN[i][:, j]) == 0 or np.isnan(np.var(input_train_NN[i][:, j])):\n",
    "            input_dim -= 1\n",
    "            vars_to_remove.append(j)\n",
    "    input_train_NN[i] = np.delete(input_train_NN[i], vars_to_remove, axis=1)\n",
    "    input_valid_NN[i] = np.delete(input_valid_NN[i], vars_to_remove, axis=1)\n",
    "    input_test_NN[i] = np.delete(input_test_NN[i], vars_to_remove, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking standardization:\n",
    "thresh = 1e-5\n",
    "[np.abs(np.mean(input_train_NN[0][:, j]))<thresh and np.abs(np.var(input_train_NN[0][:, j])-1)<thresh for j in range(input_train_NN[0].shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41min 55s, sys: 3.99 s, total: 41min 59s\n",
      "Wall time: 41min 59s\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "mean_rf_mse = 0\n",
    "\n",
    "for i in range(2, n_layers):\n",
    "    # Instantiate model with 100 decision trees\n",
    "    rf = RandomForestRegressor(n_estimators = 47, random_state = 10)\n",
    "\n",
    "    # Train the model on training data\n",
    "    rf.fit(input_train_NN[i], output_train_NN[i])\n",
    "    \n",
    "    # model_fold_3 is implemented in ICON-A\n",
    "    batch_size = 2**20\n",
    "\n",
    "    for j in range(1 + input_test_NN[i].shape[0]//batch_size):\n",
    "        if j == 0:\n",
    "            clc_predictions = rf.predict(input_test_NN[i][j*batch_size:(j+1)*batch_size])\n",
    "        else:\n",
    "            clc_predictions = np.concatenate((clc_predictions, rf.predict(input_test_NN[i][j*batch_size:(j+1)*batch_size])), axis=0)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "    rf_mse = mean_squared_error(output_test_NN[i], clc_predictions)\n",
    "    \n",
    "    with open('/home/b/b309170/workspace_icon-ml/iconml_clc/additional_content/baselines/RFs/RF_results.txt', 'a') as file:\n",
    "        file.write('The MSE on the test set of the neighborhood-based R2B4 RF on layer %d is %.2f.\\n'%(i, rf_mse)) \n",
    "    mean_rf_mse += rf_mse\n",
    "\n",
    "with open('/home/b/b309170/workspace_icon-ml/iconml_clc/additional_content/baselines/RFs/RF_results.txt', 'a') as file:\n",
    "    file.write('The overall MSE on the test set of the neighborhood-based R2B4 RF is %.2f.\\n'%(mean_rf_mse/(n_layers-2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds_kernel",
   "language": "python",
   "name": "clouds_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
