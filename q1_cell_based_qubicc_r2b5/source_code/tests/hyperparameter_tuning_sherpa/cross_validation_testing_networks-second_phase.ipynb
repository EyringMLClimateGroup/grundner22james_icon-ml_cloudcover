{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning: Second phase\n",
    "\n",
    "Run **SHERPA**. Fix batchsize = 1024. Fix Adam. Do not shuffle the input data as that takes a lot of time. <br>\n",
    "*First phase:* Start with 3 epochs each. Here we can already discard some models. <br>\n",
    "*Second phase:* Run 3 epochs with a parameter space confined to the four best models from phase 1. Add a learning rate scheduler a la Stephan Rasp (Divide learning rate by 20 every two epochs). <br>\n",
    "*Third phase:* Run 6 epochs with the two best models from phase 2. With Sherpa, vary only the learning rate scheduler. Use cross-validation here to truly get a good estimate of generalization error!. <br>\n",
    "\n",
    "To vary: \n",
    "- Learning rate (Learning rate scheduler)\n",
    "- Model layers (only max 1-4 hidden layers)\n",
    "- Regularization methods\n",
    "- Hidden Units\n",
    "- Activation Functions (not the last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best results from phase 1:** <br>\n",
    "Activation_1: lrelu or leaky_relu <br>\n",
    "Activation_2: lrelu or elu <br>\n",
    "Activation_3: relu or leaky_relu <br>\n",
    "Dropout: 0.15 - 0.3 <br>\n",
    "Epsilon: 0 or 0.1 <br>\n",
    "l1_reg: 0.0001 to 0.007 <br>\n",
    "l2_reg: 0.001 to 0.007 <br>\n",
    "lr_init: 0.001-0.009 <br>\n",
    "model_depth: 3 or 4 <br>\n",
    "hidden units: 32 to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best results from Phase No. 1:\n",
    "\n",
    "# Trial-ID                                         1\n",
    "# Status                                   COMPLETED\n",
    "# Iteration                                        2\n",
    "# activation_1    <function lrelu at 0x2abbb526a830>\n",
    "# activation_2    <function lrelu at 0x2abbb526a830>\n",
    "# activation_3                                   NaN\n",
    "# activation_4                                   NaN\n",
    "# activation_5                                  relu\n",
    "# activation_6                                  relu\n",
    "# activation_7                                  relu\n",
    "# dropout                                   0.184124\n",
    "# epsilon                                        0.1\n",
    "# l1_reg                                    0.000162\n",
    "# l2_reg                                    0.007437\n",
    "# lrinit                                    0.008726\n",
    "# model_depth                                      3\n",
    "# num_units                                      256\n",
    "# Objective                                42.318375\n",
    "# Name: 0, dtype: object\n",
    "\n",
    "# Trial-ID                                              2\n",
    "# Status                                        COMPLETED\n",
    "# Iteration                                             2\n",
    "# activation_1    <function leaky_relu at 0x2b915efe3ef0>\n",
    "# activation_2         <function lrelu at 0x2b9167db78c0>\n",
    "# activation_3                                       relu\n",
    "# activation_4                                        NaN\n",
    "# activation_5                                        NaN\n",
    "# activation_6                                        NaN\n",
    "# activation_7                                        NaN\n",
    "# dropout                                        0.151082\n",
    "# epsilon                                             0.0\n",
    "# l1_reg                                         0.002857\n",
    "# l2_reg                                         0.006965\n",
    "# lrinit                                         0.002045\n",
    "# model_depth                                           4\n",
    "# num_units                                            32\n",
    "# Objective                                     44.167797\n",
    "# Name: 1, dtype: object\n",
    "\n",
    "# Trial-ID                                              2\n",
    "# Status                                        COMPLETED\n",
    "# Iteration                                             2\n",
    "# activation_1         <function lrelu at 0x2ba500c62830>\n",
    "# activation_2                                        elu\n",
    "# activation_3    <function leaky_relu at 0x2ba4f7e8eef0>\n",
    "# activation_4                                        NaN\n",
    "# activation_5                                        NaN\n",
    "# activation_6                                        NaN\n",
    "# activation_7                                        NaN\n",
    "# dropout                                        0.231274\n",
    "# epsilon                                             0.0\n",
    "# l1_reg                                         0.007085\n",
    "# l2_reg                                          0.00134\n",
    "# lrinit                                         0.005609\n",
    "# model_depth                                           4\n",
    "# num_units                                            64\n",
    "# Objective                                     44.761337\n",
    "# Name: 1, dtype: object\n",
    "\n",
    "# Trial-ID                                         1\n",
    "# Status                                   COMPLETED\n",
    "# Iteration                                        2\n",
    "# activation_1    <function lrelu at 0x2add07c84830>\n",
    "# activation_2    <function lrelu at 0x2add07c84830>\n",
    "# activation_3                                   NaN\n",
    "# activation_4                                   NaN\n",
    "# activation_5                                  relu\n",
    "# activation_6                                  relu\n",
    "# activation_7                                  relu\n",
    "# dropout                                        0.3\n",
    "# epsilon                                        0.1\n",
    "# l1_reg                                       0.004\n",
    "# l2_reg                                       0.004\n",
    "# lrinit                                      0.0012\n",
    "# model_depth                                      3\n",
    "# num_units                                      128\n",
    "# Objective                                49.790131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting only with a few epochs\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran with 800GB (750GB should also be fine)\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "#Import sklearn before tensorflow (static Thread-local storage)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "t0 = time.time()\n",
    "path = '/pf/b/b309170'\n",
    "path_figures = path + '/workspace_icon-ml/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/figures'\n",
    "path_model = path + '/workspace_icon-ml/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/saved_models'\n",
    "path_data = path + '/my_work/icon-ml_data/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/based_on_var_interpolated_data'\n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "sys.path.insert(0, path + '/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "# Add sherpa\n",
    "sys.path.insert(0, path + '/my_work/sherpa')\n",
    "\n",
    "#import sherpa\n",
    "#import sherpa.algorithms.bayesian_optimization as bayesian_optimization\n",
    "\n",
    "# Reloading custom file to incorporate changes dynamically\n",
    "import importlib\n",
    "import my_classes\n",
    "importlib.reload(my_classes)\n",
    "\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import TimeOut\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 10\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_visible_devices(gpus[3], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't run on a CPU node\n",
    "try:\n",
    "    # Prevents crashes of the code\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "    # Allow the growth of memory Tensorflow allocates (limits memory usage overall)\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_narval = np.load(path_data + '/cloud_cover_input_narval.npy')\n",
    "# input_qubicc = np.load(path_data + '/cloud_cover_input_qubicc.npy')\n",
    "# output_narval = np.load(path_data + '/cloud_cover_output_narval.npy')\n",
    "# output_qubicc = np.load(path_data + '/cloud_cover_output_qubicc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.concatenate((np.load(path_data + '/cloud_cover_input_narval.npy'), \n",
    "                             np.load(path_data + '/cloud_cover_input_qubicc.npy')), axis=0)\n",
    "output_data = np.concatenate((np.load(path_data + '/cloud_cover_output_narval.npy'), \n",
    "                              np.load(path_data + '/cloud_cover_output_qubicc.npy')), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_narval = np.load(path_data + '/cloud_cover_output_narval.npy').shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1008913906, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(samples_total, no_of_features) = input_data.shape\n",
    "(samples_total, no_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Temporal cross-validation*\n",
    "\n",
    "Split into 2-weeks increments (when working with 3 months of data). It's 25 day increments with 5 months of data. <br>\n",
    "1.: Validate on increments 1 and 4 <br>\n",
    "2.: Validate on increments 2 and 5 <br>\n",
    "3.: Validate on increments 3 and 6\n",
    "\n",
    "--> 2/3 training data, 1/3 validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folds = []\n",
    "validation_folds = []\n",
    "two_week_incr = samples_total//6\n",
    "\n",
    "for i in range(3):\n",
    "    # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "    first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "    second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "    validation_folds.append(np.append(first_incr, second_incr))\n",
    "    training_folds.append(np.arange(samples_total))\n",
    "    training_folds[i] = np.delete(training_folds[i], validation_folds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 3-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We loop through the folds\n",
    "def run_cross_validation(i):\n",
    "    \n",
    "    filename = 'cross_validation_cell_based_fold_%d'%(i+1)\n",
    "    \n",
    "    #Standardize according to the fold\n",
    "    scaler.fit(input_data[training_folds[i]])\n",
    "\n",
    "    #Load the data for the respective fold and convert it to tf data\n",
    "    input_train = scaler.transform(input_data[training_folds[i]])\n",
    "    input_valid = scaler.transform(input_data[validation_folds[i]])\n",
    "    output_train = output_data[training_folds[i]]\n",
    "    output_valid = output_data[validation_folds[i]]\n",
    "    \n",
    "    # Column-based: batchsize of 128\n",
    "    # Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "    # I'm not shuffling for hyperparameter tuning\n",
    "    train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                                tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "                .shuffle(10**5, seed=seed) \\\n",
    "                .batch(batch_size=1024, drop_remainder=True) \\\n",
    "                .prefetch(1)\n",
    "    \n",
    "    # No need to add prefetch.\n",
    "    # tf data with batch_size=10**5 makes the validation evaluation 10 times faster\n",
    "    valid_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_valid), \n",
    "                                tf.data.Dataset.from_tensor_slices(output_valid))) \\\n",
    "                .batch(batch_size=10**5, drop_remainder=False)\n",
    "    \n",
    "    return train_ds, valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "494519\n"
     ]
    }
   ],
   "source": [
    "#Should be a pretty unique number\n",
    "random_num = np.random.randint(500000)\n",
    "print(random_num)\n",
    "\n",
    "def save_model(study, today, optimizer):\n",
    "    out_path = '/pf/b/b309170/workspace_icon-ml/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/sherpa_results/'+\\\n",
    "            today+'_'+optimizer+'_'+str(random_num)\n",
    "    \n",
    "    study.results = study.results[study.results['Status']=='COMPLETED'] #To specify results\n",
    "    study.results.index = study.results['Trial-ID']  #Trial-ID serves as a better index\n",
    "    # Remove those hyperparameters that actually do not appear in the model\n",
    "    for i in range(1, max(study.results['Trial-ID']) + 1):\n",
    "        depth = study.results.at[i, 'model_depth']\n",
    "        for j in range(depth, 4): #Or up to 8\n",
    "            study.results.at[i, 'activation_%d'%j] = None\n",
    "#             study.results.at[i, 'bn_%d'%j] = None\n",
    "    # Create the directory and save the SHERPA-output in it\n",
    "    try:\n",
    "        os.mkdir(out_path)\n",
    "    except OSError:\n",
    "        print('Creation of the directory %s failed' % out_path)\n",
    "    else: \n",
    "        print('Successfully created the directory %s' % out_path)\n",
    "    study.save(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Leaky_ReLU:\n",
    "from tensorflow import nn \n",
    "\n",
    "def lrelu(x):\n",
    "    return nn.leaky_relu(x, alpha=0.01)\n",
    "\n",
    "OPTIMIZER = 'adam'\n",
    "parameters = [sherpa.Ordinal('num_units', [32, 64, 128, 256]), #No need to vary these per layer. Could add 512.\n",
    "             sherpa.Discrete('model_depth', [3, 4]), #Originally [2,8] although 8 was never truly tested\n",
    "             sherpa.Choice('activation_1', ['relu', nn.leaky_relu]), #Adding SeLU is trickier\n",
    "             sherpa.Choice('activation_2', ['elu']), \n",
    "             sherpa.Choice('activation_3', ['relu']),\n",
    "             sherpa.Continuous('lrinit', [0.001, 0.01], 'log'),\n",
    "             sherpa.Ordinal('epsilon', [1e-8, 0.1]),\n",
    "             sherpa.Continuous('dropout', [0.15, 0.3]),\n",
    "             sherpa.Continuous('l1_reg', [0.0001, 0.007]),\n",
    "             sherpa.Continuous('l2_reg', [0.001, 0.007])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sherpa.core:\n",
      "-------------------------------------------------------\n",
      "SHERPA Dashboard running. Access via\n",
      "http://10.50.13.252:8880 if on a cluster or\n",
      "http://localhost:8880 if running locally.\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"sherpa.app.app\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    }
   ],
   "source": [
    "# max_num_trials is left unspecified, so the optimization will run until the end of the job-runtime\n",
    "\n",
    "# good_hyperparams = pd.DataFrame({'num_units': [128], 'model_depth': [3], 'activation_1': [lrelu], 'activation_2':[lrelu],\n",
    "#                    'activation_3':['relu'], 'activation_4':['relu'], 'activation_5':['relu'], 'activation_6':['relu'],\n",
    "#                    'activation_7':['relu'], 'lrinit':[0.0012], 'epsilon':[0.1], 'dropout':[0.3], \n",
    "#                                  'l1_reg':[0.004], 'l2_reg':[0.004]})\n",
    "\n",
    "# # I expect an objective of around 61.\n",
    "\n",
    "# alg = bayesian_optimization.GPyOpt(initial_data_points=good_hyperparams)\n",
    "\n",
    "alg = bayesian_optimization.GPyOpt() \n",
    "study = sherpa.Study(parameters=parameters, algorithm=alg, lower_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce lr every two epochs, starting at the second epoch\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch > 0 and epoch%2==0:\n",
    "        return lr/20\n",
    "    else:\n",
    "        return lr\n",
    "    \n",
    "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0036447139546774226\n",
      "1\n",
      "2\n",
      "3\n",
      "Epoch 1/3\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0036447138991206884.\n",
      "2/2 - 1s - loss: 0.9846 - val_loss: 7.4126\n",
      "Epoch 2/3\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0036447138991206884.\n",
      "2/2 - 0s - loss: 0.9509 - val_loss: 0.9493\n",
      "Epoch 3/3\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00018223569495603442.\n",
      "2/2 - 0s - loss: 0.9042 - val_loss: 0.9859\n",
      "1\n",
      "2\n",
      "3\n",
      "Epoch 1/3\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001822357007768005.\n",
      "2/2 - 0s - loss: 0.9001 - val_loss: 0.8831\n",
      "Epoch 2/3\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0001822357007768005.\n",
      "2/2 - 0s - loss: 0.8905 - val_loss: 0.8792\n",
      "Epoch 3/3\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.111785038840025e-06.\n",
      "2/2 - 0s - loss: 0.9343 - val_loss: 0.8791\n",
      "1\n",
      "2\n",
      "3\n",
      "Epoch 1/3\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 9.111785402637906e-06.\n",
      "2/2 - 0s - loss: 0.8922 - val_loss: 0.8783\n",
      "Epoch 2/3\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 9.111785402637906e-06.\n",
      "2/2 - 0s - loss: 0.8958 - val_loss: 0.8781\n",
      "Epoch 3/3\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 4.5558927013189533e-07.\n",
      "2/2 - 0s - loss: 0.8886 - val_loss: 0.8781\n",
      "Successfully created the directory /pf/b/b309170/workspace_icon-ml/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/sherpa_results/2021-05_adam_494519\n",
      "0.00784683756002348\n",
      "1\n",
      "2\n",
      "3\n",
      "Epoch 1/3\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.007846837863326073.\n",
      "2/2 - 1s - loss: 7.3631 - val_loss: 13.6408\n",
      "Epoch 2/3\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.007846837863326073.\n",
      "2/2 - 0s - loss: 4.7769 - val_loss: 245.4697\n",
      "Epoch 3/3\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00039234189316630366.\n",
      "2/2 - 0s - loss: 5.7570 - val_loss: 235.8732\n",
      "1\n",
      "2\n",
      "3\n",
      "Epoch 1/3\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0003923418989870697.\n",
      "2/2 - 0s - loss: 6.1498 - val_loss: 8.4927\n",
      "Epoch 2/3\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0003923418989870697.\n",
      "2/2 - 0s - loss: 5.7885 - val_loss: 7.6503\n",
      "Epoch 3/3\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 1.9617094949353486e-05.\n",
      "2/2 - 0s - loss: 5.5734 - val_loss: 7.6058\n",
      "1\n",
      "2\n",
      "3\n",
      "Epoch 1/3\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1.9617094949353486e-05.\n",
      "2/2 - 0s - loss: 5.5819 - val_loss: 7.5632\n",
      "Epoch 2/3\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 1.9617094949353486e-05.\n",
      "2/2 - 0s - loss: 5.5098 - val_loss: 7.5136\n",
      "Epoch 3/3\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 9.808547474676743e-07.\n",
      "2/2 - 0s - loss: 5.4746 - val_loss: 7.5111\n",
      "Creation of the directory /pf/b/b309170/workspace_icon-ml/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/sherpa_results/2021-05_adam_494519 failed\n",
      "0.004420234959476211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-fff4c36c0fe0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Not using the keras_callback here as then the objective of a given trial is fixed after only one run of model fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_cross_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscheduler_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-225-addf961f820a>\u001b[0m in \u001b[0;36mrun_cross_validation\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#Standardize according to the fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtraining_folds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Usually setting patience=8\n",
    "today = str(datetime.date.today())[:7] # YYYY-MM\n",
    "\n",
    "for trial in study:\n",
    "    \n",
    "    val_loss = []\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    par = trial.parameters\n",
    "\n",
    "    # Input layer\n",
    "    model.add(Dense(units=par['num_units'], activation=par['activation_1'], input_dim=no_of_features,\n",
    "                   kernel_regularizer=l1_l2(l1=par['l1_reg'], l2=par['l2_reg'])))\n",
    "\n",
    "    # Hidden layers    \n",
    "    for j in range(2, par['model_depth']):\n",
    "        model.add(Dense(units=par['num_units'], activation=par['activation_'+str(j)], \n",
    "                        kernel_regularizer=l1_l2(l1=par['l1_reg'], l2=par['l2_reg'])))\n",
    "        model.add(Dropout(par['dropout'])) #After every hidden layer we (potentially) add a dropout layer\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='linear', \n",
    "                    kernel_regularizer=l1_l2(l1=par['l1_reg'], l2=par['l2_reg'])))\n",
    "\n",
    "    # Optimizer: Adam is relatively robust w.r.t. its beta-parameters \n",
    "    optimizer = Adam(lr=par['lrinit'], epsilon=par['epsilon']) \n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "    # Cross-validate\n",
    "    # Not using the keras_callback here as then the objective of a given trial is fixed after only one run of model fit\n",
    "    for i in range(1):\n",
    "        train_ds, valid_ds = run_cross_validation(i)\n",
    "        history = model.fit(train_ds, epochs=epochs, verbose=2, validation_data=valid_ds, callbacks=[scheduler_callback]) \n",
    "        val_loss.append(np.min(history.history['val_loss']))\n",
    "    \n",
    "    # Using add_observation instead of keras_callback. \n",
    "    # With i = 3\n",
    "#     study.add_observation(trial, objective=np.mean(val_loss), context={'Val-loss First Fold': val_loss[0], \n",
    "#                                                                  'Val-loss Second Fold': val_loss[1], \n",
    "#                                                                  'Val-loss Third Fold': val_loss[2]})\n",
    "    \n",
    "    study.add_observation(trial, objective=np.mean(val_loss))\n",
    "    \n",
    "    study.finalize(trial)\n",
    "    save_model(study, today, OPTIMIZER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds113_kernel",
   "language": "python",
   "name": "clouds113_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
