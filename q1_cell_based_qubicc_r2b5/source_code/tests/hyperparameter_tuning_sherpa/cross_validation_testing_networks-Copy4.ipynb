{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we recover the good results from sherpa_results2021-05-04_adam?\n",
    "\n",
    "Can we achieve a validation error of <40 by using a smaller batch size (i.e. 512)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran with 800GB (750GB should also be fine)\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "#Import sklearn before tensorflow (static Thread-local storage)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "t0 = time.time()\n",
    "path = '/pf/b/b309170'\n",
    "path_figures = path + '/workspace_icon-ml/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/figures'\n",
    "path_model = path + '/workspace_icon-ml/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/saved_models'\n",
    "path_data = path + '/my_work/icon-ml_data/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/based_on_var_interpolated_data'\n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "sys.path.insert(0, path + '/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "# Add sherpa\n",
    "sys.path.insert(0, path + '/my_work/sherpa')\n",
    "\n",
    "import sherpa\n",
    "import sherpa.algorithms.bayesian_optimization as bayesian_optimization\n",
    "\n",
    "# Reloading custom file to incorporate changes dynamically\n",
    "import importlib\n",
    "import my_classes\n",
    "importlib.reload(my_classes)\n",
    "\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import TimeOut\n",
    "\n",
    "import datetime\n",
    "\n",
    "# Minutes per fold\n",
    "timeout = 2120 \n",
    "\n",
    "# For logging purposes\n",
    "days = 'all_days'\n",
    "\n",
    "# Maximum amount of epochs for each model\n",
    "epochs = 50\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 10\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# For store_mean_model_biases\n",
    "VERT_LAYERS = 31\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_visible_devices(gpus[3], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevents crashes of the code\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[0], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow the growth of memory Tensorflow allocates (limits memory usage overall)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_narval = np.load(path_data + '/cloud_cover_input_narval.npy')\n",
    "# input_qubicc = np.load(path_data + '/cloud_cover_input_qubicc.npy')\n",
    "# output_narval = np.load(path_data + '/cloud_cover_output_narval.npy')\n",
    "# output_qubicc = np.load(path_data + '/cloud_cover_output_qubicc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.concatenate((np.load(path_data + '/cloud_cover_input_narval.npy'), \n",
    "                             np.load(path_data + '/cloud_cover_input_qubicc.npy')), axis=0)\n",
    "output_data = np.concatenate((np.load(path_data + '/cloud_cover_output_narval.npy'), \n",
    "                              np.load(path_data + '/cloud_cover_output_qubicc.npy')), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_narval = np.load(path_data + '/cloud_cover_output_narval.npy').shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1008913906, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(samples_total, no_of_features) = input_data.shape\n",
    "(samples_total, no_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Temporal cross-validation*\n",
    "\n",
    "Split into 2-weeks increments (when working with 3 months of data). It's 25 day increments with 5 months of data. <br>\n",
    "1.: Validate on increments 1 and 4 <br>\n",
    "2.: Validate on increments 2 and 5 <br>\n",
    "3.: Validate on increments 3 and 6\n",
    "\n",
    "--> 2/3 training data, 1/3 validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folds = []\n",
    "validation_folds = []\n",
    "two_week_incr = samples_total//6\n",
    "\n",
    "for i in range(3):\n",
    "    # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "    first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "    second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "    validation_folds.append(np.append(first_incr, second_incr))\n",
    "    training_folds.append(np.arange(samples_total))\n",
    "    training_folds[i] = np.delete(training_folds[i], validation_folds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function for the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function for the last layer\n",
    "def my_act_fct(x):\n",
    "    return K.minimum(K.maximum(x, 0), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 3-fold cross-validation\n",
    "\n",
    "Actually only set i=1 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By decreasing timeout we make sure every fold gets the same amount of time\n",
    "# After all, data-loading took some time (Have 3 folds, 60 seconds/minute)\n",
    "# timeout = timeout - 1/3*1/60*(time.time() - t0)\n",
    "timeout = timeout - 1/60*(time.time() - t0)\n",
    "t0 = time.time()\n",
    "\n",
    "#We loop through the folds\n",
    "for i in range(1,2):\n",
    "    \n",
    "    filename = 'cross_validation_cell_based_fold_%d'%(i+1)\n",
    "    \n",
    "    #Standardize according to the fold\n",
    "    scaler.fit(input_data[training_folds[i]])\n",
    "\n",
    "    #Load the data for the respective fold and convert it to tf data\n",
    "    input_train = scaler.transform(input_data[training_folds[i]])\n",
    "    input_valid = scaler.transform(input_data[validation_folds[i]]) \n",
    "    output_train = output_data[training_folds[i]]\n",
    "    output_valid = output_data[validation_folds[i]]\n",
    "    \n",
    "    # Clear memory (Reduces memory requirement to 151 GB)\n",
    "    del input_data, output_data, first_incr, second_incr, validation_folds, training_folds\n",
    "    gc.collect()\n",
    "    \n",
    "    # Column-based: batchsize of 128\n",
    "    # Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "    # I'm not shuffling for hyperparameter tuning\n",
    "    train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                                tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "                .batch(batch_size=512, drop_remainder=True) \\\n",
    "                .prefetch(1)\n",
    "    \n",
    "    # Clear memory\n",
    "    del input_train, output_train\n",
    "    gc.collect()\n",
    "    \n",
    "    # No need to add prefetch.\n",
    "    # tf data with batch_size=10**5 makes the validation evaluation 10 times faster\n",
    "    valid_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_valid), \n",
    "                                tf.data.Dataset.from_tensor_slices(output_valid))) \\\n",
    "                .batch(batch_size=10**5, drop_remainder=True)\n",
    "    \n",
    "    # Clear memory (Reduces memory requirement to 151 GB)\n",
    "    del input_valid, output_valid\n",
    "    gc.collect()\n",
    "    \n",
    "#     #Feed the model\n",
    "#     model.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),\n",
    "#         loss=tf.keras.losses.MeanSquaredError()\n",
    "#     )\n",
    "    \n",
    "#     #Train the model\n",
    "# #     time_callback = TimeOut(t0, timeout*(i+1))\n",
    "#     time_callback = TimeOut(t0, timeout)\n",
    "#     history = model.fit(train_ds, epochs=epochs, verbose=2, validation_data=valid_ds, \n",
    "#                         callbacks=[time_callback])\n",
    "# #     history = model.fit(train_ds, epochs=epochs, validation_data=valid_ds, callbacks=[time_callback])\n",
    "\n",
    "#     #Save the model     \n",
    "#     #Serialize model to YAML\n",
    "#     model_yaml = model.to_yaml()\n",
    "#     with open(os.path.join(path_model, filename+\".yaml\"), \"w\") as yaml_file:\n",
    "#         yaml_file.write(model_yaml)\n",
    "#     #Serialize model and weights to a single HDF5-file\n",
    "#     model.save(os.path.join(path_model, filename+'.h5'), \"w\")\n",
    "#     print('Saved model to disk')\n",
    "    \n",
    "#     #Plot the training history\n",
    "#     if len(history.history['loss']) > len(history.history['val_loss']):\n",
    "#         del history.history['loss'][-1]\n",
    "#     pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "#     plt.grid(True)\n",
    "#     plt.ylabel('Mean Squared Error')\n",
    "#     plt.xlabel('Number of epochs')\n",
    "#     plt.savefig(os.path.join(path_figures, filename+'.pdf'))\n",
    "    \n",
    "#     with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "#         file.write('Results from the %d-th fold\\n'%(i+1))\n",
    "#         file.write('Training epochs: %d\\n'%(len(history.history['val_loss'])))\n",
    "#         file.write('Weights restored from epoch: %d\\n\\n'%(1+np.argmin(history.history['val_loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_num = np.random.randint(500000)\n",
    "# print(random_num)\n",
    "\n",
    "# def save_model(study, today, optimizer):\n",
    "#     path = '/pf/b/b309170/workspace_icon-ml/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/sherpa_results/'+\\\n",
    "#             today+'_'+optimizer+'_'+random_num\n",
    "    \n",
    "#     study.results = study.results[study.results['Status']=='COMPLETED'] #To specify results\n",
    "#     study.results.index = study.results['Trial-ID']  #Trial-ID serves as a better index\n",
    "#     # Remove those hyperparameters that actually do not appear in the model\n",
    "#     for i in range(1, max(study.results['Trial-ID']) + 1):\n",
    "#         depth = study.results.at[i, 'model_depth']\n",
    "#         for j in range(depth, 5): #Or up to 8\n",
    "#             study.results.at[i, 'activation_%d'%j] = None\n",
    "# #             study.results.at[i, 'bn_%d'%j] = None\n",
    "#     # Create the directory and save the SHERPA-output in it\n",
    "#     try:\n",
    "#         os.mkdir(path)\n",
    "#     except OSError:\n",
    "#         print('Creation of the directory %s failed' % path)\n",
    "#     else: \n",
    "#         print('Successfully created the directory %s' % path)\n",
    "#     study.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Good Reference: https://arxiv.org/pdf/1206.5533.pdf (Bengio), https://arxiv.org/pdf/2004.10652.pdf (Ott)\n",
    "# # lrelu = lambda x: relu(x, alpha=0.01)\n",
    "\n",
    "# For Leaky_ReLU:\n",
    "from tensorflow import nn \n",
    "\n",
    "def lrelu(x):\n",
    "    return nn.leaky_relu(x, alpha=0.01)\n",
    "\n",
    "# OPTIMIZER = 'adam'\n",
    "# parameters = [sherpa.Ordinal('num_units', [16, 32, 64, 128, 256]), #No need to vary these per layer. Could add 512.\n",
    "#              sherpa.Discrete('model_depth', [2, 5]), #Originally [2,8] although 8 was never truly tested\n",
    "#              sherpa.Choice('activation_1', ['relu', 'elu', 'tanh', nn.leaky_relu, lrelu]), #Adding SeLU is trickier\n",
    "#              sherpa.Choice('activation_2', ['relu', 'elu', 'tanh', nn.leaky_relu, lrelu]), \n",
    "#              sherpa.Choice('activation_3', ['relu', 'elu', 'tanh', nn.leaky_relu, lrelu]),\n",
    "#              sherpa.Choice('activation_4', ['relu', 'elu', 'tanh', nn.leaky_relu, lrelu]),\n",
    "#              sherpa.Continuous('lrinit', [1e-4, 1e-1], 'log'),\n",
    "#              sherpa.Ordinal('epsilon', [1e-8, 1e-7, 0.1, 1]),\n",
    "#              sherpa.Continuous('dropout', [0., 0.5]),\n",
    "#              sherpa.Continuous('l1_reg', [0, 0.01]),\n",
    "#              sherpa.Continuous('l2_reg', [0, 0.01])]\n",
    "\n",
    "              \n",
    "# #              sherpa.Ordinal('bn_1', [0, 1]),\n",
    "# #              sherpa.Ordinal('bn_2', [0, 1]),\n",
    "# #              sherpa.Ordinal('bn_3', [0, 1]),\n",
    "# #              sherpa.Ordinal('bn_4', [0, 1]),\n",
    "# #              sherpa.Ordinal('bn_5', [0, 1]),\n",
    "# #              sherpa.Ordinal('bn_6', [0, 1]),\n",
    "# #              sherpa.Ordinal('bn_7', [0, 1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # max_num_trials is left unspecified, so the optimization will run until the end of the job-runtime\n",
    "\n",
    "# good_hyperparams = pd.DataFrame({'num_units': [256], 'model_depth': [3], 'activation_1': [lrelu], 'activation_2':[lrelu],\n",
    "#                    'activation_3':['relu'], 'activation_4':['relu'], 'activation_5':['relu'], 'activation_6':['relu'],\n",
    "#                    'activation_7':['relu'], 'lrinit':[0.008725626554323051], 'epsilon':[0.1], 'dropout':[0.1841244119677411], \n",
    "#                                  'l1_reg':[0.00016220861742929693], 'l2_reg':[0.007436944699610299]})\n",
    "\n",
    "# # I expect an objective of around 61.\n",
    "\n",
    "# alg = bayesian_optimization.GPyOpt(initial_data_points=good_hyperparams)\n",
    "\n",
    "# # alg = bayesian_optimization.GPyOpt() \n",
    "# study = sherpa.Study(parameters=parameters, algorithm=alg, lower_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting only with a few epochs\n",
    "epochs = 3\n",
    "\n",
    "# Usually setting patience=8\n",
    "today = str(str(datetime.date.today())[:7]) # YYYY-MM\n",
    "\n",
    "par = {'num_units': 256, 'model_depth': 3, 'activation_1': lrelu, 'activation_2': lrelu, \n",
    "       'lrinit': 0.008726, 'epsilon': 0.1, 'dropout': 0.184124,\n",
    "       'l1_reg': 0.000162, 'l2_reg': 0.007437}\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(units=par['num_units'], activation=par['activation_1'], input_dim=no_of_features,\n",
    "               kernel_regularizer=l1_l2(l1=par['l1_reg'], l2=par['l2_reg'])))\n",
    "#     if (par['bn_1']==1):\n",
    "#         model.add(BatchNormalization()) #There's some debate on whether to use it before or after the activation fct\n",
    "\n",
    "# Hidden layers    \n",
    "for j in range(2, par['model_depth']):\n",
    "    model.add(Dense(units=par['num_units'], activation=par['activation_'+str(j)], \n",
    "                    kernel_regularizer=l1_l2(l1=par['l1_reg'], l2=par['l2_reg'])))\n",
    "    model.add(Dropout(par['dropout'])) #After every hidden layer we (potentially) add a dropout layer\n",
    "#         if (par['bn_'+str(j)]==1):\n",
    "#             model.add(BatchNormalization())\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='linear', \n",
    "                kernel_regularizer=l1_l2(l1=par['l1_reg'], l2=par['l2_reg'])))\n",
    "\n",
    "# Optimizer: Adam is relatively robust w.r.t. its beta-parameters \n",
    "optimizer = Adam(lr=par['lrinit'], epsilon=par['epsilon']) \n",
    "model.compile(loss='mse', optimizer=optimizer)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_ds, epochs=epochs, verbose=1, validation_data=valid_ds) ## 3 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning rate scheduler**\n",
    "\n",
    "We should add it to Sherpa when we test more than 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scheduler(epoch, lr):\n",
    "#     if epoch < 10:\n",
    "#         return lr\n",
    "#     else:\n",
    "#         return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "# callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "    \n",
    "# model = tf.keras.Sequential(\n",
    "#             [\n",
    "#                 tf.keras.layers.Dense(256, activation='relu', input_dim = no_of_features),\n",
    "#                 tf.keras.layers.Dense(256, activation='relu'),\n",
    "#                 tf.keras.layers.Dense(1, activation=my_act_fct, dtype='float32'),\n",
    "#             ],\n",
    "#             name=\"cell_based_model\",\n",
    "#         )\n",
    "\n",
    "# #Feed the model\n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(learning_rate=0.02),\n",
    "#     loss=tf.keras.losses.MeanSquaredError()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(train_ds, epochs=5, verbose=2, callbacks=[callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds113_kernel",
   "language": "python",
   "name": "clouds113_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
