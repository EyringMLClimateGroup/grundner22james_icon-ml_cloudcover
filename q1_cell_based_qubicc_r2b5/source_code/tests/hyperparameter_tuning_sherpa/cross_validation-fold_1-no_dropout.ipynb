{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "1. We read the data from the npy files\n",
    "2. We combine the QUBICC and NARVAL data\n",
    "4. Set up cross validation\n",
    "\n",
    "During cross-validation:\n",
    "\n",
    "1. We scale the data, convert to tf data\n",
    "2. Plot training progress, model biases \n",
    "3. Write losses and epochs into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran with 800GB (750GB should also be fine)\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "#Import sklearn before tensorflow (static Thread-local storage)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "# For Leaky_ReLU:\n",
    "from tensorflow import nn \n",
    "\n",
    "t0 = time.time()\n",
    "path = '/pf/b/b309170'\n",
    "\n",
    "qubicc_only = False\n",
    "if qubicc_only:\n",
    "    path_figures = path + '/workspace_icon-ml/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/qubicc_only/figures'\n",
    "    path_model = path + '/workspace_icon-ml/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/qubicc_only/saved_models'\n",
    "else:\n",
    "    path_figures = path + '/workspace_icon-ml/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/figures'\n",
    "    path_model = path + '/workspace_icon-ml/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/saved_models'\n",
    "\n",
    "path_data = path + '/my_work/icon-ml_data/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/based_on_var_interpolated_data'\n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "sys.path.insert(0, path + '/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "\n",
    "# Reloading custom file to incorporate changes dynamically\n",
    "import importlib\n",
    "import my_classes\n",
    "importlib.reload(my_classes)\n",
    "\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import TimeOut\n",
    "\n",
    "# Minutes per fold\n",
    "timeout = 2120 \n",
    "\n",
    "# For logging purposes\n",
    "days = 'all_days'\n",
    "\n",
    "# Maximum amount of epochs for each model\n",
    "# epochs = 30 ##\n",
    "epochs = 100 ## Just for this qubicc_only run\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 10\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# For store_mean_model_biases\n",
    "VERT_LAYERS = 31\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_visible_devices(gpus[3], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't run on a CPU node\n",
    "try:\n",
    "    # Prevents crashes of the code\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "    # Allow the growth of memory Tensorflow allocates (limits memory usage overall)\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_narval = np.load(path_data + '/cloud_cover_input_narval.npy')\n",
    "# input_qubicc = np.load(path_data + '/cloud_cover_input_qubicc.npy')\n",
    "# output_narval = np.load(path_data + '/cloud_cover_output_narval.npy')\n",
    "# output_qubicc = np.load(path_data + '/cloud_cover_output_qubicc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.concatenate((np.load(path_data + '/cloud_cover_input_narval.npy'), \n",
    "                             np.load(path_data + '/cloud_cover_input_qubicc.npy')), axis=0)\n",
    "output_data = np.concatenate((np.load(path_data + '/cloud_cover_output_narval.npy'), \n",
    "                              np.load(path_data + '/cloud_cover_output_qubicc.npy')), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_narval = np.load(path_data + '/cloud_cover_output_narval.npy').shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if qubicc_only:\n",
    "    input_data = input_data[samples_narval:]\n",
    "    output_data = output_data[samples_narval:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1008901640, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(samples_total, no_of_features) = input_data.shape\n",
    "(samples_total, no_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick random subset of data to make testing 800 times faster\n",
    "np.random.seed(10)\n",
    "ind = np.random.randint(samples_total, size=(10**7)) ## Was 10**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = input_data[ind]\n",
    "output_data = output_data[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(samples_total, no_of_features) = input_data.shape\n",
    "(samples_total, no_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Temporal cross-validation*\n",
    "\n",
    "Split into 2-weeks increments (when working with 3 months of data). It's 25 day increments with 5 months of data. <br>\n",
    "1.: Validate on increments 1 and 4 <br>\n",
    "2.: Validate on increments 2 and 5 <br>\n",
    "3.: Validate on increments 3 and 6\n",
    "\n",
    "--> 2/3 training data, 1/3 validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folds = []\n",
    "validation_folds = []\n",
    "two_week_incr = samples_total//6\n",
    "\n",
    "for i in range(3):\n",
    "    # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "    first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "    second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "    validation_folds.append(np.append(first_incr, second_incr))\n",
    "    training_folds.append(np.arange(samples_total))\n",
    "    training_folds[i] = np.delete(training_folds[i], validation_folds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function for the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Dense(units=64, activation='tanh', input_dim=no_of_features, \n",
    "                kernel_regularizer=l1_l2(l1=0.004749, l2=0.008732)))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(units=64, activation=nn.leaky_relu, kernel_regularizer=l1_l2(l1=0.004749, l2=0.008732)))\n",
    "# model.add(Dropout(0.221)) # We drop 18% of the hidden nodes\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Third hidden layer\n",
    "model.add(Dense(units=64, activation='tanh', kernel_regularizer=l1_l2(l1=0.004749, l2=0.008732)))\n",
    "# model.add(Dropout(0.221)) # We drop 18% of the hidden nodes\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='linear', kernel_regularizer=l1_l2(l1=0.004749, l2=0.008732)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce lr every two epochs, starting at the second epoch\n",
    "def scheduler_stephan(epoch, lr):\n",
    "    if epoch > 0 and epoch%2==0:\n",
    "        return lr/20\n",
    "    else:\n",
    "        return lr\n",
    "    \n",
    "callback_stephan = tf.keras.callbacks.LearningRateScheduler(scheduler_stephan, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This would be the loss of a NN which outputs zeros everywhere\n",
    "# np.mean(np.array(output_data)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This would be the loss of a NN which outputs the best constant value everywhere\n",
    "# constant_mean = np.mean(np.array(output_data))\n",
    "# np.mean((np.array(output_data) - constant_mean)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeing up memory (~46 GB). Memory usage after this cell: 251 GB\n",
    "# del input_narval, input_qubicc, output_narval, output_qubicc\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 3-fold cross-validation\n",
    "\n",
    "When the training is lost in a local minimum, often a re-run helps with a different initialization of the model weights.\n",
    "Or possibly a different shuffling seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6485/6485 - 34s - loss: 100.7673 - val_loss: 59.9475\n",
      "Epoch 2/100\n",
      "6485/6485 - 32s - loss: 57.0345 - val_loss: 53.0149\n",
      "Epoch 3/100\n",
      "6485/6485 - 32s - loss: 53.7376 - val_loss: 52.8864\n",
      "Epoch 4/100\n",
      "6485/6485 - 31s - loss: 51.8410 - val_loss: 50.2802\n",
      "Epoch 5/100\n",
      "6485/6485 - 32s - loss: 50.4044 - val_loss: 48.6273\n",
      "Epoch 6/100\n",
      "6485/6485 - 31s - loss: 49.3032 - val_loss: 46.7706\n",
      "Epoch 7/100\n",
      "6485/6485 - 31s - loss: 48.3289 - val_loss: 46.2113\n",
      "Epoch 8/100\n",
      "6485/6485 - 31s - loss: 47.4847 - val_loss: 46.4810\n",
      "Epoch 9/100\n",
      "6485/6485 - 32s - loss: 46.7782 - val_loss: 45.6559\n",
      "Epoch 10/100\n",
      "6485/6485 - 32s - loss: 46.0882 - val_loss: 43.6707\n",
      "Epoch 11/100\n",
      "6485/6485 - 33s - loss: 45.5116 - val_loss: 44.1241\n",
      "Epoch 12/100\n",
      "6485/6485 - 32s - loss: 44.8786 - val_loss: 42.9333\n",
      "Epoch 13/100\n",
      "6485/6485 - 32s - loss: 44.3463 - val_loss: 44.6832\n",
      "Epoch 14/100\n",
      "6485/6485 - 33s - loss: 43.7696 - val_loss: 41.9417\n",
      "Epoch 15/100\n",
      "6485/6485 - 32s - loss: 43.2679 - val_loss: 48.1100\n",
      "Epoch 16/100\n",
      "6485/6485 - 32s - loss: 42.7725 - val_loss: 41.1291\n",
      "Epoch 17/100\n",
      "6485/6485 - 32s - loss: 42.2727 - val_loss: 42.3815\n",
      "Epoch 18/100\n",
      "6485/6485 - 32s - loss: 41.8285 - val_loss: 40.5773\n",
      "Epoch 19/100\n",
      "6485/6485 - 33s - loss: 41.3945 - val_loss: 40.6505\n",
      "Epoch 20/100\n",
      "6485/6485 - 33s - loss: 40.9933 - val_loss: 42.0842\n",
      "Epoch 21/100\n",
      "6485/6485 - 32s - loss: 40.6425 - val_loss: 40.8311\n",
      "Epoch 22/100\n",
      "6485/6485 - 32s - loss: 40.3333 - val_loss: 38.8229\n",
      "Epoch 23/100\n",
      "6485/6485 - 32s - loss: 39.9745 - val_loss: 39.4169\n",
      "Epoch 24/100\n",
      "6485/6485 - 32s - loss: 39.7631 - val_loss: 41.2250\n",
      "Epoch 25/100\n",
      "6485/6485 - 32s - loss: 39.4150 - val_loss: 38.6991\n",
      "Epoch 26/100\n",
      "6485/6485 - 32s - loss: 39.0938 - val_loss: 47.3566\n",
      "Epoch 27/100\n",
      "6485/6485 - 32s - loss: 38.8882 - val_loss: 38.2097\n",
      "Epoch 28/100\n",
      "6485/6485 - 32s - loss: 38.6102 - val_loss: 50.2627\n",
      "Epoch 29/100\n",
      "6485/6485 - 32s - loss: 38.3572 - val_loss: 41.6449\n",
      "Epoch 30/100\n",
      "6485/6485 - 32s - loss: 38.1475 - val_loss: 36.9887\n",
      "Epoch 31/100\n",
      "6485/6485 - 32s - loss: 37.8995 - val_loss: 38.3920\n",
      "Epoch 32/100\n",
      "6485/6485 - 33s - loss: 37.7394 - val_loss: 36.6385\n",
      "Epoch 33/100\n",
      "6485/6485 - 32s - loss: 37.5876 - val_loss: 50.8706\n",
      "Epoch 34/100\n",
      "6485/6485 - 32s - loss: 37.3877 - val_loss: 37.8913\n",
      "Epoch 35/100\n",
      "6485/6485 - 32s - loss: 37.2130 - val_loss: 38.4131\n",
      "Epoch 36/100\n",
      "6485/6485 - 32s - loss: 37.0474 - val_loss: 36.7358\n",
      "Epoch 37/100\n",
      "6485/6485 - 32s - loss: 36.9194 - val_loss: 37.1788\n",
      "Epoch 38/100\n",
      "6485/6485 - 32s - loss: 36.7827 - val_loss: 36.9901\n",
      "Epoch 39/100\n",
      "6485/6485 - 33s - loss: 36.6480 - val_loss: 35.3224\n",
      "Epoch 40/100\n",
      "6485/6485 - 33s - loss: 36.5097 - val_loss: 35.9307\n",
      "Epoch 41/100\n",
      "6485/6485 - 32s - loss: 36.4045 - val_loss: 36.6706\n",
      "Epoch 42/100\n",
      "6485/6485 - 33s - loss: 36.2644 - val_loss: 41.3290\n",
      "Epoch 43/100\n",
      "6485/6485 - 32s - loss: 36.1497 - val_loss: 37.8583\n",
      "Epoch 44/100\n",
      "6485/6485 - 33s - loss: 36.0546 - val_loss: 35.6626\n",
      "Epoch 45/100\n",
      "6485/6485 - 33s - loss: 35.9272 - val_loss: 35.7756\n",
      "Epoch 46/100\n",
      "6485/6485 - 32s - loss: 35.8384 - val_loss: 37.7462\n",
      "Epoch 47/100\n",
      "6485/6485 - 33s - loss: 35.7354 - val_loss: 34.3841\n",
      "Epoch 48/100\n",
      "6485/6485 - 33s - loss: 35.6378 - val_loss: 34.8293\n",
      "Epoch 49/100\n",
      "6485/6485 - 32s - loss: 35.5538 - val_loss: 35.1266\n",
      "Epoch 50/100\n",
      "6485/6485 - 32s - loss: 35.4507 - val_loss: 39.1602\n",
      "Epoch 51/100\n",
      "6485/6485 - 33s - loss: 35.3714 - val_loss: 35.6328\n",
      "Epoch 52/100\n",
      "6485/6485 - 31s - loss: 35.3145 - val_loss: 38.6606\n",
      "Epoch 53/100\n",
      "6485/6485 - 32s - loss: 35.2011 - val_loss: 34.0668\n",
      "Epoch 54/100\n",
      "6485/6485 - 33s - loss: 35.1329 - val_loss: 37.2829\n",
      "Epoch 55/100\n",
      "6485/6485 - 32s - loss: 35.0593 - val_loss: 37.6887\n",
      "Epoch 56/100\n",
      "6485/6485 - 33s - loss: 34.9883 - val_loss: 34.9635\n",
      "Epoch 57/100\n",
      "6485/6485 - 32s - loss: 34.9678 - val_loss: 37.5977\n",
      "Epoch 58/100\n",
      "6485/6485 - 33s - loss: 34.8732 - val_loss: 34.4541\n",
      "Epoch 59/100\n",
      "6485/6485 - 32s - loss: 34.8252 - val_loss: 47.6004\n",
      "Epoch 60/100\n",
      "6485/6485 - 33s - loss: 34.7586 - val_loss: 33.4540\n",
      "Epoch 61/100\n",
      "6485/6485 - 32s - loss: 34.6492 - val_loss: 36.8582\n",
      "Epoch 62/100\n",
      "6485/6485 - 32s - loss: 34.5846 - val_loss: 36.6262\n",
      "Epoch 63/100\n",
      "6485/6485 - 32s - loss: 34.5637 - val_loss: 33.1110\n",
      "Epoch 64/100\n",
      "6485/6485 - 32s - loss: 34.5147 - val_loss: 35.4021\n",
      "Epoch 65/100\n",
      "6485/6485 - 32s - loss: 34.4608 - val_loss: 34.2494\n",
      "Epoch 66/100\n",
      "6485/6485 - 32s - loss: 34.3473 - val_loss: 42.3084\n",
      "Epoch 67/100\n",
      "6485/6485 - 33s - loss: 34.3254 - val_loss: 36.4321\n",
      "Epoch 68/100\n",
      "6485/6485 - 33s - loss: 34.2927 - val_loss: 36.8080\n",
      "Epoch 69/100\n",
      "6485/6485 - 32s - loss: 34.2195 - val_loss: 45.1395\n",
      "Epoch 70/100\n",
      "6485/6485 - 32s - loss: 34.2042 - val_loss: 36.5575\n",
      "Epoch 71/100\n",
      "6485/6485 - 32s - loss: 34.1269 - val_loss: 34.0521\n",
      "Epoch 72/100\n",
      "6485/6485 - 31s - loss: 34.0907 - val_loss: 35.5286\n",
      "Epoch 73/100\n",
      "6485/6485 - 32s - loss: 34.0330 - val_loss: 46.1058\n",
      "Epoch 74/100\n",
      "6485/6485 - 33s - loss: 33.9847 - val_loss: 33.8839\n",
      "Epoch 75/100\n",
      "6485/6485 - 33s - loss: 33.9920 - val_loss: 36.3374\n",
      "Epoch 76/100\n"
     ]
    }
   ],
   "source": [
    "# By decreasing timeout we make sure every fold gets the same amount of time\n",
    "# After all, data-loading took some time (Have 3 folds, 60 seconds/minute)\n",
    "# timeout = timeout - 1/3*1/60*(time.time() - t0)\n",
    "timeout = timeout - 1/60*(time.time() - t0)\n",
    "t0 = time.time()\n",
    "\n",
    "#We loop through the folds\n",
    "for i in range(0,1):\n",
    "    \n",
    "    filename = 'cross_validation_cell_based_fold_%d_no_dropout'%(i+1)\n",
    "    \n",
    "    #Standardize according to the fold\n",
    "    scaler.fit(input_data[training_folds[i]])\n",
    "\n",
    "    #Load the data for the respective fold and convert it to tf data\n",
    "    input_train = scaler.transform(input_data[training_folds[i]])\n",
    "    input_valid = scaler.transform(input_data[validation_folds[i]]) \n",
    "    output_train = output_data[training_folds[i]]\n",
    "    output_valid = output_data[validation_folds[i]]\n",
    "    \n",
    "    # Clear memory (Reduces memory requirement to 151 GB)\n",
    "    del input_data, output_data, first_incr, second_incr, validation_folds, training_folds\n",
    "    gc.collect()\n",
    "    \n",
    "    # Column-based: batchsize of 128\n",
    "    # Cell-based: batchsize of at least 512\n",
    "    # Shuffle is actually very important because we start off with the uppermost layers with clc=0 basically throughout\n",
    "    # This can push us into a local minimum, preferrably yielding clc=0.\n",
    "    # The size of the shuffle buffer significantly impacts RAM requirements! Do not increase to above 10000.\n",
    "    # Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "    # We might want to cache before shuffling, however it seems to slow down training\n",
    "    # We do not repeat after shuffle, because the validation set should be evaluated after each epoch\n",
    "    train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                                tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "                .shuffle(10**5, seed=seed) \\\n",
    "                .batch(batch_size=1028, drop_remainder=True) \\\n",
    "                .prefetch(1)\n",
    "    \n",
    "    # Clear memory\n",
    "    del input_train, output_train\n",
    "    gc.collect()\n",
    "    \n",
    "    # No need to add prefetch.\n",
    "    # tf data with batch_size=10**5 makes the validation evaluation 10 times faster\n",
    "    valid_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_valid), \n",
    "                                tf.data.Dataset.from_tensor_slices(output_valid))) \\\n",
    "                .batch(batch_size=10**5, drop_remainder=True)\n",
    "    \n",
    "    # Clear memory (Reduces memory requirement to 151 GB)\n",
    "    del input_valid, output_valid\n",
    "    gc.collect()\n",
    "    \n",
    "    #Feed the model. Increase the learning rate by a factor of 2 when increasing the batch size by a factor of 4\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adadelta(learning_rate=0.000433, epsilon=0.1),\n",
    "        loss=tf.keras.losses.MeanSquaredError()\n",
    "    )\n",
    "    \n",
    "    #Train the model\n",
    "#     time_callback = TimeOut(t0, timeout*(i+1))\n",
    "    time_callback = TimeOut(t0, timeout)\n",
    "    history = model.fit(train_ds, validation_data=valid_ds, epochs=epochs, verbose=2)\n",
    "#     history = model.fit(train_ds, epochs=epochs, validation_data=valid_ds, callbacks=[time_callback])\n",
    "\n",
    "    #Save the model     \n",
    "    #Serialize model to YAML\n",
    "    model_yaml = model.to_yaml()\n",
    "    with open(os.path.join(path_model, filename+\".yaml\"), \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "    #Serialize model and weights to a single HDF5-file\n",
    "    model.save(os.path.join(path_model, filename+'.h5'), \"w\")\n",
    "    print('Saved model to disk')\n",
    "    \n",
    "    #Plot the training history\n",
    "    if len(history.history['loss']) > len(history.history['val_loss']):\n",
    "        del history.history['loss'][-1]\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "    plt.grid(True)\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.savefig(os.path.join(path_figures, filename+'.pdf'))\n",
    "    \n",
    "    with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "        file.write('Results from the %d-th fold\\n'%(i+1))\n",
    "        file.write('Training epochs: %d\\n'%(len(history.history['val_loss'])))\n",
    "        file.write('Weights restored from epoch: %d\\n\\n'%(1+np.argmin(history.history['val_loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (based on the module python3/2022.01)",
   "language": "python",
   "name": "python3_2022_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
