{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests concerning the speed of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ran with 800GB (750GB should also be fine)\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "#Import sklearn before tensorflow (static Thread-local storage)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "# For Leaky_ReLU:\n",
    "from tensorflow import nn \n",
    "\n",
    "t0 = time.time()\n",
    "path = '/pf/b/b309170'\n",
    "path_figures = path + '/workspace_icon-ml/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/figures'\n",
    "path_model = path + '/workspace_icon-ml/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/saved_models'\n",
    "path_data = path + '/my_work/icon-ml_data/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/based_on_var_interpolated_data'\n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "sys.path.insert(0, path + '/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "\n",
    "# Reloading custom file to incorporate changes dynamically\n",
    "import importlib\n",
    "import my_classes\n",
    "importlib.reload(my_classes)\n",
    "\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import TimeOut\n",
    "\n",
    "# Minutes per fold\n",
    "timeout = 2120 \n",
    "\n",
    "# For logging purposes\n",
    "days = 'all_days'\n",
    "\n",
    "# Maximum amount of epochs for each model\n",
    "epochs = 30\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 10\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# For store_mean_model_biases\n",
    "VERT_LAYERS = 31\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_visible_devices(gpus[3], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevents crashes of the code\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[0], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow the growth of memory Tensorflow allocates (limits memory usage overall)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_narval = np.load(path_data + '/cloud_cover_input_narval.npy')\n",
    "# input_qubicc = np.load(path_data + '/cloud_cover_input_qubicc.npy')\n",
    "# output_narval = np.load(path_data + '/cloud_cover_output_narval.npy')\n",
    "# output_qubicc = np.load(path_data + '/cloud_cover_output_qubicc.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.concatenate((np.load(path_data + '/cloud_cover_input_narval.npy'), \n",
    "                             np.load(path_data + '/cloud_cover_input_qubicc.npy')), axis=0)\n",
    "output_data = np.concatenate((np.load(path_data + '/cloud_cover_output_narval.npy'), \n",
    "                              np.load(path_data + '/cloud_cover_output_qubicc.npy')), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_narval = np.load(path_data + '/cloud_cover_output_narval.npy').shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(samples_total, no_of_features) = input_data.shape\n",
    "(samples_total, no_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Temporal cross-validation*\n",
    "\n",
    "Split into 2-weeks increments (when working with 3 months of data). It's 25 day increments with 5 months of data. <br>\n",
    "1.: Validate on increments 1 and 4 <br>\n",
    "2.: Validate on increments 2 and 5 <br>\n",
    "3.: Validate on increments 3 and 6\n",
    "\n",
    "--> 2/3 training data, 1/3 validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folds = []\n",
    "validation_folds = []\n",
    "two_week_incr = samples_total//6\n",
    "\n",
    "for i in range(3):\n",
    "    # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "    first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "    second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "    validation_folds.append(np.append(first_incr, second_incr))\n",
    "    training_folds.append(np.arange(samples_total))\n",
    "    training_folds[i] = np.delete(training_folds[i], validation_folds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function for the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(x):\n",
    "    return nn.leaky_relu(x, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Dense(units=256, activation=lrelu, input_dim=no_of_features, \n",
    "                kernel_regularizer=l1_l2(l1=0.000162, l2=0.007437)))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(units=256, activation=lrelu, kernel_regularizer=l1_l2(l1=0.000162, l2=0.007437)))\n",
    "model.add(Dropout(0.184124)) # We drop 18% of the hidden nodes\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='linear', kernel_regularizer=l1_l2(l1=0.000162, l2=0.007437)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This would be the loss of a NN which outputs zeros everywhere\n",
    "# np.mean(np.array(output_data)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This would be the loss of a NN which outputs the best constant value everywhere\n",
    "# constant_mean = np.mean(np.array(output_data))\n",
    "# np.mean((np.array(output_data) - constant_mean)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Freeing up memory (~46 GB). Memory usage after this cell: 251 GB\n",
    "# del input_narval, input_qubicc, output_narval, output_qubicc\n",
    "\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 3-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By decreasing timeout we make sure every fold gets the same amount of time\n",
    "# After all, data-loading took some time (Have 3 folds, 60 seconds/minute)\n",
    "# timeout = timeout - 1/3*1/60*(time.time() - t0)\n",
    "timeout = timeout - 1/60*(time.time() - t0)\n",
    "t0 = time.time()\n",
    "\n",
    "#We loop through the folds\n",
    "for i in range(1,2):\n",
    "    \n",
    "    filename = 'cross_validation_cell_based_fold_%d'%(i+1)\n",
    "    \n",
    "    #Standardize according to the fold\n",
    "    scaler.fit(input_data[training_folds[i]])\n",
    "\n",
    "    #Load the data for the respective fold and convert it to tf data\n",
    "    input_train = scaler.transform(input_data[training_folds[i]])\n",
    "    input_valid = scaler.transform(input_data[validation_folds[i]]) \n",
    "    output_train = output_data[training_folds[i]]\n",
    "    output_valid = output_data[validation_folds[i]]\n",
    "    \n",
    "    # Clear memory (Reduces memory requirement to 151 GB)\n",
    "    del input_data, output_data, first_incr, second_incr, validation_folds, training_folds\n",
    "    gc.collect()\n",
    "    \n",
    "#     # Column-based: batchsize of 128\n",
    "#     # Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "#     train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "#                                 tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "#                 .shuffle(1000, seed=seed) \\\n",
    "#                 .batch(batch_size=1028, drop_remainder=True) \\\n",
    "#                 .prefetch(1)\n",
    "    \n",
    "#     # Clear memory\n",
    "#     del input_train, output_train\n",
    "#     gc.collect()\n",
    "    \n",
    "#     # No need to add prefetch.\n",
    "#     # tf data with batch_size=10**5 makes the validation evaluation 10 times faster\n",
    "#     valid_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_valid), \n",
    "#                                 tf.data.Dataset.from_tensor_slices(output_valid))) \\\n",
    "#                 .batch(batch_size=10**5, drop_remainder=True)\n",
    "    \n",
    "#     # Clear memory (Reduces memory requirement to 151 GB)\n",
    "#     del input_valid, output_valid\n",
    "#     gc.collect()\n",
    "    \n",
    "    #Feed the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.008726, epsilon=0.1),\n",
    "        loss=tf.keras.losses.MeanSquaredError()\n",
    "    )\n",
    "    \n",
    "#     #Train the model\n",
    "# #     time_callback = TimeOut(t0, timeout*(i+1))\n",
    "#     time_callback = TimeOut(t0, timeout)\n",
    "#     history = model.fit(train_ds, epochs=epochs, verbose=2, validation_data=valid_ds, \n",
    "#                         callbacks=[time_callback])\n",
    "# #     history = model.fit(train_ds, epochs=epochs, validation_data=valid_ds, callbacks=[time_callback])\n",
    "\n",
    "#     #Save the model     \n",
    "#     #Serialize model to YAML\n",
    "#     model_yaml = model.to_yaml()\n",
    "#     with open(os.path.join(path_model, filename+\".yaml\"), \"w\") as yaml_file:\n",
    "#         yaml_file.write(model_yaml)\n",
    "#     #Serialize model and weights to a single HDF5-file\n",
    "#     model.save(os.path.join(path_model, filename+'.h5'), \"w\")\n",
    "#     print('Saved model to disk')\n",
    "    \n",
    "#     #Plot the training history\n",
    "#     if len(history.history['loss']) > len(history.history['val_loss']):\n",
    "#         del history.history['loss'][-1]\n",
    "#     pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "#     plt.grid(True)\n",
    "#     plt.ylabel('Mean Squared Error')\n",
    "#     plt.xlabel('Number of epochs')\n",
    "#     plt.savefig(os.path.join(path_figures, filename+'.pdf'))\n",
    "    \n",
    "#     with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "#         file.write('Results from the %d-th fold\\n'%(i+1))\n",
    "#         file.write('Training epochs: %d\\n'%(len(history.history['val_loss'])))\n",
    "#         file.write('Weights restored from epoch: %d\\n\\n'%(1+np.argmin(history.history['val_loss'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with 800 Mio samples (three-hourly QUBICC data and stricter class balancing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-based: batchsize of 128\n",
    "# Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(100000, seed=seed) \\\n",
    "            .batch(batch_size=1028, drop_remainder=True) \\\n",
    "            .prefetch(1)\n",
    "\n",
    "# shuffle 1000 and bs 2056 increases...\n",
    "# shuffle 1000 and bs 1028 increases... but it starts to decrease after a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "654289/654289 [==============================] - 2060s 3ms/step - loss: 451.2101\n",
      "Epoch 2/5\n",
      "654289/654289 [==============================] - 2099s 3ms/step - loss: 451.2089\n",
      "Epoch 3/5\n",
      "654289/654289 [==============================] - 2095s 3ms/step - loss: 451.2076\n",
      "Epoch 4/5\n",
      "654289/654289 [==============================] - 2069s 3ms/step - loss: 451.2128\n",
      "Epoch 5/5\n",
      "161484/654289 [======>.......................] - ETA: 25:57 - loss: 107.0768"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-based: batchsize of 128\n",
    "# Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(1000000, seed=seed) \\\n",
    "            .batch(batch_size=1028, drop_remainder=True) \\\n",
    "            .prefetch(1)\n",
    "\n",
    "# shuffle 1000 and bs 2056 increases...\n",
    "# shuffle 1000 and bs 1028 increases... but it starts to decrease after a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "654289/654289 [==============================] - 2832s 4ms/step - loss: 451.2174\n",
      "Epoch 2/5\n",
      "654289/654289 [==============================] - 2839s 4ms/step - loss: 451.2190\n",
      "Epoch 3/5\n",
      "654289/654289 [==============================] - 2898s 4ms/step - loss: 451.2184\n",
      "Epoch 4/5\n",
      "654289/654289 [==============================] - 2821s 4ms/step - loss: 451.2192\n",
      "Epoch 5/5\n",
      "654289/654289 [==============================] - 2839s 4ms/step - loss: 451.2195\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-based: batchsize of 128\n",
    "# Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(100000, seed=seed) \\\n",
    "            .batch(batch_size=512, drop_remainder=True) \\\n",
    "            .prefetch(1)\n",
    "\n",
    "# shuffle 1000 and bs 2056 increases...\n",
    "# shuffle 1000 and bs 1028 increases... but it starts to decrease after a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 222813/1313689 [====>.........................] - ETA: 55:20 - loss: 215.5292"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with two-hourly QUBICC data is quite infeasible:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size of 1028 is faster than one of 2048!\n",
    "So batch size of 1028 is best. A shuffle buffer of 100000 is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-based: batchsize of 128\n",
    "# Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(100000, seed=seed) \\\n",
    "            .batch(batch_size=1028, drop_remainder=True) \\\n",
    "            .prefetch(1)\n",
    "\n",
    "# shuffle 1000 and bs 2056 increases...\n",
    "# shuffle 1000 and bs 1028 increases... but it starts to decrease after a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1226713/1226713 [==============================] - 4056s 3ms/step - loss: 231.7048\n",
      "Epoch 2/5\n",
      "1226713/1226713 [==============================] - 4068s 3ms/step - loss: 231.7042\n",
      "Epoch 3/5\n",
      " 241540/1226713 [====>.........................] - ETA: 54:04 - loss: 120.0847"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0f05ee6c1374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/b309170/conda/envs/clouds113/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/b309170/conda/envs/clouds113/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/b309170/conda/envs/clouds113/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/b309170/conda/envs/clouds113/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/b309170/conda/envs/clouds113/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/b309170/conda/envs/clouds113/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/work/b309170/conda/envs/clouds113/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-based: batchsize of 128\n",
    "# Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(100000, seed=200) \\\n",
    "            .batch(batch_size=1028, drop_remainder=True) \\\n",
    "            .prefetch(1)\n",
    "\n",
    "# shuffle 1000 and bs 2056 increases...\n",
    "# shuffle 1000 and bs 1028 increases... but it starts to decrease after a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1226713/1226713 [==============================] - 4069s 3ms/step - loss: 653.1688\n",
      "Epoch 2/5\n",
      "1226713/1226713 [==============================] - 4069s 3ms/step - loss: 653.1722\n",
      "Epoch 3/5\n",
      "1226713/1226713 [==============================] - 4069s 3ms/step - loss: 653.1620\n",
      "Epoch 4/5\n",
      "1226713/1226713 [==============================] - 4083s 3ms/step - loss: 653.1627\n",
      "Epoch 5/5\n",
      " 411624/1226713 [=========>....................] - ETA: 45:12 - loss: 70.7281"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle after every epoch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(100000, seed=seed, reshuffle_each_iteration=True) \\\n",
    "            .batch(batch_size=1028, drop_remainder=True) \\\n",
    "            .prefetch(1)\n",
    "\n",
    "# shuffle 1000 and bs 2056 increases...\n",
    "# shuffle 1000 and bs 1028 increases... but it starts to decrease after a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1226713/1226713 [==============================] - 4083s 3ms/step - loss: 231.7048\n",
      "Epoch 2/5\n",
      "1226713/1226713 [==============================] - 4081s 3ms/step - loss: 231.7042\n",
      "Epoch 3/5\n",
      "1226713/1226713 [==============================] - 4091s 3ms/step - loss: 231.7020\n",
      "Epoch 4/5\n",
      "1226713/1226713 [==============================] - 4109s 3ms/step - loss: 231.7028\n",
      "Epoch 5/5\n",
      "1226713/1226713 [==============================] - 4083s 3ms/step - loss: 231.7029\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle after every epoch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(100000, seed=200, reshuffle_each_iteration=True) \\\n",
    "            .batch(batch_size=1028, drop_remainder=True) \\\n",
    "            .prefetch(1)\n",
    "\n",
    "# shuffle 1000 and bs 2056 increases...\n",
    "# shuffle 1000 and bs 1028 increases... but it starts to decrease after a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1226713/1226713 [==============================] - 4096s 3ms/step - loss: 653.1688\n",
      "Epoch 2/5\n",
      " 740383/1226713 [=================>............] - ETA: 27:10 - loss: 173.8855"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-based: batchsize of 1028\n",
    "# Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(1000, seed=seed) \\\n",
    "            .batch(batch_size=1028, drop_remainder=True) \\\n",
    "            .prefetch(1)\n",
    "\n",
    "# shuffle 1000 and bs 2056 increases...\n",
    "# shuffle 1000 and bs 1028 increases... but it starts to decrease after a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226713/1226713 [==============================] - 3219s 3ms/step - loss: 653.1750\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-based: batchsize of 128\n",
    "# Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(10, seed=seed) \\\n",
    "            .batch(batch_size=1028, drop_remainder=True) \\\n",
    "            .prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226713/1226713 [==============================] - 2769s 2ms/step - loss: 653.1287\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-based: batchsize of 128\n",
    "# Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(1000, seed=seed) \\\n",
    "            .batch(batch_size=2048, drop_remainder=True) \\\n",
    "            .prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615752/615752 [==============================] - 2899s 5ms/step - loss: 650.7554\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-based: batchsize of 128\n",
    "# Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(10000, seed=seed) \\\n",
    "            .batch(batch_size=1028, drop_remainder=True) \\\n",
    "            .prefetch(1)\n",
    "\n",
    "# shuffle 1000 and bs 2056 increases...\n",
    "# shuffle 1000 and bs 1028 increases... but it starts to decrease after a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226713/1226713 [==============================] - 3304s 3ms/step - loss: 653.1707\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-based: batchsize of 128\n",
    "# Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(100000, seed=seed) \\\n",
    "            .batch(batch_size=1028, drop_remainder=True) \\\n",
    "            .prefetch(1)\n",
    "\n",
    "# shuffle 1000 and bs 2056 increases...\n",
    "# shuffle 1000 and bs 1028 increases... but it starts to decrease after a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226713/1226713 [==============================] - 4071s 3ms/step - loss: 231.7048\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different seed\n",
    "# Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(100000, seed=100) \\\n",
    "            .batch(batch_size=1028, drop_remainder=True) \\\n",
    "            .prefetch(1)\n",
    "\n",
    "# shuffle 1000 and bs 2056 increases...\n",
    "# shuffle 1000 and bs 1028 increases... but it starts to decrease after a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226713/1226713 [==============================] - 4300s 4ms/step - loss: 653.1737\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to shuffle after batching\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .batch(batch_size=1028, drop_remainder=True) \\\n",
    "            .shuffle(100000, seed=seed) \\\n",
    "            .prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226713/1226713 [==============================] - 4208s 3ms/step - loss: 653.1186\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle dataset with numpy and train afterwards\n",
    "permuted_indices = np.random.permutation(np.arange(input_train.shape[0]))\n",
    "\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train[permuted_indices]), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train[permuted_indices]))) \\\n",
    "            .batch(batch_size=1028, drop_remainder=True) \\\n",
    "            .prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1226713/1226713 [==============================] - 2623s 2ms/step - loss: 653.1733\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is a batchsize of 512 really much slower than one of 1028? Yes\n",
    "\n",
    "# Column-based: batchsize of 128\n",
    "# Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(10, seed=seed) \\\n",
    "            .batch(batch_size=512, drop_remainder=True) \\\n",
    "            .prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 765699/2463010 [========>.....................] - ETA: 1:44:06 - loss: 76.0917"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-716b7692d0e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/work/b309170/conda/envs/clouds113/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/b309170/conda/envs/clouds113/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/b309170/conda/envs/clouds113/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/b309170/conda/envs/clouds113/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/b309170/conda/envs/clouds113/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/b309170/conda/envs/clouds113/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/work/b309170/conda/envs/clouds113/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column-based: batchsize of 128\n",
    "# Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                            tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "            .shuffle(100000, seed=seed) \\\n",
    "            .batch(batch_size=1028, drop_remainder=True) \\\n",
    "            .prefetch(1)\n",
    "\n",
    "# shuffle 1000 and bs 2056 increases...\n",
    "# shuffle 1000 and bs 1028 increases... but it starts to decrease after a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 372919/1226713 [========>.....................] - ETA: 49:20 - loss: 78.0647"
     ]
    }
   ],
   "source": [
    "# Try multiple epochs\n",
    "history = model.fit(train_ds, epochs=3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try multiple epochs\n",
    "history = model.fit(train_ds, epochs=6, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds113_kernel",
   "language": "python",
   "name": "clouds113_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
