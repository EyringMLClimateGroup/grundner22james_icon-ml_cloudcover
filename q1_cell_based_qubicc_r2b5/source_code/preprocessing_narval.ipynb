{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing NARVAL\n",
    "\n",
    "**This data was not used to train the NNs**\n",
    "\n",
    "Converting the data into npy makes it possible for us to work with it efficiently; originally we require 500GB of RAM which is always difficult to guarantee. We preprocess QUBICC in another ipynb notebook precisely because of this issue.\n",
    "\n",
    "1) We read the data\n",
    "2) Reshape variables so that they have equal dimensionality\n",
    "3) Reshape into data samples fit for the NN and convert into a DataFrame\n",
    "4) Downsample the data: Remove data above 21kms, remove condensate-free clouds, combat class-imbalance\n",
    "5) Split into input and output\n",
    "6) Save as npy\n",
    "\n",
    "Note: We neither scale nor split the data into training/validation/test sets. <br>\n",
    "The reason is that i) in order to scale we need the entire dataset but this can only be done in conjunction with the Qubicc dataset. Also for cross-validation different scalings will be necessary based on different subsets of the data, ii) The split into subsets will be done by the cross-validation procedure or not at all when training the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "# import importlib\n",
    "# importlib.reload(my_classes)\n",
    "\n",
    "base_path = '/pf/b/b309170'\n",
    "output_path = base_path + '/my_work/icon-ml_data/cloud_cover_parameterization/grid_cell_based_QUBICC_R02B05/based_on_var_interpolated_data'\n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "sys.path.insert(0, base_path + '/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "\n",
    "# Which days to load\n",
    "days_narval = 'all'\n",
    "\n",
    "from my_classes import load_data\n",
    "\n",
    "VERT_LAYERS = 31\n",
    "\n",
    "#Set a numpy seed for the permutation later on!\n",
    "np.random.seed(10)\n",
    "\n",
    "# Set output_var to one of {'clc', 'cl_area'}\n",
    "output_var = 'cl_area'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reading the data\n",
    "### Input:\n",
    "- fr_land: Fraction of land\n",
    "- coriolis: Coriolis parameter\n",
    "- zg: Geometric height at full levels (3D)\n",
    "- qv: Specific water vapor content (3D)\n",
    "- qc: Specific cloud water content (3D)\n",
    "- qi: Specific cloud ice content (3D)\n",
    "- temp: Temperature (3D)\n",
    "- pres: Pressure (3D)\n",
    "- u: Zonal wind (3D)\n",
    "- v: Meridional wind (3D)\n",
    "\n",
    "$10$ input nodes\n",
    "\n",
    "### Output:\n",
    "- clc: Cloud Cover\n",
    "\n",
    "$1$ output nodes\n",
    "\n",
    "The data above 21km is capped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cl_area I only need the output as I already have the input\n",
    "# I still need 'qc', 'qi', 'clc' for condensate-free clouds\n",
    "# If I were to use 'cl_area' for condensate-free clouds I would get an estimate \n",
    "# which is slightly different due to coarse-graining\n",
    "\n",
    "order_of_vars_narval = ['qv', 'qc', 'qi', 'temp', 'pres', 'u', 'v', 'zg', 'coriolis', 'fr_land', output_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NARVAL data\n",
    "data_dict = load_data(source='narval', days=days_narval, resolution='R02B05', \n",
    "                             order_of_vars=order_of_vars_narval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data_dict.keys():\n",
    "    print(key, data_dict[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(TIME_STEPS, VERT_LAYERS, HORIZ_FIELDS) = data_dict[output_var].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping into nd-arrays of equaling shapes (don't reshape in the vertical)\n",
    "data_dict['zg'] = np.repeat(np.expand_dims(data_dict['zg'], 0), TIME_STEPS, axis=0)\n",
    "try: \n",
    "    data_dict['coriolis'] = np.repeat(np.expand_dims(data_dict['coriolis'], 0), TIME_STEPS, axis=0)\n",
    "    data_dict['coriolis'] = np.repeat(np.expand_dims(data_dict['coriolis'], 1), VERT_LAYERS, axis=1)\n",
    "    data_dict['fr_land'] = np.repeat(np.expand_dims(data_dict['fr_land'], 0), TIME_STEPS, axis=0)\n",
    "    data_dict['fr_land'] = np.repeat(np.expand_dims(data_dict['fr_land'], 1), VERT_LAYERS, axis=1)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1721, 31, 4450)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carry along information about the vertical layer of a grid cell. int16 is sufficient for < 1000.\n",
    "vert_layers = np.int16(np.repeat(np.expand_dims(np.arange(1, VERT_LAYERS+1), 0), TIME_STEPS, axis=0))\n",
    "vert_layers = np.repeat(np.expand_dims(vert_layers, 2), HORIZ_FIELDS, axis=2)\n",
    "vert_layers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237411950"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshaping into 1D-arrays and converting dict into a DataFrame-object (the following is based on Aurelien Geron)\n",
    "for key in order_of_vars_narval:\n",
    "    data_dict[key] = np.reshape(data_dict[key], -1) \n",
    "    vert_layers = np.reshape(vert_layers, -1)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict)\n",
    "\n",
    "# Number of samples/rows\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downsampling the data (minority class: clc = 0)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data above 21kms\n",
    "df = df.loc[df['zg'] < 21000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no nans left\n",
    "assert np.all(np.isnan(df) == False) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some quick sanity checks regarding the input data\n",
    "if output_var == 'clc':\n",
    "    assert np.all(df['temp'] > 150) and np.all(df['pres'] > 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove condensate-free clouds (7.3% of clouds)\n",
    "# Here we have to use 'clc' to keep the size of the output consistent!\n",
    "df = df.loc[~((df['clc'] > 0) & (df['qc'] == 0) & (df['qi'] == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138399774\n"
     ]
    }
   ],
   "source": [
    "# We ensure that clc != 0 is as large as clc = 0 (which then has 294 Mio samples) and keep the original order intact\n",
    "df_noclc = df.loc[df['clc']==0]\n",
    "print(len(df_noclc))\n",
    "\n",
    "# len(downsample_indices) will be the number of noclc samples that remain\n",
    "downsample_ratio = (len(df) - len(df_noclc))/len(df_noclc)\n",
    "shuffled_indices = np.random.permutation(df.loc[df['clc']==0].index)\n",
    "size_noclc = int(len(df_noclc)*downsample_ratio)\n",
    "downsample_indices = shuffled_indices[:size_noclc] \n",
    "\n",
    "# Concatenate df.loc[df['cl']!=0].index and downsample_indices\n",
    "final_indices = np.concatenate((downsample_indices, df.loc[df['clc']!=0].index))\n",
    "\n",
    "# Sort final_indices so that we can more or less recover the timesteps\n",
    "final_indices = np.sort(final_indices)\n",
    "\n",
    "# Label-based (loc) not positional-based\n",
    "df = df.loc[final_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126853676"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of samples after downsampling\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifies df as well\n",
    "def split_input_output(dataset):\n",
    "    output_df = dataset[output_var]\n",
    "    del dataset[output_var]\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = split_input_output(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "if output_var == 'clc':\n",
    "    np.save(output_path + '/cloud_cover_input_narval.npy', np.float32(df))\n",
    "    np.save(output_path + '/cloud_cover_output_narval.npy', np.float32(output_df))\n",
    "elif output_var == 'cl_area':\n",
    "    np.save(output_path + '/cloud_area_output_narval.npy', np.float32(output_df))\n",
    "\n",
    "# Save the corresponding vertical layers (int16 is sufficient for layers < 1000)\n",
    "if output_var == 'clc':\n",
    "    np.save(output_path + '/samples_vertical_layers_narval.npy', vert_layers[df.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test whether qi from the saved data coincides with the qi here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If this yields True then we're done\n",
    "np.all(old_input[:,2] == df['qi'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (based on the module python3/2022.01)",
   "language": "python",
   "name": "python3_2022_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
