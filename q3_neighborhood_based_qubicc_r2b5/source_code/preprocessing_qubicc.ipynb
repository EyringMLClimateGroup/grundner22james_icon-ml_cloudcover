{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Qubicc\n",
    "\n",
    "Converting the data into npy makes it possible for us to work with it efficiently; originally we require 500GB of RAM which is always difficult to guarantee. We preprocess QUBICC in another ipynb notebook precisely because of this issue.\n",
    "\n",
    "1) We read the data\n",
    "2) Reshape variables so that they have equal dimensionality\n",
    "3) Reshape into data samples fit for the NN and convert into a DataFrame\n",
    "4) Downsample the data: Remove data above 21kms, remove condensate-free clouds, combat class-imbalance\n",
    "5) Split into input and output\n",
    "6) Save as npy\n",
    "\n",
    "Note: We neither scale nor split the data into training/validation/test sets. <br>\n",
    "The reason is that i) in order to scale we need the entire dataset but this can only be done in conjunction with the Qubicc dataset. Also for cross-validation different scalings will be necessary based on different subsets of the data, ii) The split into subsets will be done by the cross-validation procedure or not at all when training the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ran with 900GB\n",
    "\n",
    "import sys\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "# import importlib\n",
    "# importlib.reload(my_classes)\n",
    "\n",
    "base_path = '/pf/b/b309170'\n",
    "output_path = base_path + '/my_work/icon-ml_data/cloud_cover_parameterization/region_based_one_nn_R02B05/based_on_var_interpolated_data'\n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "sys.path.insert(0, base_path + '/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "\n",
    "# Which days to load\n",
    "days_qubicc = 'all_hcs'\n",
    "\n",
    "from my_classes import load_data\n",
    "\n",
    "VERT_LAYERS = 31\n",
    "\n",
    "#Set a numpy seed for the permutation later on!\n",
    "np.random.seed(10)\n",
    "\n",
    "# Set output_var to one of {'cl', 'cl_area'}\n",
    "output_var = 'cl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reading the data\n",
    "### Input:\n",
    "- coriolis: Coriolis parameter\n",
    "- zg: Geometric height at full levels (3D)\n",
    "- qv: Specific water vapor content (3D)\n",
    "- qc: Specific cloud water content (3D)\n",
    "- qi: Specific cloud ice content (3D)\n",
    "- temp: Temperature (3D)\n",
    "- pres: Pressure (3D)\n",
    "- u: Zonal wind (3D)\n",
    "- v: Meridional wind (3D)\n",
    "\n",
    "$10$ input nodes\n",
    "\n",
    "### Output:\n",
    "- clc: Cloud Cover\n",
    "\n",
    "$1$ output nodes\n",
    "\n",
    "The data above 21km is capped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cl_area I only need the output as I already have the input\n",
    "# I still need 'clw', 'cli' for condensate-free clouds\n",
    "order_of_vars_qubicc = ['hus', 'clw', 'cli', 'ta', 'pfull', 'ua', 'va', 'zg', 'coriolis', output_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hus\n",
      "clw\n",
      "cli\n",
      "ta\n",
      "pfull\n",
      "ua\n",
      "va\n",
      "cl\n"
     ]
    }
   ],
   "source": [
    "# Load QUBICC data\n",
    "data_dict = load_data(source='qubicc', days=days_qubicc, resolution='R02B05', \n",
    "                             order_of_vars=order_of_vars_qubicc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7348"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the data to float32 asap!\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = np.float32(data_dict[key])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hus (2162, 31, 78069)\n",
      "clw (2162, 31, 78069)\n",
      "cli (2162, 31, 78069)\n",
      "ta (2162, 31, 78069)\n",
      "pfull (2162, 31, 78069)\n",
      "ua (2162, 31, 78069)\n",
      "va (2162, 31, 78069)\n",
      "zg (31, 78069)\n",
      "coriolis (78069,)\n",
      "cl (2162, 31, 78069)\n"
     ]
    }
   ],
   "source": [
    "for key in data_dict.keys():\n",
    "    print(key, data_dict[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(TIME_STEPS, VERT_LAYERS, HORIZ_FIELDS) = data_dict[output_var].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping into nd-arrays of equaling shapes (don't reshape in the vertical)\n",
    "data_dict['zg'] = np.repeat(np.expand_dims(data_dict['zg'], 0), TIME_STEPS, axis=0)\n",
    "try: \n",
    "    data_dict['coriolis'] = np.repeat(np.expand_dims(data_dict['coriolis'], 0), TIME_STEPS, axis=0)\n",
    "    data_dict['coriolis'] = np.repeat(np.expand_dims(data_dict['coriolis'], 1), VERT_LAYERS, axis=1)\n",
    "    # Surface temperature (Try without?)\n",
    "    temp_sfc = np.repeat(np.expand_dims(data_dict['ta'][:, -1, :], axis=1), VERT_LAYERS, axis=1)\n",
    "except: \n",
    "    print('There is probably no coriolis or temperature in order_of_vars_qubicc')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first timesteps of the QUBICC simulations since the clc values are 0 across the entire earth there \n",
    "remove_steps = []\n",
    "for i in range(data_dict[output_var].shape[0]):\n",
    "    if np.all(data_dict[output_var][i,4:,:] == 0):\n",
    "        remove_steps.append(i)\n",
    "        TIME_STEPS = TIME_STEPS - 1\n",
    "\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = np.delete(data_dict[key], remove_steps, axis=0)\n",
    "        \n",
    "try:\n",
    "    temp_sfc = np.float32(np.delete(temp_sfc, remove_steps, axis=0))\n",
    "except:\n",
    "    print('There is probably no temperature in order_of_vars_qubicc')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.000015"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our Neural Network has trained with clc in [0, 100]!\n",
    "data_dict[output_var] = 100*data_dict[output_var]\n",
    "np.max(data_dict[output_var][:, 4:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2159, 31, 78069)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carry along information about the vertical layer of a grid cell. int16 is sufficient for < 1000.\n",
    "vert_layers = np.int16(np.repeat(np.expand_dims(np.arange(1, VERT_LAYERS+1), 0), TIME_STEPS, axis=0))\n",
    "vert_layers = np.repeat(np.expand_dims(vert_layers, 2), HORIZ_FIELDS, axis=2)\n",
    "vert_layers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Subsample QUBICC data further\n",
    "\n",
    "# We reduce the data size to using only every three hours from the QUBICC data.\n",
    "# The reason is that training is almost impossible with a total data size of 3.6 Billion samples (from NARVAL we have 126 Mio samples). \n",
    "# To make it feasible we would need a training batch size of ~5000.\n",
    "# Therefore we need to decrease the amount of samples further. \n",
    "# We decrease the amount of QUBICC samples as they are less reliable than the NARVAL samples. \n",
    "# We split the dataset in half by only taking into account every two hours (we can delete every second hour as we assume \n",
    "# a relatively high temporal correlation).\n",
    "\n",
    "for key in order_of_vars_qubicc:\n",
    "    data_dict[key] = data_dict[key][0::3]\n",
    "vert_layers = vert_layers[0::3]\n",
    "try:\n",
    "    temp_sfc = temp_sfc[0::3]\n",
    "except:\n",
    "    print('There is probably no (surface) temperature.')\n",
    "    pass\n",
    "\n",
    "# Adapt time steps (roughly divided by 3)\n",
    "TIME_STEPS = data_dict[output_var].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add variables below and above\n",
    "\n",
    "def add_above_and_below(var_array):\n",
    "    '''\n",
    "        var_array: 3D tensor\n",
    "    '''\n",
    "    above = (np.insert(var_array, obj=0, values=1000*np.ones((TIME_STEPS, HORIZ_FIELDS)), axis=1))[:, :-1, :]\n",
    "    # Replace by the entry from the same cell if the one above is nan.\n",
    "    # It is a bit suboptimal that the ones above can be nan. \n",
    "    # But in QUBICC this only pertains the cli, clw and hus which are zero anyways at 21kms.\n",
    "    nan_indices = np.where(np.isnan(above))\n",
    "    above[nan_indices] = above[nan_indices[0], nan_indices[1]+1, nan_indices[2]]\n",
    "    \n",
    "    # Below is the same value as the grid cell for surface-closest layer\n",
    "    below = (np.append(var_array, values=var_array[:, -1:, :], axis=1))[:, 1:, :]\n",
    "    return above, below\n",
    "\n",
    "above = {}\n",
    "below = {}\n",
    "\n",
    "# 1000 is a value that cannot be attained physically and serves as our way of checking whether the grid cell is at the model top\n",
    "# It makes sense to insert 0 as the difference at the lowest levels. (Note that the values won't stay 0 after normalization) \n",
    "# The NN could get around these values that are not really physical by weighing the influence from below with a zg-factor.\n",
    "# Alternatively we would have to remove the variable from below altogether\n",
    "\n",
    "for key in order_of_vars_qubicc[:-2]:\n",
    "    above[key], below[key] = add_above_and_below(data_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hus</th>\n",
       "      <th>clw</th>\n",
       "      <th>cli</th>\n",
       "      <th>ta</th>\n",
       "      <th>pfull</th>\n",
       "      <th>ua</th>\n",
       "      <th>va</th>\n",
       "      <th>zg</th>\n",
       "      <th>coriolis</th>\n",
       "      <th>cl</th>\n",
       "      <th>...</th>\n",
       "      <th>ta_above</th>\n",
       "      <th>pfull_below</th>\n",
       "      <th>pfull_above</th>\n",
       "      <th>ua_below</th>\n",
       "      <th>ua_above</th>\n",
       "      <th>va_below</th>\n",
       "      <th>va_above</th>\n",
       "      <th>zg_below</th>\n",
       "      <th>zg_above</th>\n",
       "      <th>temp_sfc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214.038208</td>\n",
       "      <td>1370.786011</td>\n",
       "      <td>27.121922</td>\n",
       "      <td>19.595692</td>\n",
       "      <td>28193.783203</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1885.024536</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>23.643120</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>11.679565</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>26201.697266</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>268.771423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214.383835</td>\n",
       "      <td>1370.754395</td>\n",
       "      <td>27.276657</td>\n",
       "      <td>20.395845</td>\n",
       "      <td>28193.783203</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1884.386597</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>23.998238</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.613994</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>26201.697266</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>269.241333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>213.572433</td>\n",
       "      <td>1365.707642</td>\n",
       "      <td>26.677664</td>\n",
       "      <td>19.175291</td>\n",
       "      <td>28193.783203</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1878.876709</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>22.968130</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>10.617148</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>26201.697266</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>268.296692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214.446808</td>\n",
       "      <td>1377.688721</td>\n",
       "      <td>26.759027</td>\n",
       "      <td>19.512516</td>\n",
       "      <td>28193.783203</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1892.658691</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>24.536003</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>12.112512</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>26201.697266</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>267.593109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>213.310684</td>\n",
       "      <td>1359.823975</td>\n",
       "      <td>27.173082</td>\n",
       "      <td>19.345314</td>\n",
       "      <td>28193.783203</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1871.216309</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>22.446886</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>11.548752</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>26201.697266</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>266.535187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        hus  clw  cli          ta        pfull         ua         va  \\\n",
       "0  0.000003  NaN  NaN  214.038208  1370.786011  27.121922  19.595692   \n",
       "1  0.000003  NaN  NaN  214.383835  1370.754395  27.276657  20.395845   \n",
       "2  0.000003  NaN  NaN  213.572433  1365.707642  26.677664  19.175291   \n",
       "3  0.000003  NaN  NaN  214.446808  1377.688721  26.759027  19.512516   \n",
       "4  0.000003  NaN  NaN  213.310684  1359.823975  27.173082  19.345314   \n",
       "\n",
       "             zg  coriolis  cl  ...  ta_above  pfull_below  pfull_above  \\\n",
       "0  28193.783203  0.000116 NaN  ...    1000.0  1885.024536       1000.0   \n",
       "1  28193.783203  0.000116 NaN  ...    1000.0  1884.386597       1000.0   \n",
       "2  28193.783203  0.000116 NaN  ...    1000.0  1878.876709       1000.0   \n",
       "3  28193.783203  0.000115 NaN  ...    1000.0  1892.658691       1000.0   \n",
       "4  28193.783203  0.000118 NaN  ...    1000.0  1871.216309       1000.0   \n",
       "\n",
       "    ua_below  ua_above   va_below  va_above      zg_below  zg_above  \\\n",
       "0  23.643120    1000.0  11.679565    1000.0  26201.697266    1000.0   \n",
       "1  23.998238    1000.0  12.613994    1000.0  26201.697266    1000.0   \n",
       "2  22.968130    1000.0  10.617148    1000.0  26201.697266    1000.0   \n",
       "3  24.536003    1000.0  12.112512    1000.0  26201.697266    1000.0   \n",
       "4  22.446886    1000.0  11.548752    1000.0  26201.697266    1000.0   \n",
       "\n",
       "     temp_sfc  \n",
       "0  268.771423  \n",
       "1  269.241333  \n",
       "2  268.296692  \n",
       "3  267.593109  \n",
       "4  266.535187  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshaping into 1D-arrays and converting dict into a DataFrame-object (the following is based on Aurelien Geron)\n",
    "for key in data_dict.keys():\n",
    "    data_dict[key] = np.reshape(data_dict[key], -1) \n",
    "    vert_layers = np.reshape(vert_layers, -1)\n",
    "    \n",
    "for key in order_of_vars_qubicc[:-2]:\n",
    "    data_dict['%s_below'%key] = np.reshape(below[key], -1)\n",
    "    data_dict['%s_above'%key] = np.reshape(above[key], -1)\n",
    "    \n",
    "try:\n",
    "    data_dict['temp_sfc'] = np.reshape(temp_sfc, -1)\n",
    "except:\n",
    "    print('There is probably no (surface) temperature.')\n",
    "    pass\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del data_dict\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downsampling the data (minority class: clc = 0)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data above 21kms\n",
    "df = df.loc[df['zg'] < 21000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no nans left\n",
    "assert np.all(np.isnan(df) == False) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some quick sanity checks regarding the input data\n",
    "if output_var == 'cl':\n",
    "    assert np.all(df['ta'] > 150) and np.all(df['pfull'] > 150)\n",
    "    \n",
    "    #The upper levels have been cut off so there are no wrong values in the data anymore\n",
    "    assert df[df['ta_above']==1000].shape[0] == 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove condensate-free clouds (7.3% of clouds)\n",
    "df = df.loc[~((df['cl'] > 0) & (df['clw'] == 0) & (df['cli'] == 0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "920713771\n"
     ]
    }
   ],
   "source": [
    "# We ensure that clc != 0 is as large as clc = 0 (which then has 294 Mio samples) and keep the original order intact\n",
    "df_noclc = df.loc[df['cl']==0]\n",
    "print(len(df_noclc))\n",
    "\n",
    "# len(downsample_indices) will be the number of noclc samples that remain\n",
    "downsample_ratio = (len(df) - len(df_noclc))/len(df_noclc)\n",
    "shuffled_indices = np.random.permutation(df.loc[df['cl']==0].index)\n",
    "size_noclc = int(len(df_noclc)*downsample_ratio)\n",
    "downsample_indices = shuffled_indices[:size_noclc] \n",
    "\n",
    "# Concatenate df.loc[df[output_var]!=0].index and downsample_indices\n",
    "final_indices = np.concatenate((downsample_indices, df.loc[df['cl']!=0].index))\n",
    "\n",
    "# Sort final_indices so that we can more or less recover the timesteps\n",
    "final_indices = np.sort(final_indices)\n",
    "\n",
    "# Label-based (loc) not positional-based\n",
    "df = df.loc[final_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176638142"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of samples after downsampling\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modifies df as well\n",
    "def split_input_output(dataset):\n",
    "    output_df = dataset[output_var]\n",
    "    del dataset[output_var]\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = split_input_output(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "if output_var == 'cl':\n",
    "    np.save(output_path + '/cloud_cover_input_qubicc.npy', np.float32(df))\n",
    "    np.save(output_path + '/cloud_cover_output_qubicc.npy', np.float32(output_df))\n",
    "elif output_var == 'cl_area':\n",
    "    np.save(output_path + '/cloud_area_output_qubicc.npy', np.float32(output_df))\n",
    "\n",
    "# Save the corresponding vertical layers (int16 is sufficient for layers < 1000)\n",
    "if output_var == 'cl':\n",
    "    np.save(output_path + '/samples_vertical_layers_qubicc.npy', vert_layers[df.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tests of the cloud area output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_var == 'cl_area':\n",
    "    # Test\n",
    "    old_input = np.load(output_path + '/cloud_cover_input_qubicc.npy')\n",
    "    # If this yields True then we're done\n",
    "    print(np.all(old_input[:,2] == df['cli']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clc = np.load(output_path + '/cloud_cover_output_qubicc.npy')\n",
    "cl_area = np.load(output_path + '/cloud_area_output_qubicc.npy')\n",
    "\n",
    "diff = cl_area - clc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(diff, bins = 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(diff, bins = 100, log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should be anomalies existing due to differences in coarse-graining\n",
    "len(np.where(diff < 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(diff > 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(diff >= 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(diff < 0)[0])/len(diff) # 1.17% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(diff < 0)[0])/len(np.where(diff != 0)[0]) # 2.36% of cloudy data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds113_kernel",
   "language": "python",
   "name": "clouds113_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
