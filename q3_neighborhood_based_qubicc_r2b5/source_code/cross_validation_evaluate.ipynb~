{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the cross-validation models\n",
    "\n",
    "Compared to cross_validation_evaluate.ipynb in the column-based model it sufficed to <br>\n",
    "i) Change the names from column to cell, <br>\n",
    "ii) add leaky_relu, <br>\n",
    "iii) load layers_data and adapt the function mean_clc_per_vertical_layer accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "\n",
    "#Import sklearn before tensorflow (static Thread-local storage)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.errors import ResourceExhaustedError\n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "path_base = '/pf/b/b309170'\n",
    "sys.path.insert(0, path_base + '/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "\n",
    "import my_classes\n",
    "from my_classes import write_infofile\n",
    "\n",
    "# For Leaky_ReLU:\n",
    "from tensorflow import nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "path_base = '/pf/b/b309170'\n",
    "path_model = path_base + '/workspace_icon-ml/cloud_cover_parameterization/region_based_one_nn_R02B05/saved_models'\n",
    "path_figures = path_base + '/workspace_icon-ml/cloud_cover_parameterization/region_based_one_nn_R02B05/figures'\n",
    "path_data = path_base + '/my_work/icon-ml_data/cloud_cover_parameterization/region_based_one_nn_R02B05/based_on_var_interpolated_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lrelu(x):\n",
    "#     return nn.leaky_relu(x, alpha=0.01)\n",
    "\n",
    "custom_objects = {}\n",
    "custom_objects['leaky_relu'] = nn.leaky_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_1 = 'cross_validation_region_based_fold_1.h5'\n",
    "fold_2 = 'cross_validation_region_based_fold_2.h5'\n",
    "fold_3 = 'cross_validation_region_based_fold_3.h5'\n",
    "\n",
    "all_data = 'region_based_all_data_seed_10.h5'\n",
    "\n",
    "model_fold_1 = load_model(os.path.join(path_model, fold_1), custom_objects)\n",
    "model_fold_2 = load_model(os.path.join(path_model, fold_2), custom_objects)\n",
    "model_fold_3 = load_model(os.path.join(path_model, fold_3), custom_objects)\n",
    "\n",
    "model_all_data = load_model(os.path.join(path_model, all_data), custom_objects)\n",
    "\n",
    "# model_fold_1 = load_model(os.path.join(path_model, fold_1))\n",
    "# model_fold_2 = load_model(os.path.join(path_model, fold_2))\n",
    "# model_fold_3 = load_model(os.path.join(path_model, fold_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cloud_cover_input_qubicc.npy',\n",
       " 'samples_vertical_layers_qubicc.npy',\n",
       " 'samples_vertical_layers_narval.npy',\n",
       " 'cloud_cover_output_qubicc.npy',\n",
       " 'cloud_cover_input_narval.npy',\n",
       " 'cloud_cover_output_narval.npy']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.concatenate((np.load(path_data + '/cloud_cover_input_narval.npy'), \n",
    "                             np.load(path_data + '/cloud_cover_input_qubicc.npy')), axis=0)\n",
    "output_data = np.concatenate((np.load(path_data + '/cloud_cover_output_narval.npy'), \n",
    "                              np.load(path_data + '/cloud_cover_output_qubicc.npy')), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_data = np.concatenate((np.load(path_data + '/samples_vertical_layers_narval.npy'), \n",
    "                              np.load(path_data + '/samples_vertical_layers_qubicc.npy')), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_narval = np.load(path_data + '/cloud_cover_output_narval.npy').shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(samples_total, no_of_features) = input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define cross-validation folds to recreate training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folds = []\n",
    "validation_folds = []\n",
    "two_week_incr = samples_total//6\n",
    "\n",
    "for i in range(3):\n",
    "    # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "    first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "    second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "    validation_folds.append(np.append(first_incr, second_incr))\n",
    "    training_folds.append(np.arange(samples_total))\n",
    "    training_folds[i] = np.delete(training_folds[i], validation_folds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data will need to be scaled according to the training folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful functions to plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_clc_per_vertical_layer(model, input_data, output_data, layers_data, batch_size=2**20):\n",
    "    '''\n",
    "        Input: \n",
    "            model: neural network\n",
    "            input_data: Usually the validation data\n",
    "            output_data: The ground truth output\n",
    "            layers_data: Vector that tells us the vertical layer of a given sample\n",
    "            \n",
    "        Model prediction and the Ground Truth means per vertical layer\n",
    "    '''\n",
    "    # Predicted cloud cover means\n",
    "    # Curiously it works best if we use predict_on_batch on small subsets of the data instead of predict(..., batch_size=...) \n",
    "    for i in range(1 + input_data.shape[0]//batch_size):\n",
    "        if i == 0:\n",
    "            a = model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            a = np.concatenate((a, model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])), axis=0)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "    pred_adj = np.minimum(np.maximum(a, 0), 100)\n",
    "    \n",
    "    # Computing means with the help of layers_data\n",
    "    clc_pred_mean = []; clc_data_mean = [];\n",
    "    for i in range(5, 32):\n",
    "        ind = np.where(layers_data == i)\n",
    "        clc_data_mean.append(np.mean(output_data[ind], dtype=np.float64))\n",
    "        clc_pred_mean.append(np.mean(pred_adj[ind], dtype=np.float64))\n",
    "    \n",
    "    return clc_pred_mean, clc_data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(fig_name, fig_title, model_predictions, valid_means=None):\n",
    "    '''\n",
    "        Note that this figure truly is a different performance measure than the validation error.\n",
    "        The reason is that the mean can in principle be good even when the model is really bad.\n",
    "        \n",
    "        model_predictions: Array of length 3, covers predictions from all three folds for a given TL setup\n",
    "        > Now: Array of length 4. Included model_all_data.\n",
    "        valid_means: Array of length 3, covers validation means from all three folds for a given TL setup\n",
    "        > Now: Array of length 4. Included model_all_data.\n",
    "    '''\n",
    "#     assert len(model_biases) == 3\n",
    "    \n",
    "    # Vertical layers\n",
    "    a = np.linspace(5, 31, 27)\n",
    "    fig = plt.figure(figsize=(11,7))\n",
    "    # For model\n",
    "    ax = fig.add_subplot(111, xlabel='Mean Cloud Cover', ylabel='Vertical layer', title=fig_title)\n",
    "    if not valid_means[0] == valid_means[1] == valid_means[2] == valid_means[3]:\n",
    "        colors = ['g', 'b', 'r']\n",
    "        for i in range(len(model_predictions)):\n",
    "            ax.plot(model_predictions[i], a, colors[i])\n",
    "            if valid_means != None:\n",
    "                ax.plot(valid_means[i], a, '%s--'%colors[i])\n",
    "        plt.gca().invert_yaxis()\n",
    "        ax.legend(['Model Fold 1 Predictions', 'Fold 1 Truth', 'Model Fold 2 Predictions', 'Fold 2 Truth', \n",
    "                   'Model Fold 3 Predictions', 'Fold 3 Truth', 'Model All Data Predictions', 'Truth'])\n",
    "    else:\n",
    "        for i in range(len(model_predictions)):\n",
    "            ax.plot(model_predictions[i], a)\n",
    "        ax.plot(valid_means[0], a, 'black')\n",
    "        plt.gca().invert_yaxis()\n",
    "        ax.legend(['Model Fold 1 Predictions', 'Model Fold 2 Predictions', 'Model Fold 3 Predictions', \n",
    "                   'Model All Data Predictions', 'Truth'])\n",
    "\n",
    "    fig.savefig(os.path.join(path_figures, fig_name+'.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the models on the data\n",
    "\n",
    "Add training and validation losses to the text files. <br>\n",
    "Print results per vertical layer (respective validation set/NARVAL/QUBICC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = [] ; valid_losses = [] ; valid_means = [] ; valid_model_predictions = [] ;\n",
    "narval_means = [] ; narval_model_predictions = [] ; qubicc_means = [] ; qubicc_model_predictions = [] ;\n",
    "qubicc_month_0 = [] ; qubicc_model_pred_month_0 = [] ; qubicc_month_1 = [] ; qubicc_model_pred_month_1 = [] ;\n",
    "qubicc_month_2 = [] ; qubicc_model_pred_month_2 = [] ;\n",
    "\n",
    "for i in range(3): \n",
    "    filename = 'cross_validation_region_based_fold_%d'%(i+1)\n",
    "    # Choose appropriate model for this fold\n",
    "    if i == 0: model = model_fold_1\n",
    "    if i == 1: model = model_fold_2\n",
    "    if i == 2: model = model_fold_3\n",
    "    \n",
    "    #Standardize according to the fold\n",
    "    scaler.fit(input_data[training_folds[i]])\n",
    "    \n",
    "    #Load the data for the respective fold\n",
    "    input_train = scaler.transform(input_data[training_folds[i]])\n",
    "    input_valid = scaler.transform(input_data[validation_folds[i]])\n",
    "    output_train = output_data[training_folds[i]]\n",
    "    output_valid = output_data[validation_folds[i]]\n",
    "    \n",
    "    ## Training and validation losses\n",
    "    train_loss = model.evaluate(input_train, output_train, verbose=2, batch_size=10**5)\n",
    "    valid_loss = model.evaluate(input_valid, output_valid, verbose=2, batch_size=10**5)\n",
    "    \n",
    "    # Clear up some memory\n",
    "    del input_train, output_train\n",
    "    gc.collect()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "        file.write('Unbounded training loss: %.4f\\n'%(train_loss))\n",
    "        file.write('Unbounded validation loss: %.4f\\n'%(valid_loss))\n",
    "        \n",
    "    ## Compute mean cloud cover per vertical layer\n",
    "    # On the respective validation sets (QUBICC and NARVAL)\n",
    "    try:\n",
    "        clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_valid, output_valid, \n",
    "                                                                   layers_data[validation_folds[i]])\n",
    "    except(ResourceExhaustedError):\n",
    "        print('Resource Exhausted Qubicc')\n",
    "        clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_valid, output_valid, \n",
    "                                                                   layers_data[validation_folds[i]], batch_size=2**15)\n",
    "    valid_means.append(clc_data_mean)\n",
    "    valid_model_predictions.append(clc_pred_mean)\n",
    "    \n",
    "    # Clear up some memory\n",
    "    del input_valid, output_valid\n",
    "    gc.collect()\n",
    "    \n",
    "    # For NARVAL\n",
    "    input_narval = scaler.transform(input_data[:samples_narval])\n",
    "    output_narval = output_data[:samples_narval]\n",
    "    try:\n",
    "        clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_narval, output_narval,\n",
    "                                                                  layers_data[:samples_narval])\n",
    "    except(ResourceExhaustedError):\n",
    "        print('Resource Exhausted Narval')\n",
    "        clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_narval, output_narval, \n",
    "                                                                   layers_data[:samples_narval], \n",
    "                                                                   batch_size=2**15)\n",
    "    narval_means.append(clc_data_mean)\n",
    "    narval_model_predictions.append(clc_pred_mean)\n",
    "    \n",
    "    # Clear up some memory\n",
    "    del input_narval, output_narval\n",
    "    gc.collect()\n",
    "    \n",
    "    # For QUBICC  \n",
    "    input_qubicc = scaler.transform(input_data[samples_narval:])\n",
    "    output_qubicc = output_data[samples_narval:]\n",
    "    try:\n",
    "        clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_qubicc, output_qubicc,\n",
    "                                                                  layers_data[samples_narval:])\n",
    "    except(ResourceExhaustedError):\n",
    "        print('Resource Exhausted Qubicc')\n",
    "        clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_qubicc, output_qubicc, \n",
    "                                                                   layers_data[samples_narval:], \n",
    "                                                                   batch_size=2**15)\n",
    "    qubicc_means.append(clc_data_mean)\n",
    "    qubicc_model_predictions.append(clc_pred_mean)\n",
    "    \n",
    "    # Clear up some memory\n",
    "    del input_qubicc, output_qubicc\n",
    "    gc.collect()\n",
    "    \n",
    "    # QUBICC months\n",
    "    qubicc_month = (samples_total - samples_narval)//3\n",
    "    for month in range(3):\n",
    "        first_ind = samples_narval + month*qubicc_month\n",
    "        last_ind = samples_narval + (month+1)*qubicc_month\n",
    "        input_qubicc = scaler.transform(input_data[first_ind:last_ind])\n",
    "        output_qubicc = output_data[first_ind:last_ind]\n",
    "        try:\n",
    "            clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_qubicc, output_qubicc,\n",
    "                                                                      layers_data[first_ind:last_ind])\n",
    "        except(ResourceExhaustedError):\n",
    "            print('Resource Exhausted Qubicc')\n",
    "            clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_qubicc, output_qubicc, \n",
    "                                                                       layers_data[first_ind:last_ind],\n",
    "                                                                       batch_size=2**15)\n",
    "        if month==0: \n",
    "            qubicc_month_0.append(clc_data_mean)\n",
    "            qubicc_model_pred_month_0.append(clc_pred_mean)\n",
    "        if month==1:\n",
    "            qubicc_month_1.append(clc_data_mean)\n",
    "            qubicc_model_pred_month_1.append(clc_pred_mean)\n",
    "        if month==2:\n",
    "            qubicc_month_2.append(clc_data_mean)\n",
    "            qubicc_model_pred_month_2.append(clc_pred_mean)\n",
    "\n",
    "    # Clear up some memory\n",
    "    del input_qubicc, output_qubicc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'region_based_all_data_seed_10'\n",
    "model = model_all_data\n",
    "\n",
    "#Standardize according to the fold\n",
    "scaler.fit(input_data)\n",
    "    \n",
    "#Load the data for the respective fold\n",
    "input_train = scaler.transform(input_data)\n",
    "output_train = output_data\n",
    "\n",
    "## Training loss\n",
    "train_loss = model.evaluate(input_train, output_train, verbose=2, batch_size=10**5)\n",
    "\n",
    "# Clear up some memory\n",
    "del input_train, output_train\n",
    "gc.collect()\n",
    "\n",
    "train_losses.append(train_loss)\n",
    "\n",
    "with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "    file.write('Unbounded training loss: %.4f\\n'%(train_loss))\n",
    "    \n",
    "## For NARVAL\n",
    "input_narval = scaler.transform(input_data[:samples_narval])\n",
    "output_narval = output_data[:samples_narval]\n",
    "try:\n",
    "    clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_narval, output_narval,\n",
    "                                                              layers_data[:samples_narval])\n",
    "except(ResourceExhaustedError):\n",
    "    print('Resource Exhausted Narval')\n",
    "    clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_narval, output_narval, \n",
    "                                                               layers_data[:samples_narval], \n",
    "                                                               batch_size=2**15)\n",
    "narval_means.append(clc_data_mean)\n",
    "narval_model_predictions.append(clc_pred_mean)\n",
    "\n",
    "# Clear up some memory\n",
    "del input_narval, output_narval\n",
    "gc.collect()\n",
    "\n",
    "## For QUBICC  \n",
    "input_qubicc = scaler.transform(input_data[samples_narval:])\n",
    "output_qubicc = output_data[samples_narval:]\n",
    "try:\n",
    "    clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_qubicc, output_qubicc,\n",
    "                                                              layers_data[samples_narval:])\n",
    "except(ResourceExhaustedError):\n",
    "    print('Resource Exhausted Qubicc')\n",
    "    clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_qubicc, output_qubicc, \n",
    "                                                               layers_data[samples_narval:], \n",
    "                                                               batch_size=2**15)\n",
    "qubicc_means.append(clc_data_mean)\n",
    "qubicc_model_predictions.append(clc_pred_mean)\n",
    "\n",
    "# Clear up some memory\n",
    "del input_qubicc, output_qubicc\n",
    "gc.collect()\n",
    "\n",
    "## QUBICC months\n",
    "qubicc_month = (samples_total - samples_narval)//3\n",
    "for month in range(3):\n",
    "    first_ind = samples_narval + month*qubicc_month\n",
    "    last_ind = samples_narval + (month+1)*qubicc_month\n",
    "    input_qubicc = scaler.transform(input_data[first_ind:last_ind])\n",
    "    output_qubicc = output_data[first_ind:last_ind]\n",
    "    try:\n",
    "        clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_qubicc, output_qubicc,\n",
    "                                                                  layers_data[first_ind:last_ind])\n",
    "    except(ResourceExhaustedError):\n",
    "        print('Resource Exhausted Qubicc')\n",
    "        clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_qubicc, output_qubicc, \n",
    "                                                                   layers_data[first_ind:last_ind],\n",
    "                                                                   batch_size=2**15)\n",
    "    if month==0: \n",
    "        qubicc_month_0.append(clc_data_mean)\n",
    "        qubicc_model_pred_month_0.append(clc_pred_mean)\n",
    "    if month==1:\n",
    "        qubicc_month_1.append(clc_data_mean)\n",
    "        qubicc_model_pred_month_1.append(clc_pred_mean)\n",
    "    if month==2:\n",
    "        qubicc_month_2.append(clc_data_mean)\n",
    "        qubicc_model_pred_month_2.append(clc_pred_mean)\n",
    "\n",
    "# Clear up some memory\n",
    "del input_qubicc, output_qubicc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot results\n",
    "save_figure('cross_validation_validation_means', 'Region-based models on the respective validation sets', \n",
    "            valid_model_predictions, valid_means)\n",
    "save_figure('cross_validation_narval', 'Region-based models on the NARVAL data', \n",
    "            narval_model_predictions, narval_means)\n",
    "save_figure('cross_validation_qubicc', 'Region-based models on the QUBICC data', \n",
    "            qubicc_model_predictions, qubicc_means)\n",
    "\n",
    "# Qubicc months\n",
    "save_figure('cross_validation_qubicc_hc2', 'Region-based models on the QUBICC data, November 2004', \n",
    "            qubicc_model_pred_month_0, qubicc_month_0)\n",
    "save_figure('cross_validation_qubicc_hc3', 'Region-based models on the QUBICC data, April 2005', \n",
    "            qubicc_model_pred_month_1, qubicc_month_1)\n",
    "save_figure('cross_validation_qubicc_hc4', 'Region-based models on the QUBICC data, November 2005', \n",
    "            qubicc_model_pred_month_2, qubicc_month_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we want to reproduce the plots without running everything again:\n",
    "print('On validation sets')\n",
    "print(valid_means)\n",
    "print(valid_model_predictions)\n",
    "print('NARVAL data')\n",
    "print(narval_means)\n",
    "print(narval_model_predictions)\n",
    "print('Qubicc data')\n",
    "print(qubicc_means)\n",
    "print(qubicc_model_predictions)\n",
    "print('Qubicc data, November 2004')\n",
    "print(qubicc_month_0)\n",
    "print(qubicc_model_pred_month_0)\n",
    "print('Qubicc data, April 2005')\n",
    "print(qubicc_month_1)\n",
    "print(qubicc_model_pred_month_1)\n",
    "print('Qubicc data, November 2005')\n",
    "print(qubicc_month_2)\n",
    "print(qubicc_model_pred_month_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute bounded losses\n",
    "\n",
    "We also save the scaling parameters for the fold-based models as we haven't done that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bounded_loss(model, input_data, output_data, batch_size=2**20):\n",
    "    for i in range(1 + input_data.shape[0]//batch_size):\n",
    "        if i == 0:\n",
    "            a = model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            a = np.concatenate((a, model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])), axis=0)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "    # Bounded output!\n",
    "    pred_adj = np.minimum(np.maximum(a, 0), 100)\n",
    "    \n",
    "    # Mean Squared Error\n",
    "    return np.mean((pred_adj - output_data)**2, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "\n",
    "for i in range(3): \n",
    "    filename = 'cross_validation_region_based_fold_%d'%(i+1)\n",
    "    # Choose appropriate model for this fold\n",
    "    if i == 0: model = model_fold_1\n",
    "    if i == 1: model = model_fold_2\n",
    "    if i == 2: model = model_fold_3\n",
    "        \n",
    "    #Standardize according to the fold\n",
    "    scaler.fit(input_data[training_folds[i]])\n",
    "    \n",
    "    # We save the scaling parameters in a file [only once]\n",
    "    with open(path_model+'/scaler_%d.txt'%seed, 'a') as file:\n",
    "        file.write('Standard Scaler mean values:\\n')\n",
    "        file.write(str(scaler.mean_))\n",
    "        file.write('\\nStandard Scaler standard deviation:\\n')\n",
    "        file.write(str(np.sqrt(scaler.var_)))\n",
    "\n",
    "    # Taken from preprocessing_narval\n",
    "    input_variables = np.array(['qv', 'qc', 'qi', 'temp', 'pres', 'u', 'v', 'zg', 'coriolis', 'qv_below', 'qv_above',\n",
    "                               'qc_below', 'qc_above', 'qi_below', 'qi_above', 'temp_below', 'temp_above',\n",
    "                               'pres_below', 'pres_above', 'u_below', 'u_above', 'v_below', 'v_above', \n",
    "                               'zg_below', 'zg_above','temp_sfc'])\n",
    "    in_and_out_variables = np.array(['qv', 'qc', 'qi', 'temp', 'pres', 'u', 'v', 'zg', 'coriolis', 'qv_below', 'qv_above',\n",
    "                               'qc_below', 'qc_above', 'qi_below', 'qi_above', 'temp_below', 'temp_above',\n",
    "                               'pres_below', 'pres_above', 'u_below', 'u_above', 'v_below', 'v_above', 'zg_below', \n",
    "                               'zg_above','temp_sfc', 'clc'])\n",
    "\n",
    "    # Write the accompanying info-file [only once]\n",
    "    with open(os.path.join(path_model, filename + '.txt'), 'a') as file:\n",
    "        write_infofile(file, str(in_and_out_variables), str(input_variables), path_model, path_data, seed)\n",
    "    \n",
    "    #Load the data for the respective fold\n",
    "    input_train = scaler.transform(input_data[training_folds[i]])\n",
    "    input_valid = scaler.transform(input_data[validation_folds[i]])\n",
    "    output_train = output_data[training_folds[i]]\n",
    "    output_valid = output_data[validation_folds[i]]\n",
    "    \n",
    "    train_loss = compute_bounded_loss(model, input_train, output_train, batch_size=2**15)\n",
    "    valid_loss = compute_bounded_loss(model, input_valid, output_valid, batch_size=2**15)\n",
    "        \n",
    "    with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "        file.write('Bounded training loss: %.4f\\n'%(train_loss))\n",
    "        file.write('Bounded validation loss: %.4f\\n'%(valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'region_based_all_data_seed_10'\n",
    "model = model_all_data\n",
    "\n",
    "#Standardize according to the fold\n",
    "scaler.fit(input_data)\n",
    "\n",
    "#Load the data for the respective fold\n",
    "input_train = scaler.transform(input_data)\n",
    "output_train = output_data\n",
    "\n",
    "train_loss = compute_bounded_loss(model, input_train, output_train, batch_size=2**15)\n",
    "\n",
    "with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "    file.write('Bounded training loss: %.4f\\n'%(train_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds113_kernel",
   "language": "python",
   "name": "clouds113_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
