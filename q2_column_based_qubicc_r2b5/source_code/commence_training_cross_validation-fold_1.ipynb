{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "1. We read the data from the npy files\n",
    "2. We combine the QUBICC and NARVAL data\n",
    "3. We remove columns that are constant throughout\n",
    "4. Set up cross validation\n",
    "\n",
    "During cross-validation:\n",
    "\n",
    "1. We scale the data, convert to tf data (note that it is troublesome to save and load tf data)\n",
    "2. Plot training progress\n",
    "3. Write epochs into a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires at least 500GB to run\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "#Import sklearn before tensorflow (static Thread-local storage)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "t0 = time.time()\n",
    "path = '/pf/b/b309170'\n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "sys.path.insert(0, path + '/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "\n",
    "# Reloading custom file to incorporate changes dynamically\n",
    "import importlib\n",
    "import my_classes\n",
    "importlib.reload(my_classes)\n",
    "\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import TimeOut\n",
    "\n",
    "# Minutes per fold\n",
    "timeout = 2120 \n",
    "\n",
    "# For logging purposes\n",
    "days = 'all_days'\n",
    "\n",
    "# Maximum amount of epochs for each model\n",
    "epochs = 50\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 10\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# For store_mean_model_biases\n",
    "VERT_LAYERS = 31\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[0], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Cover or Cloud Area?\n",
    "output_var = 'cl_area' # Set output_var to one of {'clc', 'cl_area'}\n",
    "# QUBICC only or QUBICC+NARVAL training data? Always True for the paper\n",
    "qubicc_only = True\n",
    "\n",
    "path_base = os.path.join(path, 'workspace_icon-ml/cloud_cover_parameterization/grid_column_based_QUBICC_R02B05')\n",
    "path_data = os.path.join(path, 'my_work/icon-ml_data/cloud_cover_parameterization/grid_column_based_QUBICC_R02B05/based_on_var_interpolated_data')\n",
    "\n",
    "if output_var == 'clc':\n",
    "    full_output_var_name = 'cloud_cover'\n",
    "elif output_var == 'cl_area':\n",
    "    full_output_var_name = 'cloud_area'\n",
    "    \n",
    "if qubicc_only:\n",
    "    output_folder = '%s_R2B5_QUBICC'%full_output_var_name\n",
    "else:\n",
    "    output_folder = '%s_R2B5_QUBICC+NARVAL'%full_output_var_name\n",
    "path_model = os.path.join(path_base, 'saved_models', output_folder)\n",
    "path_figures = os.path.join(path_base, 'figures', output_folder)\n",
    "narval_output_file = '%s_output_narval.npy'%full_output_var_name\n",
    "qubicc_output_file = '%s_output_qubicc.npy'%full_output_var_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevents crashes of the code\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow the growth of memory Tensorflow allocates (limits memory usage overall)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_narval = np.load(path_data + '/cloud_cover_input_narval.npy')\n",
    "# input_qubicc = np.transpose(np.load(path_data + '/cloud_cover_input_qubicc.npy'))\n",
    "# output_narval = np.load(path_data + '/cloud_cover_output_narval.npy')\n",
    "# output_qubicc = np.transpose(np.load(path_data + '/cloud_cover_output_qubicc.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = np.concatenate(input_narval, input_qubicc)\n",
    "# output_data = np.concatenate(output_narval, output_qubicc)\n",
    "input_data = np.concatenate((np.load(path_data + '/cloud_cover_input_narval.npy'), \n",
    "                             np.transpose(np.load(path_data + '/cloud_cover_input_qubicc.npy'))), axis=0)\n",
    "output_data = np.concatenate((np.load(os.path.join(path_data, narval_output_file)), \n",
    "                              np.transpose(np.load(os.path.join(path_data, qubicc_output_file)))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_narval = np.load(path_data + '/cloud_cover_output_narval.npy').shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if qubicc_only:\n",
    "    input_data = input_data[samples_narval:]\n",
    "    output_data = output_data[samples_narval:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176209421, 163)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(samples_total, no_of_features) = input_data.shape\n",
    "(samples_total, no_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Temporal cross-validation*\n",
    "\n",
    "Split into 2-weeks increments (when working with 3 months of data). It's 25 day increments with 5 months of data. <br>\n",
    "1.: Validate on increments 1 and 4 <br>\n",
    "2.: Validate on increments 2 and 5 <br>\n",
    "3.: Validate on increments 3 and 6\n",
    "\n",
    "--> 2/3 training data, 1/3 validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folds = []\n",
    "validation_folds = []\n",
    "two_week_incr = samples_total//6\n",
    "\n",
    "for i in range(3):\n",
    "    # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "    first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "    second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "    validation_folds.append(np.append(first_incr, second_incr))\n",
    "    training_folds.append(np.arange(samples_total))\n",
    "    training_folds[i] = np.delete(training_folds[i], validation_folds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that are constant in at least one of the training folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This takes a bit of time\n",
    "# remove_fields = []\n",
    "# constant_0 = (np.max(input_data[training_folds[0]], axis=0) - np.min(input_data[training_folds[0]], axis=0) < 1e-10)\n",
    "# constant_1 = (np.max(input_data[training_folds[1]], axis=0) - np.min(input_data[training_folds[1]], axis=0) < 1e-10)\n",
    "# constant_2 = (np.max(input_data[training_folds[2]], axis=0) - np.min(input_data[training_folds[2]], axis=0) < 1e-10)\n",
    "# for i in range(no_of_features):\n",
    "#     if constant_0[i] or constant_1[i] or constant_2[i]:\n",
    "#         print(i)\n",
    "#         remove_fields.append(i)\n",
    "\n",
    "# These features correspond to qc_4, qc_5, qc_6, qc_7, qc_8, qc_9, zg_4, zg_5, zg_6\n",
    "remove_fields = [27, 28, 29, 30, 31, 32, 135, 136, 137]\n",
    "assert no_of_features == 163\n",
    "input_data = np.delete(input_data, remove_fields, axis=1)\n",
    "no_of_features = no_of_features - len(remove_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function for the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(\n",
    "                [\n",
    "                    tf.keras.layers.Dense(256, activation='relu', input_dim = no_of_features),\n",
    "                    tf.keras.layers.Dense(256, activation='relu'),\n",
    "                    tf.keras.layers.Dense(27, activation='linear', dtype='float32'),\n",
    "                ],\n",
    "                name=\"column_based_model\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stepping down on the ladder of complexity\n",
    "# # Can't save a custom model\n",
    "# class CustomModel(tf.keras.Model):\n",
    "    \n",
    "#     def __init__(self, model):\n",
    "#         super(CustomModel, self).__init__()\n",
    "#         self.model = model\n",
    "    \n",
    "#     def compile(self, optimizer, loss_fn):\n",
    "#         super(CustomModel, self).compile()\n",
    "#         self.optimizer = optimizer\n",
    "#         self.loss_fn = loss_fn\n",
    "    \n",
    "#     # Call accepts only tf.tensors\n",
    "#     def call(self, x):\n",
    "#         return self.model(x)  \n",
    "    \n",
    "#     # Compile with XLA (throws an error)\n",
    "#     # @tf.function(experimental_compile=True)\n",
    "# #     @tf.function\n",
    "#     def train_step(self, data):\n",
    "#         # Unpack the data. Its structure depends on your model and\n",
    "#         # on what you pass to `fit()`.\n",
    "#         x,y = data\n",
    "\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             y_pred = self(x, training=True)  # Forward pass\n",
    "#             # Compute the loss value\n",
    "#             # (the loss function is configured in `compile()`)\n",
    "#             loss = self.loss_fn(y, y_pred)\n",
    "\n",
    "#         # Compute gradients\n",
    "#         trainable_vars = self.trainable_variables\n",
    "#         gradients = tape.gradient(loss, trainable_vars)\n",
    "#         # Update weights\n",
    "#         self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "#         # Update metrics (includes the metric that tracks the loss)\n",
    "#         self.compiled_metrics.update_state(y, y_pred)\n",
    "#         # Return a dict mapping metric names to current value\n",
    "#         return {'loss': loss}\n",
    "    \n",
    "#     # Without the test step, our model would yield 0 in every kind of evaluation outside training itself\n",
    "#     def test_step(self, data):\n",
    "#         # Unpack the data\n",
    "#         x, y = data\n",
    "#         # Compute predictions\n",
    "#         y_pred = self(x, training=False)\n",
    "#         # Updates the metrics tracking the loss\n",
    "#         loss = self.loss_fn(y, y_pred)\n",
    "#         # Update the metrics.\n",
    "# #         self.compiled_metrics.update_state(y, y_pred)\n",
    "#         # Return a dict mapping metric names to current value.\n",
    "#         # Note that it will include the loss (tracked in self.metrics).\n",
    "#         return {'loss': loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 3-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By decreasing timeout we make sure every fold gets the same amount of time\n",
    "# After all, data-loading took some time (Have 3 folds, 60 seconds/minute)\n",
    "# timeout = timeout - 1/3*1/60*(time.time() - t0)\n",
    "timeout = timeout - 1/60*(time.time() - t0)\n",
    "t0 = time.time()\n",
    "\n",
    "#We loop through the folds\n",
    "for i in range(3):\n",
    "    \n",
    "#     print('hello')\n",
    "    \n",
    "    filename = 'cross_validation_column_based_fold_%d'%(i+1)\n",
    "    \n",
    "    #Standardize according to the fold\n",
    "    scaler.fit(input_data[training_folds[i],:])\n",
    "    \n",
    "#     print('hello')\n",
    "\n",
    "    #Load the data for the respective fold and convert it to tf data\n",
    "    input_train = scaler.transform(input_data[training_folds[i]])\n",
    "    input_valid = scaler.transform(input_data[validation_folds[i]])\n",
    "    output_train = output_data[training_folds[i]]\n",
    "    output_valid = output_data[validation_folds[i]]\n",
    "    \n",
    "    # Clear memory (Reduces memory requirement to 151 GB)\n",
    "    del input_data, output_data, first_incr, second_incr, validation_folds, training_folds\n",
    "    gc.collect()\n",
    "    \n",
    "#     print('hello')\n",
    "#     # Use a batchsize of 64 or 128\n",
    "#     # Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "#     train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "#                                 tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "#                 .batch(batch_size=128, drop_remainder=True).prefetch(1)\n",
    "    \n",
    "#     # Clear memory\n",
    "#     del input_train, output_train\n",
    "#     gc.collect()\n",
    "    \n",
    "# #     print('hello')\n",
    "#     # No need to add prefetch.\n",
    "#     # tf data with batch_size=10**5 makes the validation evaluation 10 times faster\n",
    "#     valid_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_valid), \n",
    "#                                 tf.data.Dataset.from_tensor_slices(output_valid))) \\\n",
    "#                 .batch(batch_size=10**5, drop_remainder=True)\n",
    "    \n",
    "#     # Clear memory (Reduces memory requirement to 151 GB)\n",
    "#     del input_valid, output_valid\n",
    "#     gc.collect()\n",
    "    \n",
    "    #Feed the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.MeanSquaredError()\n",
    "    )\n",
    "    \n",
    "    #Train the model\n",
    "#     time_callback = TimeOut(t0, timeout*(i+1))\n",
    "    time_callback = TimeOut(t0, timeout)\n",
    "    # Batch size is specified by the tf dataset\n",
    "    history = model.fit(input_train, output_train, epochs=epochs, verbose=2, batch_size=128,\n",
    "                        validation_data=(input_valid, output_valid), callbacks=[time_callback])\n",
    "#     history = model.fit(train_ds, epochs=epochs, verbose=2, \n",
    "#                         validation_data=valid_ds, callbacks=[time_callback])\n",
    "    \n",
    "    #Save the model     \n",
    "    #Serialize model to YAML\n",
    "    model_yaml = model.to_yaml()\n",
    "    with open(os.path.join(path_model, filename+\".yaml\"), \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "    #Serialize model and weights to a single HDF5-file\n",
    "    model.save(os.path.join(path_model, filename+'.h5'), \"w\")\n",
    "    print('Saved model to disk')\n",
    "    \n",
    "    #Plot the training history\n",
    "    if len(history.history['loss']) > len(history.history['val_loss']):\n",
    "        del history.history['loss'][-1]\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "    plt.grid(True)\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.savefig(os.path.join(path_figures, filename+'.pdf'))\n",
    "    \n",
    "    with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "        file.write('Results from the %d-th fold\\n'%(i+1))\n",
    "        file.write('Training epochs: %d\\n'%(len(history.history['val_loss'])))\n",
    "        file.write('Weights restored from epoch: %d\\n\\n'%(np.argmin(history.history['val_loss']) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much time did it take? <br>\n",
    "With batch_size=10^5: 330s <br>\n",
    "With batch_size=10^3: 477s <br>\n",
    "With batch_size=10^1: 8511s <br>\n",
    "Without tf data: 3000s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds113_kernel",
   "language": "python",
   "name": "clouds113_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
