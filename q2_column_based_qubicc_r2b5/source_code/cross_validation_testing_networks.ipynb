{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Run **SHERPA**. Fix batchsize = 512. Fix Adam. Do not shuffle the input data as that takes a lot of time. <br>\n",
    "*First:* Start with 3 epochs each. Here we can already discard some models. <br>\n",
    "*Then:* Run 20 epochs for all networks that have learned. Possibly add the learning rate scheduler. Usually one uses cross-validation here to truly get a good estimate of generalization error! <br>\n",
    "*Then:* Run 50 epochs to get the best network. <br>\n",
    "\n",
    "If no network learns then the batch size is simply too large?\n",
    "\n",
    "To vary: \n",
    "- Learning rate (Learning rate scheduler)\n",
    "- Model layers (only max 1-4 hidden layers)\n",
    "- Regularization methods\n",
    "- Hidden Units\n",
    "- Activation Functions (not the last)\n",
    "\n",
    "Batchsize 256: 37 minutes per epoch. <br>\n",
    "Batchsize 512: 9 minutes per epoch.\n",
    "\n",
    "**---> Find that we can not really beat the architecture of the models N1-N3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires at least 500GB to run\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import gc\n",
    "\n",
    "#Import sklearn before tensorflow (static Thread-local storage)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "t0 = time.time()\n",
    "path = '/pf/b/b309170'\n",
    "path_figures = path + '/workspace_icon-ml/cloud_cover_parameterization/grid_column_based_QUBICC_R02B05/figures'\n",
    "path_model = path + '/workspace_icon-ml/cloud_cover_parameterization/grid_column_based_QUBICC_R02B05/saved_models'\n",
    "path_data = path + '/my_work/icon-ml_data/cloud_cover_parameterization/grid_column_based_QUBICC_R02B05/based_on_var_interpolated_data'\n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "sys.path.insert(0, path + '/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "# Add sherpa\n",
    "sys.path.insert(0, path + '/my_work/sherpa')\n",
    "\n",
    "import datetime\n",
    "#import sherpa\n",
    "#import sherpa.algorithms.bayesian_optimization as bayesian_optimization\n",
    "\n",
    "# Reloading custom file to incorporate changes dynamically\n",
    "import importlib\n",
    "import my_classes\n",
    "importlib.reload(my_classes)\n",
    "\n",
    "from my_classes import read_mean_and_std\n",
    "from my_classes import TimeOut\n",
    "\n",
    "# Minutes per fold\n",
    "timeout = 2120 \n",
    "\n",
    "# For logging purposes\n",
    "days = 'all_days'\n",
    "\n",
    "# Maximum amount of epochs for each model\n",
    "epochs = 50\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed = 10\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# For store_mean_model_biases\n",
    "VERT_LAYERS = 31\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_visible_devices(gpus[3], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't run on a CPU node\n",
    "try:\n",
    "    # Prevents crashes of the code\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "\n",
    "    # Allow the growth of memory Tensorflow allocates (limits memory usage overall)\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217047\n",
      "Successfully created the directory /pf/b/b309170/workspace_icon-ml/cloud_cover_parameterization/grid_column_based_QUBICC_R02B05/sherpa_results/2021-05_217047\n"
     ]
    }
   ],
   "source": [
    "# To save the SHERPA output in\n",
    "today = str(datetime.date.today())[:7] # YYYY-MM\n",
    "random_num = np.random.randint(500000)\n",
    "print(random_num)\n",
    "\n",
    "out_path = '/pf/b/b309170/workspace_icon-ml/cloud_cover_parameterization/grid_column_based_QUBICC_R02B05/sherpa_results/'+\\\n",
    "        today+'_'+str(random_num)\n",
    "\n",
    "# Create the directory and save the SHERPA-output in it\n",
    "try:\n",
    "    os.mkdir(out_path)\n",
    "except OSError:\n",
    "    print('Creation of the directory %s failed' % out_path)\n",
    "else: \n",
    "    print('Successfully created the directory %s' % out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_narval = np.load(path_data + '/cloud_cover_input_narval.npy')\n",
    "# input_qubicc = np.transpose(np.load(path_data + '/cloud_cover_input_qubicc.npy'))\n",
    "# output_narval = np.load(path_data + '/cloud_cover_output_narval.npy')\n",
    "# output_qubicc = np.transpose(np.load(path_data + '/cloud_cover_output_qubicc.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = np.concatenate(input_narval, input_qubicc)\n",
    "# output_data = np.concatenate(output_narval, output_qubicc)\n",
    "\n",
    "input_data = np.concatenate((np.load(path_data + '/cloud_cover_input_narval.npy'), \n",
    "                             np.transpose(np.load(path_data + '/cloud_cover_input_qubicc.npy'))), axis=0)\n",
    "output_data = np.concatenate((np.load(path_data + '/cloud_cover_output_narval.npy'), \n",
    "                              np.transpose(np.load(path_data + '/cloud_cover_output_qubicc.npy'))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176209421, 163)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(samples_total, no_of_features) = input_data.shape\n",
    "(samples_total, no_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000000, 163)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "# Take only a subset to test with\n",
    "indices = np.random.randint(samples_total, size=10**7)\n",
    "input_data = input_data[indices]\n",
    "output_data = output_data[indices]\n",
    "\n",
    "(samples_total, no_of_features) = input_data.shape\n",
    "(samples_total, no_of_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Temporal cross-validation*\n",
    "\n",
    "Split into 2-weeks increments (when working with 3 months of data). It's 25 day increments with 5 months of data. <br>\n",
    "1.: Validate on increments 1 and 4 <br>\n",
    "2.: Validate on increments 2 and 5 <br>\n",
    "3.: Validate on increments 3 and 6\n",
    "\n",
    "--> 2/3 training data, 1/3 validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_folds = []\n",
    "validation_folds = []\n",
    "two_week_incr = samples_total//6\n",
    "\n",
    "for i in range(3):\n",
    "    # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "    first_incr = np.arange(samples_total//6*i, samples_total//6*(i+1))\n",
    "    second_incr = np.arange(samples_total//6*(i+3), samples_total//6*(i+4))\n",
    "\n",
    "    validation_folds.append(np.append(first_incr, second_incr))\n",
    "    training_folds.append(np.arange(samples_total))\n",
    "    training_folds[i] = np.delete(training_folds[i], validation_folds[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove columns that are constant in at least one of the training folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This takes a bit of time\n",
    "# remove_fields = []\n",
    "# constant_0 = (np.max(input_data[training_folds[0]], axis=0) - np.min(input_data[training_folds[0]], axis=0) < 1e-10)\n",
    "# constant_1 = (np.max(input_data[training_folds[1]], axis=0) - np.min(input_data[training_folds[1]], axis=0) < 1e-10)\n",
    "# constant_2 = (np.max(input_data[training_folds[2]], axis=0) - np.min(input_data[training_folds[2]], axis=0) < 1e-10)\n",
    "# for i in range(no_of_features):\n",
    "#     if constant_0[i] or constant_1[i] or constant_2[i]:\n",
    "#         print(i)\n",
    "#         remove_fields.append(i)\n",
    "\n",
    "# These features correspond to qc_4, qc_5, qc_6, qc_7, qc_8, qc_9, zg_4, zg_5, zg_6\n",
    "remove_fields = [27, 28, 29, 30, 31, 32, 135, 136, 137]\n",
    "assert no_of_features == 163\n",
    "input_data = np.delete(input_data, remove_fields, axis=1)\n",
    "no_of_features = no_of_features - len(remove_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation function for the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function for the last layer\n",
    "def my_act_fct(x):\n",
    "    return K.minimum(K.maximum(x, 0), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 3-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By decreasing timeout we make sure every fold gets the same amount of time\n",
    "# After all, data-loading took some time (Have 3 folds, 60 seconds/minute)\n",
    "# timeout = timeout - 1/3*1/60*(time.time() - t0)\n",
    "timeout = timeout - 1/60*(time.time() - t0)\n",
    "t0 = time.time()\n",
    "\n",
    "#We loop through the folds\n",
    "# for i in range(3):\n",
    "for i in range(0,1):\n",
    "    \n",
    "    filename = 'cross_validation_column_based_fold_%d'%(i+1)\n",
    "    \n",
    "    #Standardize according to the fold\n",
    "    scaler.fit(input_data[training_folds[i],:])\n",
    "\n",
    "    #Load the data for the respective fold and convert it to tf data\n",
    "    input_train = scaler.transform(input_data[training_folds[i]])\n",
    "    input_valid = scaler.transform(input_data[validation_folds[i]])\n",
    "    output_train = output_data[training_folds[i]]\n",
    "    output_valid = output_data[validation_folds[i]]\n",
    "    # Use a batchsize of 64 or 128\n",
    "    # Possibly better to use .apply(tf.data.experimental.copy_to_device(\"/gpu:0\")) before prefetch\n",
    "    train_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_train), \n",
    "                                tf.data.Dataset.from_tensor_slices(output_train))) \\\n",
    "                .batch(batch_size=128, drop_remainder=True).prefetch(1)\n",
    "    \n",
    "    # No need to add prefetch.\n",
    "    # tf data with batch_size=10**5 makes the validation evaluation 10 times faster\n",
    "    valid_ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(input_valid), \n",
    "                                tf.data.Dataset.from_tensor_slices(output_valid))) \\\n",
    "                .batch(batch_size=10**5, drop_remainder=True)\n",
    "    \n",
    "#     #Feed the model\n",
    "#     model.compile(\n",
    "#         optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "#         loss=tf.keras.losses.MeanSquaredError()\n",
    "#     )\n",
    "    \n",
    "#     #Train the model\n",
    "# #     time_callback = TimeOut(t0, timeout*(i+1))\n",
    "#     time_callback = TimeOut(t0, timeout)\n",
    "#     # Batch size is specified by the tf dataset\n",
    "#     history = model.fit(train_ds, epochs=epochs, verbose=2, \n",
    "#                         validation_data=valid_ds, callbacks=[time_callback])\n",
    "    \n",
    "#     #Save the model     \n",
    "#     #Serialize model to YAML\n",
    "#     model_yaml = model.to_yaml()\n",
    "#     with open(os.path.join(path_model, filename+\".yaml\"), \"w\") as yaml_file:\n",
    "#         yaml_file.write(model_yaml)\n",
    "#     #Serialize model and weights to a single HDF5-file\n",
    "#     model.save(os.path.join(path_model, filename+'.h5'), \"w\")\n",
    "#     print('Saved model to disk')\n",
    "    \n",
    "#     #Plot the training history\n",
    "#     if len(history.history['loss']) > len(history.history['val_loss']):\n",
    "#         del history.history['loss'][-1]\n",
    "#     pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "#     plt.grid(True)\n",
    "#     plt.ylabel('Mean Squared Error')\n",
    "#     plt.xlabel('Number of epochs')\n",
    "#     plt.savefig(os.path.join(path_figures, filename+'.pdf'))\n",
    "    \n",
    "#     with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "#         file.write('Results from the %d-th fold\\n'%(i+1))\n",
    "#         file.write('Training epochs: %d\\n'%(len(history.history['val_loss'])))\n",
    "#         file.write('Weights restored from epoch: %d\\n\\n'%(np.argmin(history.history['val_loss'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear memory (Reduces memory requirement to 238 GB)\n",
    "del input_data, output_data, first_incr, second_incr, validation_folds, training_folds, input_train, output_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(study, today, optimizer):  \n",
    "    study.results = study.results[study.results['Status']=='COMPLETED'] #To specify results\n",
    "    study.results.index = study.results['Trial-ID']  #Trial-ID serves as a better index\n",
    "    # Remove those hyperparameters that actually do not appear in the model\n",
    "    for i in range(1, max(study.results['Trial-ID']) + 1):\n",
    "        depth = study.results.at[i, 'model_depth']\n",
    "        for j in range(depth, 5): #Or up to 8\n",
    "            study.results.at[i, 'activation_%d'%j] = None\n",
    "#             study.results.at[i, 'bn_%d'%j] = None\n",
    "    study.save(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good Reference: https://arxiv.org/pdf/1206.5533.pdf (Bengio), https://arxiv.org/pdf/2004.10652.pdf (Ott)\n",
    "# lrelu = lambda x: relu(x, alpha=0.01)\n",
    "\n",
    "# For Leaky_ReLU:\n",
    "from tensorflow import nn \n",
    "\n",
    "def lrelu(x):\n",
    "    return nn.leaky_relu(x, alpha=0.01)\n",
    "\n",
    "parameters = [sherpa.Ordinal('num_units', [16, 32, 64, 128, 256]), #No need to vary these per layer. Could add 512.\n",
    "             sherpa.Discrete('model_depth', [2, 5]), #Originally [2,8] although 8 was never truly tested\n",
    "             sherpa.Choice('activation_1', ['relu', 'elu', 'tanh', nn.leaky_relu, lrelu]), #Adding SeLU is trickier\n",
    "             sherpa.Choice('activation_2', ['relu', 'elu', 'tanh', nn.leaky_relu, lrelu]), \n",
    "             sherpa.Choice('activation_3', ['relu', 'elu', 'tanh', nn.leaky_relu, lrelu]),\n",
    "             sherpa.Choice('activation_4', ['relu', 'elu', 'tanh', nn.leaky_relu, lrelu]),\n",
    "             sherpa.Choice('last_activation', ['linear', my_act_fct]), # my_act_fct doesn't work in the grid cell based case\n",
    "             sherpa.Continuous('lrinit', [1e-4, 1e-1], 'log'),\n",
    "             sherpa.Ordinal('epsilon', [1e-8, 1e-7, 0.1, 1]),\n",
    "             sherpa.Continuous('dropout', [0]), # Better to have 0 here\n",
    "             sherpa.Continuous('l1_reg', [0, 0.01]),\n",
    "             sherpa.Continuous('l2_reg', [0, 0.01]),\n",
    "             sherpa.Ordinal('bn_1', [0, 1]),\n",
    "             sherpa.Ordinal('bn_2', [0, 1]),\n",
    "             sherpa.Ordinal('bn_3', [1]), # Better to have 1 here\n",
    "             sherpa.Ordinal('bn_4', [0]), # Better to have 0 here\n",
    "             sherpa.Choice('optimizer', ['adam', 'RMSprop', 'adadelta', 'nadam'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sherpa.core:\n",
      "-------------------------------------------------------\n",
      "SHERPA Dashboard running. Access via\n",
      "http://10.50.13.245:8880 if on a cluster or\n",
      "http://localhost:8880 if running locally.\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"sherpa.app.app\" (lazy loading)\n",
      " * Debug mode: on\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# max_num_trials is left unspecified, so the optimization will run until the end of the job-runtime\n",
    "\n",
    "# good_hyperparams = pd.DataFrame({'num_units': [256], 'model_depth': [3], 'activation_1': ['relu'], 'activation_2':['relu'],\n",
    "#                    'activation_3':['relu'], 'activation_4':['relu'], 'last_activation':[my_act_fct], 'lrinit':[0.001], 'epsilon':[1e-7],\n",
    "#                    'dropout':[0], 'l1_reg':[0], 'l2_reg':[0], 'bn_1':[0], 'bn_2':[0], 'bn_3':[0], 'bn_4':[0], 'optimizer':['adam']})\n",
    "\n",
    "# alg = bayesian_optimization.GPyOpt(initial_data_points=good_hyperparams, max_num_trials=100)\n",
    "\n",
    "alg = bayesian_optimization.GPyOpt() \n",
    "study = sherpa.Study(parameters=parameters, algorithm=alg, lower_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "52083/52083 - 133s - loss: 343.7736 - val_loss: 342.9725\n",
      "Epoch 2/3\n",
      "52083/52083 - 132s - loss: 342.9219 - val_loss: 342.7646\n",
      "Epoch 3/3\n",
      "52083/52083 - 132s - loss: 342.7440 - val_loss: 342.5469\n",
      "Epoch 1/3\n",
      "52083/52083 - 277s - loss: 357.1911 - val_loss: 349.1093\n",
      "Epoch 2/3\n",
      "52083/52083 - 276s - loss: 353.6483 - val_loss: 349.0049\n",
      "Epoch 3/3\n",
      "52083/52083 - 277s - loss: 353.3495 - val_loss: 348.7092\n",
      "Epoch 1/3\n",
      "52083/52083 - 174s - loss: 1665.8573 - val_loss: 665.3303\n",
      "Epoch 2/3\n",
      "52083/52083 - 173s - loss: 1400.4211 - val_loss: 656.0166\n",
      "Epoch 3/3\n",
      "52083/52083 - 173s - loss: 807.2425 - val_loss: 443.8656\n",
      "Epoch 1/3\n",
      "52083/52083 - 149s - loss: 348.0784 - val_loss: 346.1411\n",
      "Epoch 2/3\n",
      "52083/52083 - 148s - loss: 346.8481 - val_loss: 345.5026\n",
      "Epoch 3/3\n",
      "52083/52083 - 148s - loss: 346.7727 - val_loss: 345.5513\n",
      "Epoch 1/3\n",
      "52083/52083 - 216s - loss: 350.9333 - val_loss: 347.8096\n",
      "Epoch 2/3\n",
      "52083/52083 - 215s - loss: 350.1321 - val_loss: 347.9420\n",
      "Epoch 3/3\n",
      "52083/52083 - 214s - loss: 350.0687 - val_loss: 347.9176\n",
      "Epoch 1/3\n",
      "52083/52083 - 174s - loss: 350.9407 - val_loss: 348.5149\n",
      "Epoch 2/3\n",
      "52083/52083 - 181s - loss: 350.1039 - val_loss: 348.2517\n",
      "Epoch 3/3\n",
      "52083/52083 - 175s - loss: 350.0154 - val_loss: 348.2900\n",
      "Epoch 1/3\n",
      "52083/52083 - 161s - loss: 350.0511 - val_loss: 348.1964\n",
      "Epoch 2/3\n",
      "52083/52083 - 154s - loss: 349.3862 - val_loss: 348.3375\n",
      "Epoch 3/3\n",
      "52083/52083 - 155s - loss: 349.3639 - val_loss: 348.0351\n",
      "Epoch 1/3\n",
      "52083/52083 - 282s - loss: 348.8513 - val_loss: 346.0770\n",
      "Epoch 2/3\n",
      "52083/52083 - 281s - loss: 346.9414 - val_loss: 345.9006\n",
      "Epoch 3/3\n",
      "52083/52083 - 280s - loss: 346.8847 - val_loss: 345.9371\n",
      "Epoch 1/3\n",
      "52083/52083 - 131s - loss: 362.7834 - val_loss: 350.1695\n",
      "Epoch 2/3\n",
      "52083/52083 - 131s - loss: 349.1590 - val_loss: 348.1777\n",
      "Epoch 3/3\n",
      "52083/52083 - 130s - loss: 347.9009 - val_loss: 347.3916\n",
      "Epoch 1/3\n",
      "52083/52083 - 143s - loss: 349.7953 - val_loss: 347.9929\n",
      "Epoch 2/3\n"
     ]
    }
   ],
   "source": [
    "#Starting only with a few epochs\n",
    "epochs = 3\n",
    "\n",
    "for trial in study:\n",
    "\n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    par = trial.parameters\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Dense(units=par['num_units'], activation=par['activation_1'], input_dim=no_of_features,\n",
    "                   kernel_regularizer=l1_l2(l1=par['l1_reg'], l2=par['l2_reg'])))\n",
    "#     if (par['bn_1']==1):\n",
    "#         model.add(BatchNormalization()) #There's some debate on whether to use it before or after the activation fct\n",
    "    \n",
    "    # Hidden layers    \n",
    "    for j in range(2, par['model_depth']):\n",
    "        model.add(Dense(units=par['num_units'], activation=par['activation_'+str(j)], \n",
    "                        kernel_regularizer=l1_l2(l1=par['l1_reg'], l2=par['l2_reg'])))\n",
    "        model.add(Dropout(par['dropout'])) #After every hidden layer we (potentially) add a dropout layer\n",
    "#         if (par['bn_'+str(j)]==1):\n",
    "#             model.add(BatchNormalization())\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(27, activation=par['last_activation'], \n",
    "                    kernel_regularizer=l1_l2(l1=par['l1_reg'], l2=par['l2_reg'])))\n",
    "    \n",
    "    if par['optimizer'] == 'adam':\n",
    "        # Optimizer: Adam is relatively robust w.r.t. its beta-parameters \n",
    "        optimizer = tf.keras.optimizers.Adam(lr=par['lrinit'], epsilon=par['epsilon'])\n",
    "    elif par['optimizer'] == 'RMSprop':\n",
    "        # Optimizer: RMSprop is robust w.r.t. its hyperparameters\n",
    "        optimizer = tf.keras.optimizers.RMSprop(lr=par['lrinit'], epsilon=par['epsilon'])\n",
    "    elif par['optimizer'] == 'SGD':\n",
    "        optimizer = tf.keras.optimizers.SGD(lr=par['lrinit'], momentum=par['epsilon']) \n",
    "    elif par['optimizer'] == 'adadelta':\n",
    "        optimizer = tf.keras.optimizers.Adadelta(lr=par['lrinit'], epsilon=par['epsilon'])\n",
    "    elif par['optimizer'] == 'nadam':\n",
    "        optimizer = tf.keras.optimizers.Nadam(lr=par['lrinit'], epsilon=par['epsilon'])\n",
    "        \n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(train_ds, epochs=epochs, verbose=2, validation_data=valid_ds,\n",
    "              callbacks=[study.keras_callback(trial, objective_name='val_loss')]) ## 3 epochs\n",
    "    \n",
    "    \n",
    "    study.finalize(trial)\n",
    "    save_model(study, today, par['optimizer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds_kernel",
   "language": "python",
   "name": "clouds_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
