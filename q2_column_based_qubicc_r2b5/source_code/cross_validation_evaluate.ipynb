{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the cross-validation models\n",
    "\n",
    "Throws an error when run inside a slurm job:\n",
    "\n",
    "*QStandardPaths: XDG_RUNTIME_DIR not set, defaulting to '/tmp/runtime-b309170'\n",
    "qt.qpa.screen: QXcbConnection: Could not connect to display mlogin103:31.0\n",
    "Could not connect to any X display.*\n",
    "\n",
    "-> This error happens inside save_figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 16:04:03.175493: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "\n",
    "#Import sklearn before tensorflow (static Thread-local storage)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.errors import ResourceExhaustedError\n",
    "import tensorflow as tf\n",
    "\n",
    "# Add path with my_classes to sys.path\n",
    "path = '/home/b/b309170'\n",
    "sys.path.insert(0, path + '/workspace_icon-ml/iconml_clc/')\n",
    "\n",
    "import my_classes\n",
    "from my_classes import write_infofile\n",
    "from my_classes import read_mean_and_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[0], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Won't run on a CPU node\n",
    "try:\n",
    "    # Prevents crashes of the code\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.set_visible_devices(physical_devices[0], 'GPU')\n",
    "    # Allow the growth of memory Tensorflow allocates (limits memory usage overall)\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Cover or Cloud Area?\n",
    "output_var = 'clc' # Set output_var to one of {'clc', 'cl_area'}\n",
    "# QUBICC only or QUBICC+NARVAL training data?\n",
    "qubicc_only = True\n",
    "# Do we evaluate a model trained on all data?\n",
    "all_data_model = False\n",
    "\n",
    "path_base = os.path.join(path, 'workspace_icon-ml/cloud_cover_parameterization/grid_column_based_QUBICC_R02B05')\n",
    "path_data = os.path.join(path, 'my_work/icon-ml_data/cloud_cover_parameterization/grid_column_based_QUBICC_R02B05/based_on_var_interpolated_data')\n",
    "\n",
    "if output_var == 'clc':\n",
    "    full_output_var_name = 'cloud_cover'\n",
    "elif output_var == 'cl_area':\n",
    "    full_output_var_name = 'cloud_area'\n",
    "    \n",
    "if qubicc_only:\n",
    "    output_folder = '%s_R2B5_QUBICC'%full_output_var_name\n",
    "else:\n",
    "    output_folder = '%s_R2B5_QUBICC+NARVAL'%full_output_var_name\n",
    "path_model = os.path.join(path_base, 'saved_models', output_folder)\n",
    "path_figures = os.path.join(path_base, 'figures', output_folder)\n",
    "narval_output_file = '%s_output_narval.npy'%full_output_var_name\n",
    "qubicc_output_file = '%s_output_qubicc.npy'%full_output_var_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_1 = 'cross_validation_column_based_fold_1.h5'\n",
    "fold_2 = 'cross_validation_column_based_fold_2.h5'\n",
    "fold_3 = 'cross_validation_column_based_fold_3.h5'\n",
    "\n",
    "model_fold_1 = load_model(os.path.join(path_model, fold_1))\n",
    "model_fold_2 = load_model(os.path.join(path_model, fold_2))\n",
    "model_fold_3 = load_model(os.path.join(path_model, fold_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.concatenate((np.load(path_data + '/cloud_cover_input_narval.npy'), \n",
    "                             np.transpose(np.load(path_data + '/cloud_cover_input_qubicc.npy'))), axis=0)\n",
    "output_data = np.concatenate((np.load(os.path.join(path_data, narval_output_file)), \n",
    "                              np.transpose(np.load(os.path.join(path_data, qubicc_output_file)))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_narval = np.load(os.path.join(path_data, narval_output_file)).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "(samples_total, no_of_features) = input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove columns that were constant in at least one of the training folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_fields = [27, 28, 29, 30, 31, 32, 135, 136, 137]\n",
    "assert no_of_features == 163\n",
    "input_data = np.delete(input_data, remove_fields, axis=1)\n",
    "no_of_features = no_of_features - len(remove_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define cross-validation folds to recreate training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 91933934  91933935  91933936 ... 120025759 120025760 120025761]\n",
      "[120025762 120025763 120025764 ... 148117587 148117588 148117589]\n",
      "[148117590 148117591 148117592 ... 176209415 176209416 176209417]\n"
     ]
    }
   ],
   "source": [
    "def set_training_validation_folds(samples_total, samples_narval):\n",
    "    training_folds = []\n",
    "    validation_folds = []\n",
    "    two_week_incr = (samples_total-samples_narval)//6\n",
    "\n",
    "    for i in range(3):\n",
    "        # Note that this is a temporal split since time was the first dimension in the original tensor\n",
    "        first_incr = np.arange(samples_narval+two_week_incr*i, samples_narval+two_week_incr*(i+1))\n",
    "        second_incr = np.arange(samples_narval+two_week_incr*(i+3), samples_narval+two_week_incr*(i+4))\n",
    "        \n",
    "        print(second_incr)\n",
    "\n",
    "        validation_folds.append(np.append(first_incr, second_incr))\n",
    "        training_folds.append(np.arange(samples_narval, samples_total))\n",
    "        training_folds[i] = np.setdiff1d(training_folds[i], validation_folds[i])\n",
    "        \n",
    "    return training_folds, validation_folds\n",
    "\n",
    "if qubicc_only:\n",
    "    # We have to skip the NARVAL data if we do qubicc_only\n",
    "    training_folds, validation_folds = set_training_validation_folds(samples_total, samples_narval)\n",
    "else:\n",
    "    training_folds, validation_folds = set_training_validation_folds(samples_total, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data will need to be scaled according to the training folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful functions to plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_clc_per_vertical_layer(model, input_data, output_data, batch_size=2**20):\n",
    "    '''\n",
    "        Model prediction and the Ground Truth\n",
    "    '''\n",
    "    # output_var means for first model\n",
    "    clc_data_mean = []\n",
    "    for i in range(27):\n",
    "        clc_data_mean.append(np.mean(output_data[:, i], dtype=np.float64))\n",
    "    # Predicted output_var means\n",
    "#     # The batch predicting makes things faster, however, it can run into oom problems\n",
    "#     # Start with a large batch size and decrease it until it works\n",
    "#     for j in range(3):\n",
    "#         try:\n",
    "#             pred_adj = np.minimum(np.maximum(model.predict(input_valid, batch_size=batch_size//(8**j)), 0), 100)\n",
    "#             break\n",
    "#         except(ResourceExhaustedError):\n",
    "#             K.clear_session()\n",
    "#             gc.collect()\n",
    "#             print('Model predict did not work with a batch size of %d'%(batch_size//(8**j)))\n",
    "\n",
    "    # Curiously it works best if we use predict_on_batch on small subsets of the data instead of predict(..., batch_size=...) \n",
    "    # In future correct to: for i in range(1 + input_data.shape[0]//batch_size):\n",
    "    for i in range(input_data.shape[0]//batch_size): \n",
    "        if i == 0:\n",
    "            a = model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            a = np.concatenate((a, model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])), axis=0)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "    pred_adj = np.minimum(np.maximum(a, 0), 100) \n",
    "    \n",
    "    return list(np.mean(pred_adj, axis=0, dtype=np.float64)), clc_data_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(fig_name, fig_title, model_predictions, valid_means=None, all_data_model=False):\n",
    "    '''\n",
    "        Note that this figure truly is a different performance measure than the validation error.\n",
    "        The reason is that the mean can in principle be good even when the model is really bad.\n",
    "        \n",
    "        model_predictions: Array of length 3 or 4, covers predictions from all three folds for a given TL setup\n",
    "        valid_means: Array of length 3 or 4, covers validation means from all three folds for a given TL setup\n",
    "   '''\n",
    "#     assert len(model_biases) == 3\n",
    "    \n",
    "    # Vertical layers\n",
    "    a = np.linspace(5, 31, 27)\n",
    "    fig = plt.figure(figsize=(11,7))\n",
    "    # For model\n",
    "    ax = fig.add_subplot(111, xlabel='Mean %s'%output_var, ylabel='Vertical layer', title=fig_title)\n",
    "    \n",
    "    if all_data_model:    \n",
    "        if not valid_means[0] == valid_means[1] == valid_means[2]:\n",
    "            colors = ['g', 'b', 'r']\n",
    "            for i in range(len(model_predictions)):\n",
    "                ax.plot(model_predictions[i], a, colors[i])\n",
    "                if valid_means != None:\n",
    "                    ax.plot(valid_means[i], a, '%s--'%colors[i])\n",
    "            plt.gca().invert_yaxis()\n",
    "            ax.legend(['Model Fold 1 Predictions', 'Fold 1 Truth', 'Model Fold 2 Predictions', 'Fold 2 Truth', \n",
    "                       'Model Fold 3 Predictions', 'Fold 3 Truth', 'Model All Data Predictions', 'Truth'])\n",
    "        else:\n",
    "            for i in range(len(model_predictions)):\n",
    "                ax.plot(model_predictions[i], a)\n",
    "            ax.plot(valid_means[0], a, 'black')\n",
    "            plt.gca().invert_yaxis()\n",
    "            ax.legend(['Model Fold 1 Predictions', 'Model Fold 2 Predictions', 'Model Fold 3 Predictions', \n",
    "                       'Model All Data Predictions', 'Truth'])\n",
    "    else:\n",
    "        if not valid_means[0] == valid_means[1] == valid_means[2]:\n",
    "            colors = ['g', 'b', 'r']\n",
    "            for i in range(len(model_predictions)):\n",
    "                ax.plot(model_predictions[i], a, colors[i])\n",
    "                if valid_means != None:\n",
    "                    ax.plot(valid_means[i], a, '%s--'%colors[i])\n",
    "            plt.gca().invert_yaxis()\n",
    "            ax.legend(['Model Fold 1 Predictions', 'Fold 1 Truth', 'Model Fold 2 Predictions', 'Fold 2 Truth', \n",
    "                       'Model Fold 3 Predictions', 'Fold 3 Truth'])\n",
    "        else:\n",
    "            for i in range(len(model_predictions)):\n",
    "                ax.plot(model_predictions[i], a)\n",
    "            ax.plot(valid_means[0], a, 'black')\n",
    "            plt.gca().invert_yaxis()\n",
    "            ax.legend(['Model Fold 1 Predictions', 'Model Fold 2 Predictions', 'Model Fold 3 Predictions', \n",
    "                       'Truth'])\n",
    "\n",
    "    fig.savefig(os.path.join(path_figures, fig_name+'.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the models on the data\n",
    "\n",
    "Add training and validation losses to the text files. <br>\n",
    "Print results per vertical layer (respective validation set/NARVAL/QUBICC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses = [] ; valid_losses = [] ; valid_means = [] ; valid_model_predictions = [] ;\n",
    "narval_means = [] ; narval_model_predictions = [] ; qubicc_means = [] ; qubicc_model_predictions = [] ;\n",
    "qubicc_month_0 = [] ; qubicc_model_pred_month_0 = [] ; qubicc_month_1 = [] ; qubicc_model_pred_month_1 = [] ;\n",
    "qubicc_month_2 = [] ; qubicc_model_pred_month_2 = [] ;\n",
    "\n",
    "for i in range(3): \n",
    "    filename = 'cross_validation_column_based_fold_%d'%(i+1)\n",
    "    # Choose appropriate model for this fold\n",
    "    if i == 0: model = model_fold_1\n",
    "    if i == 1: model = model_fold_2\n",
    "    if i == 2: model = model_fold_3\n",
    "    \n",
    "    #Standardize according to the fold\n",
    "    scaler.fit(input_data[training_folds[i]])\n",
    "    \n",
    "    #Load the data for the respective fold\n",
    "    input_train = scaler.transform(input_data[training_folds[i]])\n",
    "    input_valid = scaler.transform(input_data[validation_folds[i]])\n",
    "    output_train = output_data[training_folds[i]]\n",
    "    output_valid = output_data[validation_folds[i]]\n",
    "    \n",
    "    ## Training and validation losses\n",
    "    train_loss = model.evaluate(input_train, output_train, verbose=2, batch_size=10**5)\n",
    "    valid_loss = model.evaluate(input_valid, output_valid, verbose=2, batch_size=10**5)\n",
    "    \n",
    "    # Clear up some memory\n",
    "    del input_train, output_train\n",
    "    gc.collect()\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "        file.write('Unbounded training loss: %.4f\\n'%(train_loss))\n",
    "        file.write('Unbounded validation loss: %.4f\\n'%(valid_loss))\n",
    "        \n",
    "    ## Compute mean cloud cover per vertical layer\n",
    "    # On the respective validation sets (QUBICC and NARVAL)\n",
    "    try:\n",
    "        clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_valid, output_valid)\n",
    "    except(ResourceExhaustedError):\n",
    "        print('Resource Exhausted Qubicc')\n",
    "        clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_valid, output_valid, \n",
    "                                                                   batch_size=2**15)\n",
    "    valid_means.append(clc_data_mean)\n",
    "    valid_model_predictions.append(clc_pred_mean)\n",
    "    \n",
    "    # Clear up some memory\n",
    "    del input_valid, output_valid\n",
    "    gc.collect()\n",
    "    \n",
    "    # For NARVAL\n",
    "    input_narval = scaler.transform(input_data[:samples_narval])\n",
    "    output_narval = output_data[:samples_narval]\n",
    "    try:\n",
    "        clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_narval, output_narval)\n",
    "    except(ResourceExhaustedError):\n",
    "        print('Resource Exhausted Narval')\n",
    "        clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_narval, output_narval, \n",
    "                                                                   batch_size=2**15)\n",
    "    narval_means.append(clc_data_mean)\n",
    "    narval_model_predictions.append(clc_pred_mean)\n",
    "    \n",
    "    # Clear up some memory\n",
    "    del input_narval, output_narval\n",
    "    gc.collect()\n",
    "    \n",
    "    # For QUBICC  \n",
    "    input_qubicc = scaler.transform(input_data[samples_narval:])\n",
    "    output_qubicc = output_data[samples_narval:]\n",
    "    try:\n",
    "        clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_qubicc, output_qubicc)\n",
    "    except(ResourceExhaustedError):\n",
    "        print('Resource Exhausted Qubicc')\n",
    "        clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_qubicc, output_qubicc, \n",
    "                                                                   batch_size=2**15)\n",
    "    qubicc_means.append(clc_data_mean)\n",
    "    qubicc_model_predictions.append(clc_pred_mean)\n",
    "    \n",
    "    # Clear up some memory\n",
    "    del input_qubicc, output_qubicc\n",
    "    gc.collect()\n",
    "    \n",
    "    # QUBICC months\n",
    "    qubicc_month = (samples_total - samples_narval)//3\n",
    "    for month in range(3):\n",
    "        first_ind = samples_narval + month*qubicc_month\n",
    "        last_ind = samples_narval + (month+1)*qubicc_month\n",
    "        input_qubicc = scaler.transform(input_data[first_ind:last_ind])\n",
    "        output_qubicc = output_data[first_ind:last_ind]\n",
    "        try:\n",
    "            clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_qubicc, output_qubicc)\n",
    "        except(ResourceExhaustedError):\n",
    "            print('Resource Exhausted Qubicc')\n",
    "            clc_pred_mean, clc_data_mean = mean_clc_per_vertical_layer(model, input_qubicc, output_qubicc, \n",
    "                                                                       batch_size=2**15)\n",
    "        if month==0: \n",
    "            qubicc_month_0.append(clc_data_mean)\n",
    "            qubicc_model_pred_month_0.append(clc_pred_mean)\n",
    "        if month==1:\n",
    "            qubicc_month_1.append(clc_data_mean)\n",
    "            qubicc_model_pred_month_1.append(clc_pred_mean)\n",
    "        if month==2:\n",
    "            qubicc_month_2.append(clc_data_mean)\n",
    "            qubicc_model_pred_month_2.append(clc_pred_mean)\n",
    "\n",
    "    # Clear up some memory\n",
    "    del input_qubicc, output_qubicc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot results\n",
    "save_figure('cross_validation_validation_means', 'Column-based models on the respective validation sets', \n",
    "            valid_model_predictions, valid_means, all_data_model)\n",
    "save_figure('cross_validation_narval', 'Column-based models on the NARVAL data', \n",
    "            narval_model_predictions, narval_means, all_data_model)\n",
    "save_figure('cross_validation_qubicc', 'Column-based models on the QUBICC data', \n",
    "            qubicc_model_predictions, qubicc_means, all_data_model)\n",
    "# Qubicc months (I checked below that the order is hc2, then hc3, then hc4.)\n",
    "save_figure('cross_validation_qubicc_hc2', 'Column-based models on the QUBICC data, November 2004', \n",
    "            qubicc_model_pred_month_0, qubicc_month_0, all_data_model)\n",
    "save_figure('cross_validation_qubicc_hc3', 'Column-based models on the QUBICC data, April 2005', \n",
    "            qubicc_model_pred_month_1, qubicc_month_1, all_data_model)\n",
    "save_figure('cross_validation_qubicc_hc4', 'Column-based models on the QUBICC data, November 2005', \n",
    "            qubicc_model_pred_month_2, qubicc_month_2, all_data_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we want to reproduce the plots without running everything again:\n",
    "with open(os.path.join(path_figures, 'values_for_figures.txt'), 'w') as file:\n",
    "    file.write('On validation sets\\n')\n",
    "    file.write(str(valid_means))\n",
    "    file.write(str(valid_model_predictions))\n",
    "    file.write('\\n\\nNARVAL data\\n')\n",
    "    file.write(str(narval_means))\n",
    "    file.write(str(narval_model_predictions))\n",
    "    file.write('\\n\\nQubicc data\\n')\n",
    "    file.write(str(qubicc_means))\n",
    "    file.write(str(qubicc_model_predictions))\n",
    "    file.write('\\n\\nQubicc data, November 2004\\n')\n",
    "    file.write(str(qubicc_month_0))\n",
    "    file.write(str(qubicc_model_pred_month_0))\n",
    "    file.write('\\n\\nQubicc data, April 2005\\n')\n",
    "    file.write(str(qubicc_month_1))\n",
    "    file.write(str(qubicc_model_pred_month_1))\n",
    "    file.write('\\n\\nQubicc data, November 2005\\n')\n",
    "    file.write(str(qubicc_month_2))\n",
    "    file.write(str(qubicc_model_pred_month_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The QUBICC data is loaded in the order that I would expect (hc2, then hc3, then hc4)\n",
    "\n",
    "path = '/pf/b/b309170/my_work/QUBICC/data_var_vertinterp_R02B05/'\n",
    "resolution = 'R02B05'\n",
    "\n",
    "# Order of experiments\n",
    "DS = xr.open_mfdataset(path+'hus/*'+resolution+'.nc', combine='by_coords')\n",
    "print(DS.time[0*len(DS.time)//3])\n",
    "print(DS.time[1*len(DS.time)//3])\n",
    "print(DS.time[2*len(DS.time)//3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute bounded losses\n",
    "\n",
    "We also save the scaling parameters for the fold-based models as we haven't done that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes long!\n",
    "def compute_bounded_loss(model, input_data, output_data, batch_size=2**20):\n",
    "    for i in range(1 + input_data.shape[0]//batch_size): \n",
    "        if i == 0:\n",
    "            a = model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            a = np.concatenate((a, model.predict_on_batch(input_data[i*batch_size:(i+1)*batch_size])), axis=0)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        \n",
    "    pred_adj = np.minimum(np.maximum(a, 0), 100)\n",
    "    \n",
    "    # Mean Squared Error\n",
    "    return np.mean((pred_adj - output_data)**2, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "seed = 10\n",
    "\n",
    "for i in range(3): # for i in range(3): \n",
    "    filename = 'cross_validation_column_based_fold_%d'%(i+1)\n",
    "    # Choose appropriate model for this fold\n",
    "    if i == 0: model = model_fold_1\n",
    "    if i == 1: model = model_fold_2\n",
    "    if i == 2: model = model_fold_3\n",
    "        \n",
    "    #Standardize according to the fold\n",
    "    scaler.fit(input_data[training_folds[i]])\n",
    "    \n",
    "#     # We save the scaling parameters in a file [only once]\n",
    "#     seed_i = int(str(seed) + str(i))\n",
    "#     with open(path_model+'/scaler_%d.txt'%seed_i, 'a') as file:\n",
    "#         file.write('Standard Scaler mean values:\\n')\n",
    "#         file.write(str(scaler.mean_))\n",
    "#         file.write('\\nStandard Scaler standard deviation:\\n')\n",
    "#         file.write(str(np.sqrt(scaler.var_)))\n",
    "        \n",
    "#     # Define remove_fields\n",
    "#     remove_fields = [27, 28, 29, 30, 31, 32, 135, 136, 137]\n",
    "\n",
    "#     # Taken from preprocessing_narval\n",
    "#     input_variables = []\n",
    "#     variables = ['qv', 'qc', 'qi', 'temp', 'pres', 'zg']\n",
    "#     for el in variables:\n",
    "#         for i in range(21, 48):\n",
    "#             input_variables.append(el+'_%d'%i)\n",
    "#     input_variables.append('fr_land')\n",
    "\n",
    "#     in_and_out_variables = input_variables.copy()\n",
    "#     variables = [output_var]\n",
    "#     for el in variables:\n",
    "#         for i in range(21, 48):\n",
    "#             in_and_out_variables.append(el+'_%d'%i)\n",
    "        \n",
    "#     in_and_out_variables = np.delete(in_and_out_variables, remove_fields)\n",
    "#     input_variables = np.delete(input_variables, remove_fields)\n",
    "\n",
    "#     # Write the accompanying info-file [only once]\n",
    "#     with open(os.path.join(path_model, filename + '.txt'), 'a') as file:\n",
    "#         write_infofile(file, str(in_and_out_variables), str(input_variables), path_model, path_data, seed_i)\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    #Load the data for the respective fold\n",
    "    input_train = scaler.transform(input_data[training_folds[i]])\n",
    "    input_valid = scaler.transform(input_data[validation_folds[i]])\n",
    "    output_train = output_data[training_folds[i]]\n",
    "    output_valid = output_data[validation_folds[i]]\n",
    "    \n",
    "    train_loss = compute_bounded_loss(model, input_train, output_train, batch_size=2**17)\n",
    "    valid_loss = compute_bounded_loss(model, input_valid, output_valid, batch_size=2**17)\n",
    "        \n",
    "    with open(os.path.join(path_model, filename+'.txt'), 'a') as file:\n",
    "        file.write('Bounded training loss: %.4f\\n'%(train_loss))\n",
    "        file.write('Bounded validation loss: %.4f\\n'%(valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### How often are the predictions of the model from split 2 outside [0, 100]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_fold_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the input data.\n",
    "if output_var == 'cl_area':\n",
    "    mean = [2.57681365e-06, 2.60161901e-06, 2.86229890e-06, 3.49524686e-06,  6.32444387e-06, 1.62852938e-05, 4.26197236e-05, 1.00492283e-04,  2.10850387e-04, 3.96992495e-04, 6.62768743e-04, 1.00639902e-03,  1.42273038e-03, 1.89269379e-03, 2.42406883e-03, 2.97704256e-03,  3.52303812e-03, 4.15430913e-03, 4.89285256e-03, 5.71192194e-03,  6.58451740e-03, 7.47955824e-03, 8.42949837e-03, 9.18162558e-03,  9.58900058e-03, 9.80246788e-03, 9.98071441e-03, 1.99662055e-48,  1.97795858e-36, 2.80309683e-33, 1.17327341e-31, 1.33296743e-30,  1.45585956e-29, 2.57897497e-16, 1.24502901e-08, 5.43912468e-07,  1.97554777e-06, 2.10205332e-06, 3.45718981e-06, 4.17987790e-06,  4.89876027e-06, 6.03250921e-06, 6.71487544e-06, 7.71281746e-06,  9.96528417e-06, 1.40351017e-05, 1.87534642e-05, 2.15523809e-05,  1.77725032e-05, 1.10700238e-05, 6.98113679e-06, 5.98240074e-06,  8.03857856e-06, 1.55278994e-05, 1.98903187e-13, 1.45240003e-10,  2.39426913e-08, 5.63226688e-07, 3.10209365e-06, 6.64324795e-06,  8.83422658e-06, 9.89681102e-06, 9.97096463e-06, 7.74324652e-06,  4.95774608e-06, 2.61087000e-06, 1.29680563e-06, 7.46596833e-07,  4.94444102e-07, 3.51674311e-07, 2.61199355e-07, 2.03219747e-07,  1.66907845e-07, 1.42871199e-07, 1.25114261e-07, 1.11956533e-07,  1.02782118e-07, 9.86031894e-08, 9.95790399e-08, 1.06733810e-07,  1.26921172e-07, 2.10924633e+02, 2.07944695e+02, 2.05115507e+02,  2.03204784e+02, 2.06103772e+02, 2.12329817e+02, 2.19299382e+02,  2.26348890e+02, 2.33352039e+02, 2.40105681e+02, 2.46401637e+02,  2.52153555e+02, 2.57207037e+02, 2.61575645e+02, 2.65446543e+02,  2.68951996e+02, 2.72093136e+02, 2.74765728e+02, 2.76963041e+02,  2.78775116e+02, 2.80398659e+02, 2.81959850e+02, 2.83501227e+02,  2.84935364e+02, 2.86119192e+02, 2.86867707e+02, 2.87046277e+02,  4.78805278e+03, 6.25615004e+03, 8.06726288e+03, 1.03500805e+04,  1.30603494e+04, 1.61944127e+04, 1.97232230e+04, 2.36181577e+04,  2.78401230e+04, 3.23377105e+04, 3.70511232e+04, 4.19785078e+04,  4.70365400e+04, 5.21124420e+04, 5.72512536e+04, 6.23517142e+04,  6.72989145e+04, 7.20972394e+04, 7.66740332e+04, 8.09510300e+04,  8.49437983e+04, 8.85136468e+04, 9.16490946e+04, 9.42529147e+04,  9.63348759e+04, 9.77633315e+04, 9.86144363e+04, 2.07846270e+04,  1.91533379e+04, 1.76039570e+04, 1.61343240e+04, 1.47416307e+04,  1.34230525e+04, 1.21768751e+04, 1.10012039e+04, 9.89431495e+03,  8.85470770e+03, 7.88104473e+03, 6.97198713e+03, 6.12617252e+03,  5.34218664e+03, 4.61854836e+03, 3.95376191e+03, 3.34629894e+03,  2.79465640e+03, 2.29750295e+03, 1.85381761e+03, 1.46282067e+03,  1.12390793e+03, 8.36771545e+02, 6.01482480e+02, 4.18667943e+02,  2.90324051e+02, 2.20122534e+02, 2.57179068e-01]\n",
    "    var = [2.77480154e-14, 7.25973243e-14, 3.99634524e-13, 2.15710125e-12, 8.11907271e-12, 7.61775265e-11, 8.77316868e-10, 6.92865621e-09, 3.75024285e-08, 1.46188082e-07, 3.94244487e-07, 8.66497643e-07, 1.62353588e-06, 2.62130562e-06, 3.83192714e-06, 4.99991907e-06, 6.21865064e-06, 8.01241132e-06, 1.03693004e-05, 1.31320424e-05, 1.64074160e-05, 2.06036189e-05, 2.64319947e-05, 3.14889456e-05, 3.38882152e-05, 3.50739205e-05, 3.62542223e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 3.75468927e-24, 3.77859683e-14, 8.81399575e-12, 7.67332158e-11, 1.05524206e-10, 2.10047036e-10, 2.74443979e-10, 3.28792625e-10, 4.40183561e-10, 5.82035529e-10, 7.95968863e-10, 1.27540590e-09, 2.26932659e-09, 3.55040982e-09, 4.36412766e-09, 3.36921887e-09, 1.94898287e-09, 1.31932279e-09, 1.47840427e-09, 3.00631977e-09, 1.02475319e-08, 8.08985898e-20, 7.53000365e-15, 6.63133983e-12, 7.29001841e-11, 3.89494832e-10, 7.85358520e-10, 9.33067822e-10, 1.00578451e-09, 1.04125431e-09, 7.02788016e-10, 3.35540989e-10, 1.10604147e-10, 3.62540093e-11, 2.00711744e-11, 1.03744494e-11, 4.94660958e-12, 2.65516680e-12, 1.73694133e-12, 1.30232159e-12, 1.05309331e-12, 8.92800996e-13, 7.95007256e-13, 7.34063569e-13, 6.97679020e-13, 6.71116696e-13, 6.55904608e-13, 7.00761126e-13, 2.13386790e+01, 2.83620543e+01, 4.35373333e+01, 6.58895818e+01, 4.11327685e+01, 1.25161844e+01, 1.04050676e+01, 3.03990896e+01, 5.92444799e+01, 8.44920290e+01, 1.01262923e+02, 1.09040431e+02, 1.10571059e+02, 1.10489555e+02, 1.11241419e+02, 1.15661026e+02, 1.22910740e+02, 1.29881594e+02, 1.37074237e+02, 1.45675391e+02, 1.56059349e+02, 1.66116415e+02, 1.76086646e+02, 1.87903449e+02, 2.02713567e+02, 2.17913355e+02, 2.36656922e+02, 2.74458895e+04, 5.11391869e+04, 1.04446470e+05, 2.32219429e+05, 4.84596804e+05, 8.52038001e+05, 1.28343711e+06, 1.71290868e+06, 2.08304820e+06, 2.37089800e+06, 2.59898548e+06, 2.83694575e+06, 3.17998519e+06, 3.73075481e+06, 4.63958852e+06, 6.01537006e+06, 7.86646627e+06, 1.02806323e+07, 1.32050071e+07, 1.65713281e+07, 2.04336800e+07, 2.44266133e+07, 2.84576052e+07, 3.20684854e+07, 3.51439430e+07, 3.73393052e+07, 3.88460516e+07, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 2.78852050e+00, 2.02838259e+01, 5.57561156e+01, 1.42624190e+02, 3.40661029e+02, 7.63342522e+02, 1.66784615e+03, 3.61855509e+03, 7.50449371e+03, 1.44226070e+04, 2.53310370e+04, 4.07648391e+04, 6.09571406e+04, 8.56022800e+04, 1.13726811e+05, 1.44066944e+05, 1.75744678e+05, 2.07956727e+05, 2.39436769e+05, 2.68796829e+05, 2.94535011e+05, 3.15079095e+05, 3.29944182e+05, 3.36064654e+05, 1.78957540e-01]\n",
    "    std = np.sqrt(var)\n",
    "\n",
    "    mean = np.delete(mean, remove_fields)\n",
    "    std = np.delete(std, remove_fields)\n",
    "else:\n",
    "    mean, std = read_mean_and_std(os.path.join(path_model, \n",
    "                                               'cross_validation_column_based_fold_2.txt'))\n",
    "\n",
    "input_train = (input_data[training_folds[1]] - mean)/std\n",
    "input_valid = (input_data[validation_folds[1]] - mean)/std\n",
    "output_train = output_data[training_folds[1]]\n",
    "output_valid = output_data[validation_folds[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff8fc33e790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:7 out of the last 16 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff8fc33e790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# Need 170GB in total\n",
    "batch_size = 2**25\n",
    "\n",
    "for i in range(1 + input_valid.shape[0]//batch_size): ## 1 + input_valid.shape[0]//batch_size\n",
    "    if i == 0:\n",
    "        image_valid = model.predict_on_batch(input_valid[i*batch_size:(i+1)*batch_size])\n",
    "    else:\n",
    "        image_valid = np.concatenate((image_valid, model.predict_on_batch(input_valid[i*batch_size:(i+1)*batch_size])), axis=0)\n",
    "    K.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2846731144255428"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outside of the [0, 100] range\n",
    "(np.sum(np.where(image_valid < 0, True, False)) + np.sum(np.where(image_valid > 100, True, False)))/np.size(image_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011236769903596427"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outside of the [-1, 100] range\n",
    "(np.sum(np.where(image_valid < -1, True, False)) + np.sum(np.where(image_valid > 100, True, False)))/np.size(image_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff8fc33e790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# Need 170GB in total\n",
    "batch_size = 2**25\n",
    "\n",
    "for i in range(1 + input_train.shape[0]//batch_size): ## 1 + input_valid.shape[0]//batch_size\n",
    "    if i == 0:\n",
    "        image_train = model.predict_on_batch(input_train[i*batch_size:(i+1)*batch_size])\n",
    "    else:\n",
    "        image_train = np.concatenate((image_train, model.predict_on_batch(input_train[i*batch_size:(i+1)*batch_size])), axis=0)\n",
    "    K.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28111301628816043"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outside of the [0, 100] range\n",
    "(np.sum(np.where(image_train < 0, True, False)) + np.sum(np.where(image_train > 100, True, False)))/np.size(image_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011329645563319297"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outside of the [-1, 100] range\n",
    "(np.sum(np.where(image_train < -1, True, False)) + np.sum(np.where(image_train > 100, True, False)))/np.size(image_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_all = np.concatenate((image_train, image_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28229971564616607"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outside of the [0, 100] range\n",
    "(np.sum(np.where(image_all < 0, True, False)) + np.sum(np.where(image_all > 100, True, False)))/np.size(image_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011298687010629364"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outside of the [-1, 100] range\n",
    "(np.sum(np.where(image_all < -1, True, False)) + np.sum(np.where(image_all > 100, True, False)))/np.size(image_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (based on the module python3/2022.01)",
   "language": "python",
   "name": "python3_2022_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
