How to use the model:
model = tensorflow.keras.models.load_model(filename+'.h5')
model.predict(scaled input data)

Input/Output
------------
Input and output variables:
['qv_21' 'qv_22' 'qv_23' 'qv_24' 'qv_25' 'qv_26' 'qv_27' 'qv_28' 'qv_29'
 'qv_30' 'qv_31' 'qv_32' 'qv_33' 'qv_34' 'qv_35' 'qv_36' 'qv_37' 'qv_38'
 'qv_39' 'qv_40' 'qv_41' 'qv_42' 'qv_43' 'qv_44' 'qv_45' 'qv_46' 'qv_47'
 'qc_27' 'qc_28' 'qc_29' 'qc_30' 'qc_31' 'qc_32' 'qc_33' 'qc_34' 'qc_35'
 'qc_36' 'qc_37' 'qc_38' 'qc_39' 'qc_40' 'qc_41' 'qc_42' 'qc_43' 'qc_44'
 'qc_45' 'qc_46' 'qc_47' 'qi_21' 'qi_22' 'qi_23' 'qi_24' 'qi_25' 'qi_26'
 'qi_27' 'qi_28' 'qi_29' 'qi_30' 'qi_31' 'qi_32' 'qi_33' 'qi_34' 'qi_35'
 'qi_36' 'qi_37' 'qi_38' 'qi_39' 'qi_40' 'qi_41' 'qi_42' 'qi_43' 'qi_44'
 'qi_45' 'qi_46' 'qi_47' 'temp_21' 'temp_22' 'temp_23' 'temp_24' 'temp_25'
 'temp_26' 'temp_27' 'temp_28' 'temp_29' 'temp_30' 'temp_31' 'temp_32'
 'temp_33' 'temp_34' 'temp_35' 'temp_36' 'temp_37' 'temp_38' 'temp_39'
 'temp_40' 'temp_41' 'temp_42' 'temp_43' 'temp_44' 'temp_45' 'temp_46'
 'temp_47' 'pres_21' 'pres_22' 'pres_23' 'pres_24' 'pres_25' 'pres_26'
 'pres_27' 'pres_28' 'pres_29' 'pres_30' 'pres_31' 'pres_32' 'pres_33'
 'pres_34' 'pres_35' 'pres_36' 'pres_37' 'pres_38' 'pres_39' 'pres_40'
 'pres_41' 'pres_42' 'pres_43' 'pres_44' 'pres_45' 'pres_46' 'pres_47'
 'zg_24' 'zg_25' 'zg_26' 'zg_27' 'zg_28' 'zg_29' 'zg_30' 'zg_31' 'zg_32'
 'zg_33' 'zg_34' 'zg_35' 'zg_36' 'zg_37' 'zg_38' 'zg_39' 'zg_40' 'zg_41'
 'zg_42' 'zg_43' 'zg_44' 'zg_45' 'zg_46' 'zg_47' 'fr_land' 'clc_21'
 'clc_22' 'clc_23' 'clc_24' 'clc_25' 'clc_26' 'clc_27' 'clc_28' 'clc_29'
 'clc_30' 'clc_31' 'clc_32' 'clc_33' 'clc_34' 'clc_35' 'clc_36' 'clc_37'
 'clc_38' 'clc_39' 'clc_40' 'clc_41' 'clc_42' 'clc_43' 'clc_44' 'clc_45'
 'clc_46' 'clc_47']
The (order of) input variables:
['qv_21' 'qv_22' 'qv_23' 'qv_24' 'qv_25' 'qv_26' 'qv_27' 'qv_28' 'qv_29'
 'qv_30' 'qv_31' 'qv_32' 'qv_33' 'qv_34' 'qv_35' 'qv_36' 'qv_37' 'qv_38'
 'qv_39' 'qv_40' 'qv_41' 'qv_42' 'qv_43' 'qv_44' 'qv_45' 'qv_46' 'qv_47'
 'qc_27' 'qc_28' 'qc_29' 'qc_30' 'qc_31' 'qc_32' 'qc_33' 'qc_34' 'qc_35'
 'qc_36' 'qc_37' 'qc_38' 'qc_39' 'qc_40' 'qc_41' 'qc_42' 'qc_43' 'qc_44'
 'qc_45' 'qc_46' 'qc_47' 'qi_21' 'qi_22' 'qi_23' 'qi_24' 'qi_25' 'qi_26'
 'qi_27' 'qi_28' 'qi_29' 'qi_30' 'qi_31' 'qi_32' 'qi_33' 'qi_34' 'qi_35'
 'qi_36' 'qi_37' 'qi_38' 'qi_39' 'qi_40' 'qi_41' 'qi_42' 'qi_43' 'qi_44'
 'qi_45' 'qi_46' 'qi_47' 'temp_21' 'temp_22' 'temp_23' 'temp_24' 'temp_25'
 'temp_26' 'temp_27' 'temp_28' 'temp_29' 'temp_30' 'temp_31' 'temp_32'
 'temp_33' 'temp_34' 'temp_35' 'temp_36' 'temp_37' 'temp_38' 'temp_39'
 'temp_40' 'temp_41' 'temp_42' 'temp_43' 'temp_44' 'temp_45' 'temp_46'
 'temp_47' 'pres_21' 'pres_22' 'pres_23' 'pres_24' 'pres_25' 'pres_26'
 'pres_27' 'pres_28' 'pres_29' 'pres_30' 'pres_31' 'pres_32' 'pres_33'
 'pres_34' 'pres_35' 'pres_36' 'pres_37' 'pres_38' 'pres_39' 'pres_40'
 'pres_41' 'pres_42' 'pres_43' 'pres_44' 'pres_45' 'pres_46' 'pres_47'
 'zg_24' 'zg_25' 'zg_26' 'zg_27' 'zg_28' 'zg_29' 'zg_30' 'zg_31' 'zg_32'
 'zg_33' 'zg_34' 'zg_35' 'zg_36' 'zg_37' 'zg_38' 'zg_39' 'zg_40' 'zg_41'
 'zg_42' 'zg_43' 'zg_44' 'zg_45' 'zg_46' 'zg_47' 'fr_land']

Scaling
-------
Standard Scaler mean values:
[2.57681365e-06 2.60161901e-06 2.86229890e-06 3.49524686e-06
 6.32444387e-06 1.62852938e-05 4.26197236e-05 1.00492283e-04
 2.10850387e-04 3.96992495e-04 6.62768743e-04 1.00639902e-03
 1.42273038e-03 1.89269379e-03 2.42406883e-03 2.97704256e-03
 3.52303812e-03 4.15430913e-03 4.89285256e-03 5.71192194e-03
 6.58451740e-03 7.47955824e-03 8.42949837e-03 9.18162558e-03
 9.58900058e-03 9.80246788e-03 9.98071441e-03 2.57897497e-16
 1.24502901e-08 5.43912468e-07 1.97554777e-06 2.10205332e-06
 3.45718981e-06 4.17987790e-06 4.89876027e-06 6.03250921e-06
 6.71487544e-06 7.71281746e-06 9.96528417e-06 1.40351017e-05
 1.87534642e-05 2.15523809e-05 1.77725032e-05 1.10700238e-05
 6.98113679e-06 5.98240074e-06 8.03857856e-06 1.55278994e-05
 1.98903187e-13 1.45240003e-10 2.39426913e-08 5.63226688e-07
 3.10209365e-06 6.64324795e-06 8.83422658e-06 9.89681102e-06
 9.97096463e-06 7.74324652e-06 4.95774608e-06 2.61087000e-06
 1.29680563e-06 7.46596833e-07 4.94444102e-07 3.51674311e-07
 2.61199355e-07 2.03219747e-07 1.66907845e-07 1.42871199e-07
 1.25114261e-07 1.11956533e-07 1.02782118e-07 9.86031894e-08
 9.95790399e-08 1.06733810e-07 1.26921172e-07 2.10924633e+02
 2.07944695e+02 2.05115507e+02 2.03204784e+02 2.06103772e+02
 2.12329817e+02 2.19299382e+02 2.26348890e+02 2.33352039e+02
 2.40105681e+02 2.46401637e+02 2.52153555e+02 2.57207037e+02
 2.61575645e+02 2.65446543e+02 2.68951996e+02 2.72093136e+02
 2.74765728e+02 2.76963041e+02 2.78775116e+02 2.80398659e+02
 2.81959850e+02 2.83501227e+02 2.84935364e+02 2.86119192e+02
 2.86867707e+02 2.87046277e+02 4.78805278e+03 6.25615004e+03
 8.06726288e+03 1.03500805e+04 1.30603494e+04 1.61944127e+04
 1.97232230e+04 2.36181577e+04 2.78401230e+04 3.23377105e+04
 3.70511232e+04 4.19785078e+04 4.70365400e+04 5.21124420e+04
 5.72512536e+04 6.23517142e+04 6.72989145e+04 7.20972394e+04
 7.66740332e+04 8.09510300e+04 8.49437983e+04 8.85136468e+04
 9.16490946e+04 9.42529147e+04 9.63348759e+04 9.77633315e+04
 9.86144363e+04 1.61343240e+04 1.47416307e+04 1.34230525e+04
 1.21768751e+04 1.10012039e+04 9.89431495e+03 8.85470770e+03
 7.88104473e+03 6.97198713e+03 6.12617252e+03 5.34218664e+03
 4.61854836e+03 3.95376191e+03 3.34629894e+03 2.79465640e+03
 2.29750295e+03 1.85381761e+03 1.46282067e+03 1.12390793e+03
 8.36771545e+02 6.01482480e+02 4.18667943e+02 2.90324051e+02
 2.20122534e+02 2.57179068e-01]
Standard Scaler standard deviation:
[1.66577356e-07 2.69438906e-07 6.32166532e-07 1.46870734e-06
 2.84939866e-06 8.72797379e-06 2.96195352e-05 8.32385500e-05
 1.93655438e-04 3.82345501e-04 6.27888913e-04 9.30858552e-04
 1.27418047e-03 1.61904466e-03 1.95753088e-03 2.23604988e-03
 2.49372225e-03 2.83062031e-03 3.22013981e-03 3.62381600e-03
 4.05060687e-03 4.53912094e-03 5.14120557e-03 5.61150119e-03
 5.82135854e-03 5.92232391e-03 6.02114792e-03 1.93770206e-12
 1.94386132e-07 2.96883744e-06 8.75974976e-06 1.02724976e-05
 1.44929996e-05 1.65663508e-05 1.81326398e-05 2.09805520e-05
 2.41254125e-05 2.82129201e-05 3.57128254e-05 4.76374494e-05
 5.95853155e-05 6.60615445e-05 5.80449728e-05 4.41472861e-05
 3.63224833e-05 3.84500230e-05 5.48299167e-05 1.01230094e-04
 2.84426774e-10 8.67755936e-08 2.57513880e-06 8.53816046e-06
 1.97356234e-05 2.80242488e-05 3.05461589e-05 3.17141059e-05
 3.22684724e-05 2.65101493e-05 1.83177779e-05 1.05168506e-05
 6.02113023e-06 4.48008643e-06 3.22093921e-06 2.22409748e-06
 1.62946826e-06 1.31793070e-06 1.14119306e-06 1.02620335e-06
 9.44881472e-07 8.91631794e-07 8.56775098e-07 8.35271824e-07
 8.19217124e-07 8.09879379e-07 8.37114763e-07 4.61938080e+00
 5.32560366e+00 6.59828260e+00 8.11723979e+00 6.41348334e+00
 3.53782199e+00 3.22568870e+00 5.51353694e+00 7.69704358e+00
 9.19195458e+00 1.00629480e+01 1.04422426e+01 1.05152774e+01
 1.05114012e+01 1.05471048e+01 1.07545816e+01 1.10865116e+01
 1.13965606e+01 1.17078707e+01 1.20696061e+01 1.24923716e+01
 1.28886157e+01 1.32697644e+01 1.37077879e+01 1.42377515e+01
 1.47618886e+01 1.53836576e+01 1.65668010e+02 2.26139751e+02
 3.23181791e+02 4.81891512e+02 6.96129876e+02 9.23059045e+02
 1.13288883e+03 1.30878137e+03 1.44327690e+03 1.53977206e+03
 1.61213693e+03 1.68432353e+03 1.78325130e+03 1.93151619e+03
 2.15397041e+03 2.45262514e+03 2.80472214e+03 3.20634251e+03
 3.63386944e+03 4.07078962e+03 4.52036282e+03 4.94232873e+03
 5.33456701e+03 5.66290433e+03 5.92823270e+03 6.11058960e+03
 6.23266007e+03 1.66988637e+00 4.50375687e+00 7.46700178e+00
 1.19425370e+01 1.84570049e+01 2.76286540e+01 4.08392722e+01
 6.01544270e+01 8.66284809e+01 1.20094159e+02 1.59157271e+02
 2.01903044e+02 2.46894999e+02 2.92578673e+02 3.37234060e+02
 3.79561515e+02 4.19219129e+02 4.56022726e+02 4.89322766e+02
 5.18456198e+02 5.42710799e+02 5.61319067e+02 5.74407679e+02
 5.79710837e+02 4.23033734e-01]
=> Apply this standard scaling to (only) the input data before processing.

Preprocessed data
-----------------
/pf/b/b309170/my_work/icon-ml_data/cloud_cover_parameterization/grid_column_based_QUBICC_R02B05/based_on_var_interpolated_data/cloud_cover_input_qubicc.npy

Model
-----
Training epochs: 45
Weights restored from epoch: 42
Unbounded training loss: 7.8808
Unbounded validation loss: 8.1371
Bounded training loss: 7.7168
Bounded validation loss: 8.0274
