How to use the model:
model = tensorflow.keras.models.load_model(filename+'.h5')
model.predict(scaled input data)

Input/Output
------------
Input and output variables:
['qv_21' 'qv_22' 'qv_23' 'qv_24' 'qv_25' 'qv_26' 'qv_27' 'qv_28' 'qv_29'
 'qv_30' 'qv_31' 'qv_32' 'qv_33' 'qv_34' 'qv_35' 'qv_36' 'qv_37' 'qv_38'
 'qv_39' 'qv_40' 'qv_41' 'qv_42' 'qv_43' 'qv_44' 'qv_45' 'qv_46' 'qv_47'
 'qc_27' 'qc_28' 'qc_29' 'qc_30' 'qc_31' 'qc_32' 'qc_33' 'qc_34' 'qc_35'
 'qc_36' 'qc_37' 'qc_38' 'qc_39' 'qc_40' 'qc_41' 'qc_42' 'qc_43' 'qc_44'
 'qc_45' 'qc_46' 'qc_47' 'qi_21' 'qi_22' 'qi_23' 'qi_24' 'qi_25' 'qi_26'
 'qi_27' 'qi_28' 'qi_29' 'qi_30' 'qi_31' 'qi_32' 'qi_33' 'qi_34' 'qi_35'
 'qi_36' 'qi_37' 'qi_38' 'qi_39' 'qi_40' 'qi_41' 'qi_42' 'qi_43' 'qi_44'
 'qi_45' 'qi_46' 'qi_47' 'temp_21' 'temp_22' 'temp_23' 'temp_24' 'temp_25'
 'temp_26' 'temp_27' 'temp_28' 'temp_29' 'temp_30' 'temp_31' 'temp_32'
 'temp_33' 'temp_34' 'temp_35' 'temp_36' 'temp_37' 'temp_38' 'temp_39'
 'temp_40' 'temp_41' 'temp_42' 'temp_43' 'temp_44' 'temp_45' 'temp_46'
 'temp_47' 'pres_21' 'pres_22' 'pres_23' 'pres_24' 'pres_25' 'pres_26'
 'pres_27' 'pres_28' 'pres_29' 'pres_30' 'pres_31' 'pres_32' 'pres_33'
 'pres_34' 'pres_35' 'pres_36' 'pres_37' 'pres_38' 'pres_39' 'pres_40'
 'pres_41' 'pres_42' 'pres_43' 'pres_44' 'pres_45' 'pres_46' 'pres_47'
 'zg_24' 'zg_25' 'zg_26' 'zg_27' 'zg_28' 'zg_29' 'zg_30' 'zg_31' 'zg_32'
 'zg_33' 'zg_34' 'zg_35' 'zg_36' 'zg_37' 'zg_38' 'zg_39' 'zg_40' 'zg_41'
 'zg_42' 'zg_43' 'zg_44' 'zg_45' 'zg_46' 'zg_47' 'fr_land' 'clc_21'
 'clc_22' 'clc_23' 'clc_24' 'clc_25' 'clc_26' 'clc_27' 'clc_28' 'clc_29'
 'clc_30' 'clc_31' 'clc_32' 'clc_33' 'clc_34' 'clc_35' 'clc_36' 'clc_37'
 'clc_38' 'clc_39' 'clc_40' 'clc_41' 'clc_42' 'clc_43' 'clc_44' 'clc_45'
 'clc_46' 'clc_47']
The (order of) input variables:
['qv_21' 'qv_22' 'qv_23' 'qv_24' 'qv_25' 'qv_26' 'qv_27' 'qv_28' 'qv_29'
 'qv_30' 'qv_31' 'qv_32' 'qv_33' 'qv_34' 'qv_35' 'qv_36' 'qv_37' 'qv_38'
 'qv_39' 'qv_40' 'qv_41' 'qv_42' 'qv_43' 'qv_44' 'qv_45' 'qv_46' 'qv_47'
 'qc_27' 'qc_28' 'qc_29' 'qc_30' 'qc_31' 'qc_32' 'qc_33' 'qc_34' 'qc_35'
 'qc_36' 'qc_37' 'qc_38' 'qc_39' 'qc_40' 'qc_41' 'qc_42' 'qc_43' 'qc_44'
 'qc_45' 'qc_46' 'qc_47' 'qi_21' 'qi_22' 'qi_23' 'qi_24' 'qi_25' 'qi_26'
 'qi_27' 'qi_28' 'qi_29' 'qi_30' 'qi_31' 'qi_32' 'qi_33' 'qi_34' 'qi_35'
 'qi_36' 'qi_37' 'qi_38' 'qi_39' 'qi_40' 'qi_41' 'qi_42' 'qi_43' 'qi_44'
 'qi_45' 'qi_46' 'qi_47' 'temp_21' 'temp_22' 'temp_23' 'temp_24' 'temp_25'
 'temp_26' 'temp_27' 'temp_28' 'temp_29' 'temp_30' 'temp_31' 'temp_32'
 'temp_33' 'temp_34' 'temp_35' 'temp_36' 'temp_37' 'temp_38' 'temp_39'
 'temp_40' 'temp_41' 'temp_42' 'temp_43' 'temp_44' 'temp_45' 'temp_46'
 'temp_47' 'pres_21' 'pres_22' 'pres_23' 'pres_24' 'pres_25' 'pres_26'
 'pres_27' 'pres_28' 'pres_29' 'pres_30' 'pres_31' 'pres_32' 'pres_33'
 'pres_34' 'pres_35' 'pres_36' 'pres_37' 'pres_38' 'pres_39' 'pres_40'
 'pres_41' 'pres_42' 'pres_43' 'pres_44' 'pres_45' 'pres_46' 'pres_47'
 'zg_24' 'zg_25' 'zg_26' 'zg_27' 'zg_28' 'zg_29' 'zg_30' 'zg_31' 'zg_32'
 'zg_33' 'zg_34' 'zg_35' 'zg_36' 'zg_37' 'zg_38' 'zg_39' 'zg_40' 'zg_41'
 'zg_42' 'zg_43' 'zg_44' 'zg_45' 'zg_46' 'zg_47' 'fr_land']

Scaling
-------
Standard Scaler mean values:
[2.59973607e-06 2.64185957e-06 2.88876621e-06 3.44285382e-06
 6.24485357e-06 1.60843373e-05 4.20025914e-05 9.89456905e-05
 2.07557367e-04 3.90418468e-04 6.49948221e-04 9.84482108e-04
 1.38955580e-03 1.84827758e-03 2.36675527e-03 2.91119602e-03
 3.45899873e-03 4.09070861e-03 4.82286374e-03 5.63447868e-03
 6.50992918e-03 7.40110933e-03 8.33512327e-03 9.07630144e-03
 9.47933846e-03 9.69128802e-03 9.86871439e-03 2.49510954e-16
 1.23649218e-08 5.57770523e-07 2.00445869e-06 2.05233395e-06
 3.40365956e-06 4.12583987e-06 4.81599119e-06 6.08951532e-06
 6.73693912e-06 7.83009715e-06 1.01738458e-05 1.42109569e-05
 1.89469248e-05 2.22041875e-05 1.89580729e-05 1.18047399e-05
 7.22478683e-06 5.95459042e-06 7.54805492e-06 1.43329496e-05
 6.01180104e-14 1.30464488e-10 2.52372510e-08 6.12423572e-07
 3.08322240e-06 6.64920610e-06 8.88410520e-06 9.86874265e-06
 9.80282552e-06 7.62132081e-06 4.78819314e-06 2.51233898e-06
 1.29532904e-06 7.83470010e-07 5.15577100e-07 3.57565211e-07
 2.62846577e-07 2.03281172e-07 1.64792840e-07 1.39601372e-07
 1.21381301e-07 1.06915976e-07 9.66226616e-08 9.21813236e-08
 9.38062967e-08 1.02342360e-07 1.27181840e-07 2.11016611e+02
 2.08070134e+02 2.05187293e+02 2.03155325e+02 2.06033420e+02
 2.12187441e+02 2.19145999e+02 2.26210125e+02 2.33210225e+02
 2.39953239e+02 2.46266003e+02 2.52039750e+02 2.57104986e+02
 2.61478207e+02 2.65334159e+02 2.68807531e+02 2.71909138e+02
 2.74549075e+02 2.76735163e+02 2.78541963e+02 2.80122312e+02
 2.81636656e+02 2.83158451e+02 2.84578542e+02 2.85750508e+02
 2.86492678e+02 2.86672082e+02 4.78303626e+03 6.24881292e+03
 8.05681595e+03 1.03367190e+04 1.30441075e+04 1.61758015e+04
 1.97030358e+04 2.35966186e+04 2.78173813e+04 3.23143181e+04
 3.70273598e+04 4.19541986e+04 4.70115649e+04 5.20868580e+04
 5.72252899e+04 6.23260134e+04 6.72742871e+04 7.20744354e+04
 7.66534983e+04 8.09328239e+04 8.49281895e+04 8.85011096e+04
 9.16397892e+04 9.42465552e+04 9.63310661e+04 9.77613799e+04
 9.86136883e+04 1.61343241e+04 1.47416309e+04 1.34230528e+04
 1.21768755e+04 1.10012045e+04 9.89431589e+03 8.85470904e+03
 7.88104656e+03 6.97198957e+03 6.12617573e+03 5.34219082e+03
 4.61855375e+03 3.95376869e+03 3.34630722e+03 2.79466621e+03
 2.29751422e+03 1.85383024e+03 1.46283456e+03 1.12392294e+03
 8.36787528e+02 6.01499264e+02 4.18685329e+02 2.90341854e+02
 2.20140508e+02 2.57186751e-01]
Standard Scaler standard deviation:
[1.43780644e-07 2.75855901e-07 6.26524986e-07 1.45376036e-06
 2.84060448e-06 8.82047046e-06 2.99788110e-05 8.42585238e-05
 1.96235016e-04 3.87380444e-04 6.34926228e-04 9.41225621e-04
 1.28878636e-03 1.63975153e-03 1.98162220e-03 2.26251222e-03
 2.52503416e-03 2.85995563e-03 3.23928576e-03 3.62868616e-03
 4.04460970e-03 4.52676817e-03 5.12236449e-03 5.58698910e-03
 5.79884508e-03 5.90356959e-03 6.00613970e-03 1.93513995e-12
 1.94813802e-07 3.01923601e-06 8.87623521e-06 1.00432224e-05
 1.42897522e-05 1.63666564e-05 1.78384582e-05 2.11658199e-05
 2.41344254e-05 2.87963670e-05 3.68738856e-05 4.89030645e-05
 6.06916299e-05 6.71810401e-05 6.00022268e-05 4.52241394e-05
 3.55566753e-05 3.64022340e-05 5.05384504e-05 9.43652662e-05
 8.65713556e-11 8.26735358e-08 2.59572888e-06 8.96535682e-06
 1.95267600e-05 2.81925433e-05 3.11123778e-05 3.26363199e-05
 3.24129895e-05 2.66557872e-05 1.80969230e-05 1.03552028e-05
 6.07421919e-06 4.56744018e-06 3.17806919e-06 2.13655232e-06
 1.57319362e-06 1.27082321e-06 1.08221905e-06 9.55379792e-07
 8.65906086e-07 8.02717036e-07 7.64372593e-07 7.46152232e-07
 7.36490718e-07 7.34861611e-07 7.82410498e-07 5.13231882e+00
 5.60039498e+00 6.69402981e+00 8.12160924e+00 6.27055260e+00
 3.49133654e+00 3.40954636e+00 5.68441525e+00 7.85230548e+00
 9.32948049e+00 1.01615140e+01 1.05199354e+01 1.06039819e+01
 1.06154312e+01 1.06548374e+01 1.08368136e+01 1.11326033e+01
 1.14134305e+01 1.16933816e+01 1.20235017e+01 1.24357611e+01
 1.28500423e+01 1.32486100e+01 1.37076397e+01 1.42565691e+01
 1.47912318e+01 1.54095137e+01 1.75202851e+02 2.37155887e+02
 3.35439455e+02 4.95192177e+02 7.09100159e+02 9.34764311e+02
 1.14299075e+03 1.31696524e+03 1.44939643e+03 1.54368547e+03
 1.61402453e+03 1.68444179e+03 1.78145356e+03 1.92707311e+03
 2.14659446e+03 2.44228567e+03 2.79188423e+03 3.19139667e+03
 3.61708799e+03 4.05233798e+03 4.50035815e+03 4.92094544e+03
 5.31207223e+03 5.63944129e+03 5.90408350e+03 6.08608680e+03
 6.20781101e+03 1.66999516e+00 4.50405006e+00 7.46748709e+00
 1.19433121e+01 1.84581991e+01 2.76304292e+01 4.08416431e+01
 6.01571679e+01 8.66314248e+01 1.20097462e+02 1.59161595e+02
 2.01909295e+02 2.46903969e+02 2.92590797e+02 3.37249371e+02
 3.79579796e+02 4.19240098e+02 4.56046103e+02 4.89348294e+02
 5.18483563e+02 5.42739731e+02 5.61349185e+02 5.74438604e+02
 5.79742079e+02 4.23039124e-01]
=> Apply this standard scaling to (only) the input data before processing.

Preprocessed data
-----------------
/pf/b/b309170/my_work/icon-ml_data/cloud_cover_parameterization/grid_column_based_QUBICC_R02B05/based_on_var_interpolated_data/cloud_cover_input_qubicc.npy

Model
-----
Training epochs: 40
Weights restored from epoch: 16
Unbounded training loss: 8.0056
Unbounded validation loss: 8.1257
Bounded training loss: 7.8404
Bounded validation loss: 7.9826
