{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commence training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# import importlib\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "path = '/pf/b/b309170'\n",
    "# Add path with my_classes to sys.path\n",
    "sys.path.insert(0, path + '/workspace_icon-ml/cloud_cover_parameterization/')\n",
    "\n",
    "from my_classes import TimeOut\n",
    "\n",
    "path_data = path + '/my_work/icon-ml_data/cloud_cover_parameterization/region_based/based_on_var_interpolated_data'\n",
    "path_model = path + '/workspace_icon-ml/cloud_cover_parameterization/region_based/saved_models'\n",
    "path_fig = path + '/workspace_icon-ml/cloud_cover_parameterization/region_based/figures'\n",
    "\n",
    "NUM = 1\n",
    "# no_NNs = 27 #How many NNs do we train\n",
    "no_NNs = 9 # How many NNs do we train (usually 10). We can't train all of them in a single batch job.\n",
    "# no_NNs = 2\n",
    "timeout = 220 #Stop after how many minutes (per NN) (usually 210)\n",
    "# timeout = 15\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "ALL_INPUT_VARIABLES = np.array(['qv_i-2', 'qv_i-1', 'qv_i', 'qv_i+1', 'qv_i+2', 'qc_i-2', 'qc_i-1',\n",
    "       'qc_i', 'qc_i+1', 'qc_i+2', 'qi_i-2', 'qi_i-1', 'qi_i', 'qi_i+1',\n",
    "       'qi_i+2', 'temp_i-2', 'temp_i-1', 'temp_i', 'temp_i+1', 'temp_i+2',\n",
    "       'pres_i-2', 'pres_i-1', 'pres_i', 'pres_i+1', 'pres_i+2', 'rho_i-2',\n",
    "       'rho_i-1', 'rho_i', 'rho_i+1', 'rho_i+2', 'zg_i-2', 'zg_i-1', 'zg_i',\n",
    "       'zg_i+1', 'zg_i+2', 'fr_lake', 'clc_prev'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "input_train = np.load(path_data + '/cloud_cover_input_train_%d.npy'%NUM)\n",
    "input_valid = np.load(path_data + '/cloud_cover_input_valid_%d.npy'%NUM)\n",
    "input_test = np.load(path_data + '/cloud_cover_input_test_%d.npy'%NUM)\n",
    "output_train = np.load(path_data + '/cloud_cover_output_train_%d.npy'%NUM)\n",
    "output_valid = np.load(path_data + '/cloud_cover_output_valid_%d.npy'%NUM)\n",
    "output_test = np.load(path_data + '/cloud_cover_output_test_%d.npy'%NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, no_NNs): #for i in range(0, no_NNs)\n",
    "    t0 = time.time() \n",
    "    \n",
    "    # Training data for the i-th NN\n",
    "    start_ind_train = (output_train.shape[0]//27)*i\n",
    "    end_ind_train = (output_train.shape[0]//27)*(i+1)\n",
    "    start_ind_valid = (output_valid.shape[0]//27)*i\n",
    "    end_ind_valid = (output_valid.shape[0]//27)*(i+1)\n",
    "    start_ind_test = (output_test.shape[0]//27)*i\n",
    "    end_ind_test = (output_test.shape[0]//27)*(i+1) \n",
    "    \n",
    "    input_train_NN = input_train[start_ind_train:end_ind_train]\n",
    "    output_train_NN = output_train[start_ind_train:end_ind_train]\n",
    "    input_valid_NN = input_valid[start_ind_valid:end_ind_valid]\n",
    "    output_valid_NN = output_valid[start_ind_valid:end_ind_valid]\n",
    "    input_test_NN = input_test[start_ind_test:end_ind_test]\n",
    "    output_test_NN = output_test[start_ind_test:end_ind_test]\n",
    "    \n",
    "    # We remove the input variables with zero variance. We compute the resulting input dimension for the NN.\n",
    "    input_dim = 37\n",
    "    vars_to_remove = []\n",
    "    for j in range(37):\n",
    "        if np.var(input_train_NN[:, j]) == 0 or np.isnan(np.var(input_train_NN[:, j])):\n",
    "            input_dim -= 1\n",
    "            vars_to_remove.append(j)\n",
    "    \n",
    "    input_train_NN = np.delete(input_train_NN, vars_to_remove, 1)\n",
    "    input_valid_NN = np.delete(input_valid_NN, vars_to_remove, 1)\n",
    "    input_test_NN = np.delete(input_test_NN, vars_to_remove, 1)\n",
    "    \n",
    "    # Defining the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', input_dim = input_dim))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "\n",
    "    # The default initial learning rate of 0.001 yields a bad training process (loss keeps jumping up and down)\n",
    "    # This is only for the 0th and 1st layer, and a lower lr doesn't really improve training\n",
    "    # I'm using the default lr from the 11th layer on\n",
    "    model.compile(loss='mse', optimizer=Nadam())\n",
    "    time_callback = TimeOut(t0, timeout)\n",
    "    history = model.fit(input_train_NN, output_train_NN, batch_size=32, epochs=70, \n",
    "                        validation_data=(input_valid_NN, output_valid_NN), verbose=2, \n",
    "                        callbacks=[time_callback])\n",
    "    \n",
    "    filename = \"model_clc_all_days_final_%d_%d\"%(NUM,i)\n",
    "\n",
    "    #Serialize model to YAML\n",
    "    model_yaml = model.to_yaml()\n",
    "    with open(os.path.join(path_model, filename+\".yaml\"), \"w\") as yaml_file:\n",
    "        yaml_file.write(model_yaml)\n",
    "\n",
    "    #Serialize model and weights to a single HDF5-file\n",
    "    model.save(os.path.join(path_model, filename+'.h5'))\n",
    "    print('Saved model to disk')\n",
    "\n",
    "    #Plotting the training progress\n",
    "    if len(history.history['loss']) > len(history.history['val_loss']):\n",
    "        del history.history['loss'][-1]\n",
    "    pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "    plt.grid(True)\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.xlabel('Number of epochs')\n",
    "    plt.title('Training of the NN parameterization for the %d. layer'%(i+1))\n",
    "    plt.savefig(os.path.join(path_fig, filename+'.pdf'))\n",
    "\n",
    "    train_loss = model.evaluate(input_train_NN, output_train_NN, verbose=2, batch_size=300)\n",
    "    valid_loss = model.evaluate(input_valid_NN, output_valid_NN, verbose=2, batch_size=30)\n",
    "    test_loss = model.evaluate(input_test_NN, output_test_NN, verbose=2, batch_size=30)\n",
    "    \n",
    "    with open(os.path.join(path_model, 'model_region_based_final_%d.txt'%NUM), 'a') as file:\n",
    "        file.write('\\nRemoved input variables %d: %s\\n'%(i, np.array(ALL_INPUT_VARIABLES)[vars_to_remove]))\n",
    "        file.write('Training loss %d: %.4f\\n'%(i, train_loss))\n",
    "        file.write('Validation loss %d: %.4f\\n'%(i, valid_loss))\n",
    "        file.write('Test loss %d: %.4f\\n'%(i, test_loss))\n",
    "        file.write('Training epochs %d: %d'%(i, len(history.history['val_loss'])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clouds113_kernel",
   "language": "python",
   "name": "clouds113_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
